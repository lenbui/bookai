[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Artificial Intelligence",
    "section": "",
    "text": "About the book\nArtificial intelligence lectures",
    "crumbs": [
      "About the book"
    ]
  },
  {
    "objectID": "ai-01-introduction.html",
    "href": "ai-01-introduction.html",
    "title": "1  Introduction to Artificial Intelligence",
    "section": "",
    "text": "1.1 Successes of Artificial Intelligence",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "ai-01-introduction.html#successes-of-artificial-intelligence",
    "href": "ai-01-introduction.html#successes-of-artificial-intelligence",
    "title": "1  Introduction to Artificial Intelligence",
    "section": "",
    "text": "Language\n\nLarge Language Model\n\nChatGPT (OpenAI): Based on GPT-4, it exhibits human-level performance on various professional and academic benchmarks.\nClaude (Anthropic): Known for large context windows and strong reasoning capabilities.\nGemini (Google): A multimodal model capable of understanding and reasoning across text, images, audio, video, and code.\n\n\n\nTranslation\n\nReal-time Translation: Models like Meta’s SeamlessM4T enable nearly instantaneous speech-to-speech translation across nearly 100 languages.\n\n“The reason Boeing are doing this is to cram more seats in to make their plane more competitive with our products,” said Kevin Keniston, head of passenger comfort at Europe’s Airbus.\n\n\\downarrow\n\nKevin Keniston, người đứng đầu bộ phận thoải mái của hành khách tại Europe’s Airbus, cho biết: “Lý do Boeing làm điều này là để nhồi nhét thêm ghế để máy bay của họ cạnh tranh hơn với các sản phẩm của chúng tôi.’’\n\n\nSpeech Applications\n\nSiri (Apple): A virtual assistant that can perform tasks such as setting alarms, making calls, and answering questions using natural language.\nAlexa (Amazon): A voice-controlled assistant that can play music, control smart home devices, and provide information.\nGoogle Assistant: A virtual assistant that can perform tasks such as setting alarms, making calls, and answering questions using natural language.\n\n\n\n\nGoogle AI Studio\n\nGoogle AI Studio: A web-based solution that allows developers to quickly prototype and build AI-powered applications using Google’s AI models.\n\n\n\nAuto-captioning\n\nVinyals et al., 2015\n\n\nText-to-Image\nOpenAI’s DALL-E\n\nTEXT PROMPT:\n\n\nan illustration of a baby daikon radish in a tutu walking a dog\n\n\nAI-GENERATED IMAGES:\n\n\n\n\n\nVision\n\nDetection and Segmentation\n\nP. O. Pinheiro, T. Y. Lin, R. Collobert, and P. Dollar. Learning to refine object segments, ECCV, 2016\nFace recognition\n\nImage generation\nFaces: 1024x1024 resolution, CelebA-HQ dataset\n\nT. Karras, T. Aila, S. Laine, and J. Lehtinen, Progressive Growing of GANs for Improved Quality, Stability, and Variation, ICLR 2018\nDeepFakes\n\nH. Kim et al., Deep video portraits, SIGGRAPH, 2018\n\n\n\nGames\n\n2013: DeepMind uses deep reinforcement learning to beat humans at some Atari games\n2016: DeepMind’s AlphaGo system beats Go grandmaster Lee Sedol 4-1\n2017: AlphaZero learns to play Go and chess from scratch\nAlphaStar (2019): Achieved Grandmaster level in StarCraft II.\nCicero (2022): Mastered the game of Diplomacy, combining strategic reasoning with natural language negotiation to cooperate with humans.\n\n\n\n\nRobotics\n\nRobotic Transformers (RT-2): Google’s vision-language-action (VLA) model that controls robots using web-scale data.\nHumanoid Robots: Tesla Optimus and Boston Dynamics’ Atlas are rapidly evolving to perform complex, dynamic tasks in human environments.\n\n\n\n\nScience\n\nAccelerating Discovery\n\nAlphaFold 3 (2024): Predicts the structure and interactions of all life’s molecules (proteins, DNA, RNA, ligands) with high accuracy.\nGNoME (2023): Discovered 2.2 million new crystals, accelerating materials science by orders of magnitude.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "ai-01-introduction.html#what-is-ai",
    "href": "ai-01-introduction.html#what-is-ai",
    "title": "1  Introduction to Artificial Intelligence",
    "section": "1.2 What is AI",
    "text": "1.2 What is AI\n\nIntelligence vs. Artificial Intelligence\n\nDefinition\n\nIntelligence includes the capacity for logic, understanding, learning, reasoning, creativity, and problem solving, etc.\nArtificial Intelligence (AI) attempts not just to understand (scientific goal) but also to build (engineering goal) intelligent entities.\n\n\n\n\nThe field of Artificial Intelligence\n\nAI is one of the newest fields in science and engineering.\n\nWork started in earnest soon after World War II, and the name itself was coined at a conference at Dartmouth College in 1956.\n\nAI research aims to build intelligent entities that are capable of simulating humans in different aspects.\n\nThinking: learning, planning, knowledge refinement\nPerception: see, hear, feel, etc.\nCommunication in natural languages\nManipulation and moving objects\n\n\n\n\nWhat is AI\n\n\n\n\n\n\n\nThinking Humanly\nThinking rationally\n\n\n\n\n“The exciting new effort to make computers think … machines with minds, in the full and literal sense.” (Haugeland, 1985)\n“The study of mental faculties through the use of computational models.” (Charniak and McDermott, 1985)\n\n\n“The automation of activities that we associate with human thinking, activities such as decision-making, problem solving, learning …” (Bellman, 1978)\n“The study of the computations that make it possible to perceive, reason, and act.” (Winston, 1992)\n\n\nActing Humanly\nActing rationally\n\n\n“The art of creating machines that per- form functions that require intelligence when performed by people.” (Kurzweil, 1990)\n“Computational Intelligence is the study of the design of intelligent agents.” (Poole et al., 1998)\n\n\n“The study of how to make computers do things at which, at the moment, people are better.” (Rich and Knight, 1991)\n“AI …is concerned with intelligent behavior in artifacts.” (Nilsson, 1998)\n\n\n\n\n\n\nSystems that think like humans\nSystems that think rationally\n\n\nSystems that act like humans\nSystems that act rationally\n\n\n\n\n\nActing humanly\n\nThe Turing Test approach\nTuring (1950) “Computing machinery and intelligence”\n\n“Can machines think?” \\longrightarrow “Can machines behave intelligently?”\nOperational test for intelligent behavior: the Imitation Game. A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer\n\nProblem: Turing test is not reproducible, constructive, or amenable to mathematical analysis\n\n\n\n\nThinking humanly\n\nThe cognitive modeling approach\nRequires scientific theories of internal activities of the brain to get inside the actual workings of human minds\n\nWhat level of abstraction? “Knowledge” or “circuits”?\n\nHow to validate?\n\nPredicting and testing behavior of human subjects (top-down) or\nDirect identification from neurological data (bottom-up)\n\nThese approaches (Cognitive Science and Cognitive Neuroscience) are now distinct from AI\n\nShare that the available theories but do not explain anything resembling human intelligence.\nAll share a principal direction.\n\n\n\n\nThinking rationally\n\nThe “laws of thought” approach\n“Right thinking” is irrefutable reasoning processes\nBased on logic: notation and rules of derivation for thoughts; may or may not have proceeded to the idea of mechanization\nProblems:\n\nNot all intelligent behavior is mediated by logical deliberation\nSolving a problem “in principle” is different from solving it in practice\n\n\n\n\nActing rationally\n\nThe rational agent approach\nRational behavior is doing the right thing\n\nThe right thing which is expected to maximize goal achievement, given the available information\nDoesn’t necessarily involve thinking – e.g., blinking reflex – but thinking should be in the service of rational action\n\nAn agent is an entity that perceives and acts. Abstractly, an agent is a function from percept histories to actions\n\nf:\\mathcal{P}^{*} \\rightarrow \\mathcal{A}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "ai-01-introduction.html#foundations-of-ai",
    "href": "ai-01-introduction.html#foundations-of-ai",
    "title": "1  Introduction to Artificial Intelligence",
    "section": "1.3 Foundations of AI",
    "text": "1.3 Foundations of AI\n\nDisciplines to contribute ideas, viewpoints, and techniques to AI\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nPhilosophy\nLogic, methods of reasoning, mind as physical system, foundations of learning, language, rationality\n\n\nMathematics\nFormal representation and proof, algorithms, computation, (un)decidability, (in)tractability, probability\n\n\nEconomics\nUtility, decision theory, rational economic agents\n\n\nNeuroscience\nNeurons as information processing units\n\n\nPsychology\nHow do people behave, perceive, process information, represent knowledge.\n\n\nComputer Engineering\nBuilding fast computers\n\n\nControl theory and cybernetics\nDesign systems that maximize an objective function over time\n\n\nLinguistic\nKnowledge representation, grammar",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "ai-01-introduction.html#history-of-ai",
    "href": "ai-01-introduction.html#history-of-ai",
    "title": "1  Introduction to Artificial Intelligence",
    "section": "1.4 History of AI",
    "text": "1.4 History of AI\n\nA brief history of AI\n\n1940 – 1950: Early days\n\n1943: McCulloch & Pitts: Boolean circuit model of brain\n1950: Turing’s “Computing Machinery and Intelligence”\n\n1950 – 1970: Excitement: Look, Ma, no hands\n\n1950s: Early AI programs, including Samuel’s checkers program, Newell & Simon’s Logic Theorist, Gelernter’s Geometry Engine\n1956: Dartmouth meeting: “Artificial Intelligence” adopted\n1965: Robinson’s complete algorithm for logical reasoning\n\n1970 – 1990: Knowledge-based approaches\n\n1969 – 1980: Early development of knowledge-based systems\n1980 – 1988: Expert systems industry booms\n1988 – 1993: Expert systems industry busts: “AI Winter”\n\n1990 – 2010: Statistical approaches\n\nResurgence of probability, focus on uncertainty\nGeneral increase in technical depth\nAgents and learning systems… “AI Spring”?\n\n2010 – present: Deep learning and where are we now?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "ai-01-introduction.html#course-topics",
    "href": "ai-01-introduction.html#course-topics",
    "title": "1  Introduction to Artificial Intelligence",
    "section": "1.5 Course Topics",
    "text": "1.5 Course Topics\n\nSolving problems by searching\n\nSearch is the fundamental technique of AI.\n\nPossible answers, decisions or courses of action are structured into an abstract space, which we then search.\n\nSearch is either uninformed or informed\n\nUninformed: we move through the space without worrying about what is coming next, but recognizing the answer if we see it\nInformed: we guess what is ahead, and use that information to decide where to look next.\n\nWe may want to search for the first answer that satisfies our goal, or keep searching until we find the best answer.\n\n\n\nKnowledge and reasoning\n\nThe second most important concept in AI\nIf we are going to act rationally in our environment, then we must have some way of describing that environment and drawing inferences from that representation.\n\nHow do we describe what we know about the world?\nHow do we describe it concisely?\nHow do we describe it so that we can get hold of the right piece of knowledge when we need it?\nHow do we generate new pieces of knowledge?\nHow do we deal with uncertain knowledge?\n\n\n\n\nMachine learning\n\nIf a system is going to act truly appropriately, then it must be able to change its actions in the light of experience.\n\nHow do we generate new facts from old?\nHow do we generate new concepts?\nHow do we learn to distinguish different situations in new environments?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "ai-01-introduction.html#references",
    "href": "ai-01-introduction.html#references",
    "title": "1  Introduction to Artificial Intelligence",
    "section": "1.6 References",
    "text": "1.6 References",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "ai-02-agent.html",
    "href": "ai-02-agent.html",
    "title": "2  Intelligent Agents",
    "section": "",
    "text": "2.1 Agents and Environments",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intelligent Agents</span>"
    ]
  },
  {
    "objectID": "ai-02-agent.html#agents-and-environments",
    "href": "ai-02-agent.html#agents-and-environments",
    "title": "2  Intelligent Agents",
    "section": "",
    "text": "What is Agent\n\nAn agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators\n\n\nAgents interact with environments through sensors and actuators.\n\nOrganizations Microsoft, European Union, Real Madrid FC, an ant colony,…\nPeople teacher, physician, stock trader, engineer, researcher, travel agent, farmer, waiter…\nComputers/devices thermostat, user interface, airplane controller, network controller, game, advising system, tutoring system, diagnostic assistant, robot, Google car, Mars rover…\nAnimals dog, mouse, bird, insect, worm, bacterium, bacteria…\n\n\n\nPercept to refer to the agent’s perceptual inputs at any given instant.\nAction to refer to agent’s behavior.\nPercept sequence is a sequence of all past and present percepts the agent has ever perceived.\nAn action is described by the agent function that maps any given percept sequence to an action f:P^{*}\\to A\nAgent program: the implementation of the agent function  agent=architecture+program \n\n\n\nVacuum-cleaner\n\n\nPercepts: location and contents, e.g., [A,Dirty]\nActions: Left, Right, Suck, NoOp\nFunction:\n\n\n\nPercept sequence\nAction\n\n\n\n\n[A,Clean]\nRight\n\n\n[A,Dirty]\nSuck\n\n\n[B,Clean]\nLeft\n\n\n[B,Dirty]\nSuck\n\n\n[A,Clean],[A,Clean]\nRight\n\n\n\\vdots\n\\vdots\n\n\n\n\nfunction Reflex-Vaccum-Agent([location,status]) returns an action\n\n    if status = Dirty then return Suck\n    else if location = A then return Right\n    else if location = B then return Left\n\nWhat is the right function?\nCan it be implemented in a small agent program?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intelligent Agents</span>"
    ]
  },
  {
    "objectID": "ai-02-agent.html#good-behavior-the-concept-of-rationality",
    "href": "ai-02-agent.html#good-behavior-the-concept-of-rationality",
    "title": "2  Intelligent Agents",
    "section": "2.2 Good Behavior: The Concept of Rationality",
    "text": "2.2 Good Behavior: The Concept of Rationality\n\nRationality\n\nA rational agent is one that does the right thing\nWhat is right thing?\n\nWhen an agent is plunked down in an environment, it generates a sequence of actions according to the percepts it receives. This sequence of actions causes the environment to go through a sequence of states. If the sequence is desirable, then the agent has performed well\nPerformance measure evaluates any given sequence of environment states (not agent states)\n\n\n\nA rule of thumb\nIt is better to design performance measures according to what one actually wants in the environment, rather than according to how one thinks the agent should behave\n\nRational depends on four things:\n\nThe performance measure that defines the criterion of success.\nThe agent’s prior knowledge of the environment.\nThe actions that the agent can perform.\nThe agent’s percept sequence to date.\n\n\nDefinition of a rational agent\nFor each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.\n\n\n\nOmniscience, learning, and autonomy\n\nRational \\neq omniscient, percepts may not supply all relevant information\nRational \\neq clairvoyant, action outcomes may not be as expected\nHence, rational \\neq successful\nRational \\implies exploration, learning, autonomy\n\n\nOmniscience vs. Rationality\n\n\n\n\n\n\n\nOmniscience\nRationality\n\n\n\n\nKnows the actual outcome of its actions in advance\nRationality maximizes expected performance, while perfection maximizes actual performance\n\n\nPerfection but not practical\n\n\n\n\n\n\nInformation gathering\n\nInformation gathering by doing actions in order to modify future percepts or exploration\nThis is an important part of rationality\n\n\n\nLearning\n\nA rational agent also has to learn as much as possible from what it perceives.\n\nThe agent’s initial configuration may be modified and augmented as it gains experience.\n\nThere are extreme cases in which the environment is completely known a priori.\n\n\n\nAutonomy\n\nA rational agent should be autonomous – Learn what it can to compensate for partial or incorrect prior knowledge.\n\nIf an agent just relies on the prior knowledge of its designer rather than its own percepts then the agent lacks autonomy",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intelligent Agents</span>"
    ]
  },
  {
    "objectID": "ai-02-agent.html#the-nature-of-environments",
    "href": "ai-02-agent.html#the-nature-of-environments",
    "title": "2  Intelligent Agents",
    "section": "2.3 The Nature of Environments",
    "text": "2.3 The Nature of Environments\n\nThe task environment\n\nTask environments are essentially the “problems” to which rational agents are the “solutions”\n\nThe flavor of the task environment directly affects the appropriate design for the agent program\n\nTask environment includes the PEAS (Performance, Environment, Actuators, Sensors) description\nIn designing an agent, the first step must always be to specify the task environment as fully as possible.\n\n\n\nAn example: Automated taxi driver\n\n\nPerformance measure\n\nsafety, destination, profits, legality, comfort, \\ldots\n\nEnvironment\n\nstreets/freeways, traffic, pedestrians, weather, \\ldots\n\nActuators\n\nsteering, accelerator, brake, horn, speaker/display, \\ldots\n\nSensors\n\nvideo, accelerometers, gauges, engine sensors, keyboard, GPS, \\ldots\n\n\n\n\nSoftware agents\n\nSometimes, the environment may not be the real world.\n\nFlight simulator, video games, Internet\nThey are all artificial but very complex environments\n\nThose agents working in these environments are called software agents (softbots).\n\nAll parts of the agent are software.\n\n\n\n\nProperties of task environments\n\nFully observable vs. partially observable\nSingle agent vs. multiagent\nDeterministic vs. stochastic\nEpisodic vs. sequential\nDiscrete vs. continuous\nStatic vs. dynamic\nKnown vs. unknown\n\n\n\nFully observable vs. partially observable\n\nFully observable: The agent’s sensory gives it access to the complete state of the environment.\n\nThe agent need not maintain internal state to keep track of the world.\n\nPartially observable\n\nNoisy and inaccurate sensors\nParts of the state are simply missing from the sensor data\n\nUnobservable: The agent has no sensors at all\n\n\n\n\nSingle agent vs. multiagent\n\nSingle agent: An agent operates by itself in an environment.\n\nSolving crossword \\rightarrow single agent, playing chess \\rightarrow two agents\n\nWhich entities must be viewed as agents?\nCompetitive vs. Cooperative multiagent environment\n\nPlaying chess \\rightarrow competitive, driving on road \\rightarrow cooperative\n\n\n\n\n\nDeterministic vs. stochastic\n\nDeterministic: The next state of the environment is completely determined by the current state and the action executed by the agent.\n\nThe vacuum world \\rightarrow deterministic, driving on road \\rightarrow stochastic\n\nMost real situations are so complex that they must be treated as stochastic.\n\n\n\n\nEpisodic vs. sequential\n\nEpisodic: The agent’s experience is divided into atomic episodes, in each of which the agent receives a percept and then performs a single action\nSequential: A current decision could affect future decisions\n\n\n\n\nDiscrete vs. continuous\n\nThe discrete/continuous distinction applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agent\n\n\n\n\nStatic vs. dynamic\n\nStatic: The environment is unchanged while an agent is deliberating.\n\nCrossword puzzles \\rightarrow static, taxi driving \\rightarrow dynamic\n\nSemidynamic: The environment itself does not change with the passage of time but the agent’s performance score does\n\nChess playing with a clock\n\n\n\n\n\nKnown vs. unknown\n\nKnown environment: the outcomes (or outcome probabilities if the environment is stochastic) for all actions are given.\nUnknown environment: the agent needs to learn how it works to make good decisions.\n\n\n\n\nExamples of different environments",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intelligent Agents</span>"
    ]
  },
  {
    "objectID": "ai-02-agent.html#the-structure-of-agents",
    "href": "ai-02-agent.html#the-structure-of-agents",
    "title": "2  Intelligent Agents",
    "section": "2.4 The Structure of Agents",
    "text": "2.4 The Structure of Agents\n\nThe Structure of Agents\n agent=architecture+program \n\nArchitecture: some sort of computing device with physical sensors and actuators that this program will run on.\n\nOrdinary PC, robotic car with several onboard computers, cameras, and other sensors, etc.\n\nProgram has to be appropriate for the architecture.\n\nProgram: Walk action \\rightarrow Architecture: legs\n\n\n\n\nThe agent programs\n\nA trivial agent program: keep track of the percept sequence and index into a table of actions to decide what to do.\n\nfunction Table-Driven-Agent(percept) returns an action\n\npersistent: \n        percepts: a sequence, initially empty\n        table: a table of actions,\n               indexed by percept sequences,\n               initially fully specified\n\n    append percept to the end of percepts\n    action ← Lookup(percepts, table)\n    return action\n\nThe table-driven approach to agent construction is doomed to failure\nLet P be the set of possible percepts and\nLet T be the lifetime of the agent (the total number of percepts it will receive).\nThe lookup table will contain \\sum_{t=1}^{T}\\left|P\\right|^{t} entries \\to very huge table\n\n\n\nAgent types\nFour basic types in order of increasing generality:\n\nSimple reflex agents\nModel-based reflex agents\nGoal-based agents\nUtility-based agents\n\n\n\nSimple reflex agents\n\nThe simplest kind of agent, but of limited intelligence\nSelect actions based on the current percept, ignoring the rest of the percept history\nThe connection from percept to action is represented by condition-action rules.\nIF current percept THEN action\nLimitations\n\nKnowledge sometimes cannot be stated explicitly \\rightarrow low applicability\nWork only if the environment is fully observable\n\n\n\nfunction Simple-Reflex-Agent(percept) returns an action\n\npersistent: \n        rules: a set of condition-action rules\n\n    state ← Interpret-Input(percept)\n    rule ← Rule-Match(state, rules)\n    action ← rule.Action\n    return action\n\n\nModel-based reflex agents\n\nPartially observability \\rightarrow the agent has to keep track of an internal state\n\nDepend on the percept history and reflect some of the unobserved aspects\n\nThe agent program updates the internal state information as time goes by by encoding two kinds of knowledge\n\nHow the world evolves independently of the agent\nHow the agent’s actions affect the world\n\n\n\nfunction Model-Based-Reflex-Agent(percept) returns an action\n\npersistent: \n        state: the agent's current conception of the world state\n        model: a description of how the next state depends on current state and action\n        rules: a set of condition-action rules\n        action: the most recent action, initially none\n\n    state ← Update-State(state, action, percept, model)\n    rule ← Rule-Match(state, rules)\n    action ← rule.action\n    return action\n\n\nGoal-based agents\n\nCurrent state of the environment is always not enough\nThe agent further needs some sort of goal information that describes situations that are desirable.\nLess efficient but more flexible\n\n\n\n\nUtility-based agents\n\nGoals alone are not enough to generate high-quality behavior in most environments\nMany action sequences to get the goals, some are better and some worse\nAn agent’s utility function is essentially an internalization of the performance measure.\n\nGoal \\rightarrow success, utility \\rightarrow degree of success (how successful it is)\n\n\n\n\n\nLearning agents\nA learning agent is divided into four conceptual components\n\nLearning element \\rightarrow Making improvement\nPerformance element \\rightarrow Selecting external actions\nCritic \\rightarrow Tells the Learning element how well the agent is doing with respect to fixed performance standard. (Feedback from user or examples, good or not?)\nProblem generator \\rightarrow Suggest actions that will lead to new and informative experiences\n\n\n\n\nComponent representations\n\nThree basic representations: atomic, factored, and structured\n\n\nThree ways to represent states and the transitions between them. (a) Atomic representation: a state (such as B or C) is a black box with no internal structure; (b) Factored representation: a state consists of a vector of attribute values; values can be Boolean, real-valued, or one of a fixed set of symbols. (c) Structured representation: a state includes objects, each of which may have attributes of its own as well as relationships to other objects.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intelligent Agents</span>"
    ]
  },
  {
    "objectID": "ai-02-agent.html#references",
    "href": "ai-02-agent.html#references",
    "title": "2  Intelligent Agents",
    "section": "2.5 References",
    "text": "2.5 References",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intelligent Agents</span>"
    ]
  },
  {
    "objectID": "ai-03-search-revised.html",
    "href": "ai-03-search-revised.html",
    "title": "3  Solving Problems By Searching",
    "section": "",
    "text": "3.1 Problem-solving Agents",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Solving Problems By Searching</span>"
    ]
  },
  {
    "objectID": "ai-03-search-revised.html#problem-solving-agents",
    "href": "ai-03-search-revised.html#problem-solving-agents",
    "title": "3  Solving Problems By Searching",
    "section": "",
    "text": "Problem-solving agents\n\n\nA problem-solving agent is a specialized type of goal-based agent.\nIt uses atomic representations for the state space.\n\n\n\n\nExample: Romania travelling\n\nYou are currently located in Arad.\nYou must attend a friend’s wedding in Bucharest tomorrow.\n\n\n\n\nProperties of the environment\n\nObservable\n\nThe agent always knows the current state (e.g., each city has a sign indicating its presence).\n\nDiscrete\n\nThere are a finite number of actions available from any given state (e.g., each city connects to a small number of other cities).\n\nKnown\n\nThe agent understands the consequences of each action (i.e., which states are reached).\n\nDeterministic\n\nEach action results in exactly one outcome.\nUnder these assumptions, the solution to any problem is a fixed sequence of actions.\n\n\n\n\nSolving problems by searching\n\nSearch: The process of looking for a sequence of actions that reaches the goal.\n\nA search algorithm takes a problem as input and returns a solution in the form of an action sequence.\n\nExecution phase: Once a solution is found, the recommended actions are carried out.\n\nWhile executing the solution, the agent ignores its percepts when choosing an action, operating as an open-loop system.\n\n\nfunction Simple-Problem-Solving-Agent(percept) returns an action\n\npersistent: seq: an action sequence, initially empty\n           state: some description of the current world state\n           goal: a goal, initially null\n           problem: a problem formulation\n\n  state ← Update-State(state, percept)\n\n  if seq is empty then\n    goal ← Formulate-Goal(state)\n    problem ← Formulate-Problem(state, goal)\n    seq ← Search(problem)\n    if seq = failure then return a null action\n  \n  action ← First(seq)\n  seq ← Rest(seq)\n\nreturn action\n\n\nWell-defined problems and solutions\nA problem can be formally defined by five components:\n\nThe initial state that the agent starts in.\n\nFor example, the initial state for our agent in Romania might be described as In(Arad).\n\nA description of the possible actions available to the agent.\n\nFor example, Action(In(Arad)) = {Go(Sibiu), Go(Timisoara), Go(Zerind)}.\n\nThe transition model, which describes what each action does.\n\nFor example, Result(In(Arad), Go(Zerind)) \\to In(Zerind).\nWe use the term successor to refer to any state reachable from a given state by a single action.\nThe initial state, actions, and transition model implicitly define the state space of the problem.\nThe state space forms a directed network or graph in which the nodes are states and the links between nodes are actions.\nA path in the state space is a sequence of states connected by a sequence of actions.\n\nThe goal test, which determines whether a given state is a goal state.\n\nFor example, the agent’s goal in Romania is the singleton set {In(Bucharest)}.\n\nA path cost function that assigns a numeric cost to each path.\n\nThe step cost of taking action a in state s to reach state s’ is denoted by c(s, a, s’).\n\n\nA solution to a problem is an action sequence that leads from the initial state to a goal state.\n\nAn optimal solution has the lowest path cost among all solutions.\n\n\n\nFormulating problems by abstraction\n\nAbstraction is the process of removing detail from a representation, specifically by simplifying the state description and the actions.\nAbstraction is critical for automated problem solving because:\n\nThe real world is often too detailed to model exactly.\nWe must create an approximate, simplified model of the world for the computer to process.\n\nThe choice of a good abstraction involves:\n\nRemoving as much detail as possible, while\nRetaining validity and ensuring that the abstract actions are feasible to carry out.\n\n\n\n\nDirected Graphs\n\nA directed graph G=(V,E) consists of a set V of nodes and a set E of ordered pairs of nodes, called arcs.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Solving Problems By Searching</span>"
    ]
  },
  {
    "objectID": "ai-03-search-revised.html#example-problems",
    "href": "ai-03-search-revised.html#example-problems",
    "title": "3  Solving Problems By Searching",
    "section": "3.2 Example Problems",
    "text": "3.2 Example Problems\n\nToy problems vs. Real-world problems\n\n\n\n\n\n\n\nToy problems\nReal-world problems\n\n\n\n\nIllustrate or exercise various problem- solving methods\nMore difficult\n\n\nConcise, exact description\nNo single, agreed-upon description\n\n\nCan be used to compare performance\n\n\n\nE.g., 8-puzzle, 8-queens problem, Cryptarithmetic, Vacuum world, Missionaries and cannibals, simple route finding\nE.g., Route finding, Touring and traveling salesperson problems, VLSI layout, Robot navigation, Assembly sequencing\n\n\n\n\n\nThe vacuum world\nStates: The state is determined by both the agent’s location and the dirt locations. Thus, there are 2\\times2^{2}=8 possible world states (n\\times2^{n} states in general).\n\nInitial state: Any state can be designated as the initial state.\nActions: In this simple environment, each state has just three actions: Left, Right, and Suck. Larger environments might also include Up and Down.\nGoal test: This checks whether all the squares are clean.\nPath cost: Each step costs 1.\nTransition model: State space.\n\n\n\n\nThe 8-puzzle\nStates: A state description specifies the location of each of the eight tiles and the blank space in one of the nine squares.\n\nInitial state: Any state can be designated as the initial state.\nActions:\n\nLeft, Right, Up, or Down.\nDifferent subsets of these are possible depending on the position of the blank space.\n\nTransition model: Given a state and action, this returns the resulting state.\nGoal test: This checks whether the state matches the goal configuration.\nPath cost: Each step costs 1.\n\n\n\nIt is a member of the family of sliding-block puzzles and is NP-complete.\n8-puzzle: 9!/2 = 181,440 reachable states \\rightarrow easily solved.\n15-puzzle (on a 4\\times4 board): 1.3 trillion states \\rightarrow optimally solved in a few milliseconds.\n24-puzzle (on a 5\\times5 board): around 10^{25} states \\rightarrow optimally solved in several hours.\n\n\n\nThe 8-queens\nThere are two main approaches to formulation:\n\nAn incremental formulation involves operators that augment the state description, starting with an empty state; for the 8-queens problem, each action adds a queen to the board.\nA complete-state formulation starts with all 8 queens on the board and moves them around.\n\n\nIncremental formulation:\n\nStates: Any arrangement of 0 to 8 queens on the board constitutes a state.\n\nThe number of states is 64\\times63\\ldots57\\approx1.8\\times10^{14}.\n\nInitial state: No queens on the board.\nActions: Add a queen to any empty square.\nTransition model: Returns the board with a queen added to the specified square.\nGoal test: 8 queens are on the board, with none attacking another.\nPath cost: Irrelevant (we are only interested in the final state).\n\n\n\nPacman\n\n\n\nKnuth’s 4 problem\nDevised by Donald Knuth (1964).\n\nThis illustrates how infinite state spaces can arise.\nKnuth’s conjecture: Starting with the number 4, a sequence of factorial \\cdot!, square root \\sqrt{\\cdot}, and floor \\lfloor \\cdot\\rfloor operations can reach any desired positive integer.\n\n \\left\\lfloor \\sqrt{\\sqrt{\\sqrt{\\sqrt{\\sqrt{(4!)!}}}}}\\right\\rfloor =5 \nStates: Any positive number.\n\nInitial state: 4.\nActions: Apply factorial, square root, or floor operation.\nTransition model: Defined by the mathematical definitions of the operations (factorial applies to integers only).\nGoal test: The state matches the desired positive integer.\n\n\n\nThe route-finding problem\nConsider the airline travel problems solved by a travel-planning website.\nStates: Each state includes a location (e.g., an airport) and the current time.\n\nInitial state: Specified by the user’s query.\nActions: Take any flight from the current location, in any seat class, leaving after the current time, provided there is enough time for within-airport transfer.\nTransition model: The state resulting from taking a flight has the flight’s destination as the current location and the flight’s arrival time as the current time.\nGoal test: Are we at the final destination specified by the user?\nPath cost: This depends on monetary cost, waiting time, flight time, customs and immigration procedures, seat quality, time of day, airplane type, frequent-flyer mileage awards, etc.\n\n\n\nExample: robotic assembly\n\n\nStates: Real-valued coordinates of robot joint angles and parts of the object to be assembled.\nActions: Continuous motions of robot joints.\nGoal test: Complete assembly with no robot included!\nPath cost: Time required to execute.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Solving Problems By Searching</span>"
    ]
  },
  {
    "objectID": "ai-03-search-revised.html#searching-for-solutions",
    "href": "ai-03-search-revised.html#searching-for-solutions",
    "title": "3  Solving Problems By Searching",
    "section": "3.3 Searching for Solutions",
    "text": "3.3 Searching for Solutions\n\nSearch tree\nSince a solution is an action sequence, search algorithms work by considering various possible action sequences.\n\nSearch tree: This represents the possible action sequences starting at the initial state.\n\nThe branches correspond to actions, and the nodes correspond to states in the state space of the problem.\nThe root node of the tree corresponds to the initial state.\nActions are taken by expanding the current state (parent node), thereby generating a new set of states (child nodes).\nFrontier: The set of all leaf nodes available for expansion at any given point.\n\n\n\nSearch algorithms all share this basic structure; they vary primarily according to how they choose which state to expand next—the so-called search strategy.\n\n\nPartial search trees for finding a route from Arad to Bucharest. Nodes that have been expanded are shaded; nodes that have been generated but not yet expanded are outlined in bold; nodes that have not yet been generated are shown in faint dashed lines.\nfunction Tree-Search(problem) returns a solution, or failure\n\n  initialize the frontier using the initial state of problem\n\n  loop do\n    if the frontier is empty then\n      return failure\n    choose a leaf node and remove it from the frontier\n    if the node contains a goal state then\n      return the corresponding solution\n    expand the chosen node, adding the resulting nodes to the frontier\n\n\nRedundant paths\n\nLoopy paths cause repeated states in the search tree.\n\n\n\nRedundant paths (a general concept) are unavoidable and exist whenever there is more than one way to get from one state to another.\nFollowing redundant paths can cause a tractable problem to become intractable.\nThis is true even for algorithms that know how to avoid infinite loops.\n\n\n\nGraph search algorithm\n\nAlgorithms that forget their history are doomed to repeat it.\nTo avoid redundancy, we use a data structure called the explored set, which remembers every expanded node.\n\nfunction Graph-Search(problem) returns a solution, or failure\n\n  initialize the frontier using the initial state of problem\n  initialize the explored set to be empty\n\n  loop do\n    if the frontier is empty then\n      return failure\n    choose a leaf node and remove it from the frontier\n    if the node contains a goal state then\n      return the corresponding solution\n    add the node to the explored set\n    expand the chosen node,\n      adding the resulting nodes to the frontier\n      only if not in the frontier or explored set\n\nThe algorithm has another important property: the frontier separates the state-space graph into the explored region and the unexplored region. Every path from the initial state to an unexplored state must pass through a state in the frontier.\n\n\nThe frontier (white nodes) always separates the explored region of the state space (black nodes) from the unexplored region (gray nodes).\n\n\nInfrastructure for search algorithms\nSearch algorithms require a data structure to track the search tree being constructed. For each node n of the tree, we use a structure containing four components:\n\nn.State: The state in the state space to which the node corresponds.\nn.Parent: The node in the search tree that generated this node.\nn.Action: The action that was applied to the parent to generate the node.\nn.PathCost: The cost, traditionally denoted by g(n), of the path from the initial state to the node, as indicated by the parent pointers.\n\n\nfunction Child-Node(problem, parent, action) returns a node\n\n  return a node with\n    State ← problem.Result(parent.State, action),\n    Parent ← parent,\n    Action ← action,\n    Path-Cost ← parent.Path-Cost + problem.Step-Cost(parent.State, action)\n\n\nMeasuring problem-solving performance\nWe can evaluate an algorithm’s performance in four ways:\n\nCompleteness: Is the algorithm guaranteed to find a solution when there is one?\nOptimality: Does the strategy find the optimal solution?\nTime complexity: How long does it take to find a solution?\nSpace complexity: How much memory is needed to perform the search?\n\nTime and space complexity are measured in terms of:\n\nb: Maximum branching factor of the search tree.\nd: Depth of the least-cost/shallowest solution.\nm: Maximum depth of the state space (may be \\infty).",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Solving Problems By Searching</span>"
    ]
  },
  {
    "objectID": "ai-03-search-revised.html#references",
    "href": "ai-03-search-revised.html#references",
    "title": "3  Solving Problems By Searching",
    "section": "3.4 References",
    "text": "3.4 References",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Solving Problems By Searching</span>"
    ]
  },
  {
    "objectID": "ai-04-uninformed-revised.html",
    "href": "ai-04-uninformed-revised.html",
    "title": "4  Uninformed search strategies",
    "section": "",
    "text": "4.1 Breadth-first search",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Uninformed search strategies</span>"
    ]
  },
  {
    "objectID": "ai-04-uninformed-revised.html#breadth-first-search",
    "href": "ai-04-uninformed-revised.html#breadth-first-search",
    "title": "4  Uninformed search strategies",
    "section": "",
    "text": "Breadth-first search\n\nBreadth-first search (BFS) is a simple strategy in which the root node is expanded first, followed by all successors of the root node, then their successors, and so on.\nIn generally, all nodes at a given depth in the search tree are expanded before any nodes at the next level.\nBreadth-first search on a simple binary tree. At each stage, the node to be expanded next is indicated by a marker.\n\n\n\n\nAlgorithm\n\nThe frontier is implemented as a FIFO queue.\n\nfunction Breadth-First-Search(problem) returns a solution, or failure\n\n  node ← a node with State = problem.Initial-State, Path-Cost = 0\n  if problem.Goal-Test(node.State) then return Solution(node)\n  frontier ← node\n  explored ← ⌀\n  loop do\n    if frontier = ⌀ then return failure\n    node ← Pop(frontier) \n    add node.State to explored\n    for each action in problem.Actions(node.State) do\n      child ← Child-Node(problem,node,action)\n      if child.State is not in explored or frontier then\n        if problem.Goal-Test(child.State) then return Solution(child)\n        frontier ← Insert(child,frontier)\n\n\nProperties of breadth-first search\n\nTime: 1+b+b^{2}+b^{3}+\\ldots+b^{d}=O(b^{d})\nSpace: O(b^{d}) (O(b^{d-1}) for explored and O(b^{d}) for frontier)\nComplete: Yes (if b is finite)\nOptimal: Not optimal in general (unless step costs are identical)\n\n\n\n\nDepth\nNodes\nTime\nMemory\n\n\n\n\n2\n110\n.11 milliseconds\n107 kilobytes\n\n\n4\n11,110\n11 milliseconds\n10.6 megabytes\n\n\n6\n10^{6}\n1.1 seconds\n1 gigabyte\n\n\n8\n10^{8}\n2 minutes\n103 gigabytes\n\n\n10\n10^{10}\n3 hours\n10 terabytes\n\n\n12\n10^{12}\n13 days\n1 petabyte\n\n\n14\n10^{14}\n3.5 years\n99 petabytes\n\n\n16\n10^{16}\n350 years\n10 exabytes\n\n\n\nTime and memory requirements for breadth-first search. The numbers shown assume branching factor b = 10; 1 million nodes/second; 1000 bytes/node.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Uninformed search strategies</span>"
    ]
  },
  {
    "objectID": "ai-04-uninformed-revised.html#uniform-cost-search",
    "href": "ai-04-uninformed-revised.html#uniform-cost-search",
    "title": "4  Uninformed search strategies",
    "section": "4.2 Uniform-cost search",
    "text": "4.2 Uniform-cost search\n\nUniform-cost search\n\nUniform-cost search (UCS) expands the node n with the lowest path cost g(n).\nImplementation: The frontier is a priority queue ordered by g.\n\nThis is equivalent to Dijkstra’s algorithm.\n\nThe goal test is applied to a node only when it is selected for expansion.\nA test is added in case a better path is found to a node currently on the frontier.\n\n\n\nAlgorithm\n\nThe frontier is a priority queue.\n\nfunction Uniform-Cost-Search(problem) returns a solution, or failure\n\n  node ← a node with State = problem.Initial-State, Path-Cost = 0\n  frontier ← node // a priority queue ordered by Path-Cost\n  explored ← ⌀\n  loop do\n    if frontier = ⌀ then return failure\n    node ← Pop(frontier) // chooses the lowest-cost node in frontier\n    if problem.Goal-Test(node.State) then return Solution(node)\n    add node.State to explored\n    for each action in problem.Actions(node.State) do\n      child ← Child-Node(problem,node,action)\n      if child.State is not in explored or frontier then        \n        frontier ← Insert(child,frontier)\n      else if child.State is in frontier with higher Path-Cost then\n        replace that frontier node with child\n\nA critical question arises: Should we apply the goal test when we enqueue a node or when we dequeue it?\nFind a path from S to G\n\n\n\n\nProperties of uniform-cost search\n\nTime: O(b^{1+\\left\\lfloor \\frac{C^{*}}{\\epsilon}\\right\\rfloor }) where C^{*} is the cost of the optimal solution\nSpace: O(b^{1+\\left\\lfloor\\frac{C^{*}}{\\epsilon}\\right\\rfloor})\nComplete: Yes, if step cost \\geq\\epsilon (where \\epsilon is a small positive constant)\nOptimal: Yes, nodes are expanded in increasing order of g(n)\n\nProof (by contradiction)\n\nSuppose UCS terminates at a goal state n with a path cost g(n)=C.\nIf C is not the optimal value, then there exists another unexplored goal state n' with g(n')&lt;C.\nTherefore, there must exist a node n'' on the frontier that is on the optimal path to n' (graph separation property).\nHowever, g(n'')&lt;g(n')&lt;g(n) implies that n'' must be expanded before n, which is a contradiction.\n\n\n\nIlustration\n\nFind a shortest path from a to f\n\nStep-by-step execution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnode\nfrontier\n\n\n\n\n\na(0;null)\n\n\na(0;null)\nb(9;a) d(3;a) g(6;a)\n\n\nd(3;a)\nb(7;d) g(6;a) c(4;d) e(11;d)\n\n\nc(4;d)\nb(7;d) g(6;a) e(9;c)\n\n\ng(6;a)\nb(7;d) e(9;c)\n\n\nb(7;d)\ne(9;c)\n\n\ne(9;c)\nf(26;e)\n\n\nf(26;e)\n\\emptyset",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Uninformed search strategies</span>"
    ]
  },
  {
    "objectID": "ai-04-uninformed-revised.html#depth-first-search",
    "href": "ai-04-uninformed-revised.html#depth-first-search",
    "title": "4  Uninformed search strategies",
    "section": "4.3 Depth-first search",
    "text": "4.3 Depth-first search\n\nDepth-first search\n\nDepth-first search (DFS) expands the deepest unexpanded node first.\nImplementation: The frontier is a LIFO (Last-In-First-Out) Stack.\nDepth-first search on a binary tree. The unexplored region is shown in light gray. Explored nodes with no descendants in the frontier are removed from memory. Nodes at depth 3 have no successors and M is the only goal node.\n\n\n\n\nProperties of DFS\n\nTime: O(b^{m})\n\nPerformance is poor if m is significantly larger than d, but if solutions are dense, it may be much faster than breadth-first search.\n\nSpace: O(bm), i.e., linear space!\nComplete:\n\nNo in infinite-depth spaces.\nYes in finite spaces.\n\nOptimal: No, it finds the “leftmost” solution, regardless of depth or cost.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Uninformed search strategies</span>"
    ]
  },
  {
    "objectID": "ai-04-uninformed-revised.html#depth-limited-search",
    "href": "ai-04-uninformed-revised.html#depth-limited-search",
    "title": "4  Uninformed search strategies",
    "section": "4.4 Depth-limited search",
    "text": "4.4 Depth-limited search\n\nDepth-limited Search\n\nDepth-limited Search (DLS) is a standard DFS with a predetermined depth limit l; nodes at depth l are treated as if they have no successors.\n\nThis allows infinite problems to be solved (by imposing a finite limit).\n\nDepth limits can be chosen based on prior knowledge of the problem.\n\n\n\nAlgorithm\n\nA recursive implementation of depth-limited tree search.\n\nfunction Depth-Limited-Search(problem,limit) returns a solution, or failure/cutoff\n\n  return Recursive-Dls(Make-Node(problem.Initial-State),problem,limit)\n\nfunction Recursive-Dls(node,problem,limit) returns a solution, or failure/cutoff\n\n  if problem.Goal-Test(node.State) then return Solution(node)\n  else if limit = 0 then return cutoff\n  else\n    cutoff_occurred? ← false\n    for each action in problem.Actions(node.State) do\n      child ← Child-Node(problem,node,action)\n      result ← Recursive-Dls(child,problem,limit-1)\n      if result = cutoff then cutoff_occurred? ← true\n      else if result ≠ failure then return result\n    if cutoff_occurred? then return cutoff \n    else return failure\n\n\nProperties\n\nTime\n\nO(b^{l})\n\nSpace\n\nO(bl)\n\nCompleteness\n\nMay be incomplete if l&lt;d (the goal is deeper than the limit).\n\nOptimal: Not optimal if l&gt;d.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Uninformed search strategies</span>"
    ]
  },
  {
    "objectID": "ai-04-uninformed-revised.html#iterative-deepening-depth-first-search",
    "href": "ai-04-uninformed-revised.html#iterative-deepening-depth-first-search",
    "title": "4  Uninformed search strategies",
    "section": "4.5 Iterative deepening depth-first search",
    "text": "4.5 Iterative deepening depth-first search\n\nIterative deepening depth-first search\n\nThis is a general strategy, often used in combination with depth-first tree search to find the best depth limit.\nIt works by gradually increasing the limit until a goal is found.\n\nThe depth limit eventually reaches the depth d of the shallowest goal node.\n\n\nfunction Iterative-Deepening-Search(problem) returns a solution, or failure\n\n  for depth = 0 to ∞ do\n    result ← Depth-Limited-Search(problem,depth)\n    if result ≠ cutoff then return result\n\n\nIlustration\n\nFour iterations of iterative deepening search on a binary tree.\n\n\nProperties\n\nTime complexity\n\ndb^{1}+(d-1)b^{2}+1b^{d}=O(b^{d})\n\nSpace complexity\n\nO(bd), similar to DFS.\n\nCompleteness\n\nYes, provided the branching factor b is finite.\n\nOptimal: Not optimal in general.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Uninformed search strategies</span>"
    ]
  },
  {
    "objectID": "ai-04-uninformed-revised.html#bidirectional-search",
    "href": "ai-04-uninformed-revised.html#bidirectional-search",
    "title": "4  Uninformed search strategies",
    "section": "4.6 Bidirectional search",
    "text": "4.6 Bidirectional search\n\nBidirectional search\n\nThis strategy runs two simultaneous searches:\n\nOne from the initial state forward.\nOne from the goal state backward.\n\nThe hope is that the two searches will meet in the middle.\n\n\n\n\nProperties\n\nTime and Space complexity: O(b^{d/2})\nGoal test: Checks whether the frontiers of the two searches intersect.\nOptimality: Generally not optimal.\nWhile it sounds attractive, what is the tradeoff?\nThe space requirement for the frontiers of at least one search is significant.\nIt is not easy to search backward (predecessors are required).\n\nThis is difficult if there are multiple goals.\nIt is especially difficult if the goal is an abstract description (e.g., “no queen attacks another queen”).\n\n\n\n\nComparing uninformed search strategies\n\n\n\n\n\n\n\n\n\n\n\n\nCriterion\nBreadth-First\nUniform-Cost\nDepth-First\nDepth-Limited\nIterative Deepening\nBidirectional\n\n\n\n\nComplete\nYesa\nYesa,b\nNo\nNo\nYesa\nYesa,d\n\n\nTime\nO(b^{d})\nO(b^{1+\\left\\lfloor \\frac{C^{**}}{\\epsilon}\\right\\rfloor })\nO(b^{m})\nO(b^{l})\nO(b^{d})\nO(b^{\\frac{d}{2}})\n\n\nSpace\nO(b^{d})\nO(b^{1+\\left\\lfloor \\frac{C^{**}}{\\epsilon}\\right\\rfloor })\nO(bm)\nO(bl)\nO(bd)\nO(b^{\\frac{d}{2}})\n\n\nOptimal\nYesc\nYes\nNo\nNo\nYesc\nYesc,d\n\n\n\nEvaluation of tree-search strategies. b is the branching factor; d is the depth of the shallowest solution; m is the maximum depth of the search tree; l is the depth limit. Superscript caveats are as follows:a complete if b is finite;b complete if step costs \\geq\\epsilon for positive \\epsilon;c optimal if step costs are all identical;d if both directions use breadth-first search.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Uninformed search strategies</span>"
    ]
  },
  {
    "objectID": "ai-04-uninformed-revised.html#references",
    "href": "ai-04-uninformed-revised.html#references",
    "title": "4  Uninformed search strategies",
    "section": "4.7 References",
    "text": "4.7 References",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Uninformed search strategies</span>"
    ]
  },
  {
    "objectID": "ai-05-informed-revised.html",
    "href": "ai-05-informed-revised.html",
    "title": "5  Informed Search",
    "section": "",
    "text": "Informed search strategies",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Informed Search</span>"
    ]
  },
  {
    "objectID": "ai-05-informed-revised.html#best-first-search",
    "href": "ai-05-informed-revised.html#best-first-search",
    "title": "5  Informed Search",
    "section": "5.1 Best-first search",
    "text": "5.1 Best-first search\n\nBest-first search\n\nAn instance of the general tree/graph search algorithm.\nA node is selected for expansion based on an evaluation function, f(n), which is construed as a cost estimate.\n\nThe node with the lowest evaluation f(n) is expanded first.\nThe implementation of best-first graph search is identical to that for uniform-cost search, except for the use of f instead of g to order the priority queue.\nThe choice of f determines the search strategy.\n\nMost best-first algorithms include as a component of f a heuristic function, denoted h(n):\n\nh(n): estimated cost of the cheapest path from the state at node n to a goal state.\n\nh(n) depends only on the state at node n.\nAssumption of h:\n\nArbitrary, nonnegative, problem-specific functions.\nConstraint: if n is a goal node, then h(n)=0.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Informed Search</span>"
    ]
  },
  {
    "objectID": "ai-05-informed-revised.html#greedy-best-first-search",
    "href": "ai-05-informed-revised.html#greedy-best-first-search",
    "title": "5  Informed Search",
    "section": "5.2 Greedy best-first search",
    "text": "5.2 Greedy best-first search\n\nGreedy best-first search\n\nAttempts to expand the node that is closest to the goal, on the grounds that this is likely to lead to a solution quickly.\nThus, it evaluates nodes by using just the heuristic function:  f(n)=h(n) \n\n\n\nIllustration: Romania\n\nUse the straight-line distance heuristic, which we will call h_{SLD}.\nTable: Values of h_{SLD} – straight-line distances to Bucharest.\n\nThe initial state:\n\n\nAfter expanding Arad:\n\n\nAfter expanding Sibu:\n\n\nAfter expanding Fagaras:\n\n\n\n\n\nProperties\n\nTime: O(b^{m}), but a good heuristic can give dramatic improvement.\nSpace: O(b^{m}), keeps all nodes in memory.\nComplete: No, can get stuck in loops (e.g., Iasi \\rightarrow Neamt \\rightarrow Iasi \\rightarrow Neamt \\rightarrow …).\n\nYes in finite space with repeated-state checking.\n\nOptimal: No.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Informed Search</span>"
    ]
  },
  {
    "objectID": "ai-05-informed-revised.html#a-search",
    "href": "ai-05-informed-revised.html#a-search",
    "title": "5  Informed Search",
    "section": "5.3 A* search",
    "text": "5.3 A* search\n\nA* search\n\nA* search (pronounced A-star search) is the most widely known form of best-first search.\nIdeas:\n\nUse heuristic to guide search, but not exclusively.\nAvoid expanding paths that are already expensive.\n\nEvaluation function:  f(n)=g(n)+h(n) \n\ng(n): cost so far to reach n.\nh(n): estimated cost of the cheapest path to goal from n.\nf(n): estimated cost of the cheapest solution through n.\n\nThe algorithm is identical to Uniform-Cost-Search except that A* uses g+h instead of g.\n\n\n\nIllustration: Romania\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProperties\n\nTime: Exponential in [relative error in h \\times length of solution].\nSpace: Keeps all nodes in memory.\nComplete: Yes, unless there are infinitely many nodes with f\\leq f(G).\nOptimal: Yes, cannot expand f_{i+1} until f_{i} is finished.\n\nA* expands all nodes with f(n)&lt;C^{*}.\nA* expands some nodes with f(n)=C^{*}.\nA* expands no nodes with f(n)&gt;C^{*}.\n\n\n\n\nAdmissibility and Consistency\n\nDefinition\nh(n) is an admissible heuristic:\n\nif it never overestimates the cost to reach the goal,\nor if for every node n, h(n)\\leq h^{*}(n) (where h^{*}(n) is the true optimal cost to reach the goal state from n).\n\n\n\nDefinition\nh(n) is a consistent heuristic if, for every node n and every successor n' of n generated by any action a, the estimated cost of reaching the goal from n is no greater than the step cost of getting to n' plus the estimated cost of reaching the goal from n’:  h(n)\\leq c(n,a,n')+h(n') \n\n\n\nA consistent heuristic is also admissible.\n\n\n\nOptimal for A*\n\nUsing tree search algorithm to find a path from S to G:\n\n\n\nTheorem\nThe tree-search version of A* is optimal if h(n) is admissible.\n\nProof\nSuppose some suboptimal goal G_{2} has been generated and is in the frontier. Let n be an unexpanded node on a shortest path to an optimal goal G (graph separation).\n\nf(G)=g(G) since h(G)=0.\nf(G_{2})=g(G_{2}) since h(G_{2})=0.\ng(G_{2})&gt;g(G) since G_{2} is suboptimal.\nf(G)&gt;f(n) since h is admissible.\nSince f(G_{2})&gt;f(n), A* will never select G_{2} for expansion.\n\n\n\nUsing graph search algorithm to find a path from S to G:\n\n\n\nTheorem\nThe graph-search version of A* is optimal if h(n) is consistent.\n\nProof\n\nIf h(n) is consistent, then the values of f(n) along any path are nondecreasing. Suppose n' is a successor of n; then g(n')=g(n)+c(n,a,n') for some action a, and we have:  f(n')=g(n')+h(n')=g(n)+c(n,a,n)+h(n')\\geq g(n)+h(n)=f(n) \nWhenever A* selects a node n for expansion, the optimal path to that node has been found. Were this not the case, there would have to be another frontier node n' on the optimal path from the start node to n; since f(n')&lt;f(n), n' would have been selected first.\n\n\n\nContours\n\nThe search algorithm expands nodes in order of increasing f value.\nWe can draw contours in the state space, just like the contours in a topographic map.\n\nGradually adds f-contours of nodes; contour i has all nodes with f&lt;f_{i}, where f_{i}&lt;f_{i+1}.\n\n\nThe bands of uniform-cost search will be “circular” around the start state.\nThe bands of A* with more accurate heuristics will stretch toward the goal state and become more narrowly focused around the optimal path.\n\n\n\nThe 8-puzzle\n\nThe average solution cost for a randomly generated 8-puzzle instance is about 22 steps.\nThe branching factor b is about 3.\n8-puzzle:\n\n3^{22}\\approx 3.1\\times 10^{10} states (using an exhaustive tree search).\n9!/2=181,440 reachable states (using graph search).\n\n15-puzzle: around 10^{13} states.\nA typical instance of the 8-puzzle. The optimal solution is 26 steps long.\n\n\n\n\nAdmissible heuristics for 8-puzzle\n\nh_{1} = the number of misplaced tiles. h_{1} is an admissible heuristic because it is clear that any tile that is out of place must be moved at least once.\nh_{2} = the sum of the distances of the tiles from their goal positions. h_{2} is also admissible because any move can only move one tile one step closer to the goal.\n\nh_{1}=1+1+1+1+1+1+1+1=8 and h_{2}=3+1+2+2+2+3+3+2=18.\n\n\n\nSolve 8 puzzle\n\nUsing h_{2}:",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Informed Search</span>"
    ]
  },
  {
    "objectID": "ai-05-informed-revised.html#memory-bounded-heuristic-search",
    "href": "ai-05-informed-revised.html#memory-bounded-heuristic-search",
    "title": "5  Informed Search",
    "section": "5.4 Memory-bounded heuristic search",
    "text": "5.4 Memory-bounded heuristic search\n\nMemory-bound heuristic search\n\nIn practice, A* usually runs out of space long before it runs out of time.\nIdea: Try something like DFS, but do not forget everything about the branches we have partially explored.\n\n\n\nIterative-deepening A* (IDA*)\n\nThe main difference with IDS:\n\nCut-off uses the f-value (g+h) rather than the depth.\nAt each iteration, the cutoff value is the smallest f-value of any node that exceeded the cutoff on the previous iteration.\n\nAvoids the substantial overhead associated with keeping a sorted queue of nodes.\nPractical for many problems with unit step costs, yet difficult with real valued costs.\n\n\n\nAlgorithm\nfunction IDA*(problem) returns a solution, or failure\n\n  variables:\n    f_limit: the current f-COST limit\n    root: a node\n\n    root ← Make-Node(problem.Initial-State)\n    f_limit ← root.f\n\n    loop do\n      solution, f_limit ← DFS-Contour(problem, root, f_limit)\n      if solution is non-null then return solution\n      if f_limit = ∞ then return failure\nfunction DFS-Contour(problem, node, f_limit) returns a solution, or failure, a new f-cost limit\n\n  if node.f &gt; f_limit then return null, node.f\n  if problem.Goal-Test(node.State) then return Solution(node)\n  successors ← ⌀\n  for each action in problem.Actions(node.State) do\n    add Child-Node(problem, node, action) into successors\n  f_next ← ∞\n  for each s in successors do\n    solution, f_new ← DFS-Contour(problem, s, f_limit)\n    if solution is non-null then return solution, f_limit\n    f_next ← Min(f_next, f_new)\n  return null, f_next\n\n\nIllustration\n\nIDA* search for the shortest route from v_{0} (start node) to v_{25} (goal node); each node has a f value.\n\n\n\n\nRecursive best-first search (RBFS)\n\nKeeps track of the f-value of the best alternative path available from any ancestor of the current node.\n\nBacktracks when the current node exceeds f\\_limit.\n\nAs it backtracks, it replaces the f-value of each node along the path with the best value f(n) of its children.\n\n\n\nAlgorithm\nfunction Recursive-Best-First-Search(problem) returns a solution, or failure\n\n  return Rbfs(problem, Make-Node(problem.Initial-State), ∞)\n\nfunction Rbfs(problem, node, f_limit) returns a solution, or failure and a new f-cost limit\n\n  if problem.Goal-Test(node.State) then return Solution(node)\n  successors ← ⌀\n  for each action in problem.Actions(node.State) do\n    add Child-Node(problem, node, action) into successors\n  if successors is empty then return failure, ∞\n  for each s in successors do\n    // update f with value from previous search, if any\n    s.f ← Max(s.g + s.h, node.f)\n  loop do\n    best ← lowest f-value node in successors\n    if best.f &gt; f_limit then return failure, best.f\n    alternative ← the second-lowest f-value among successors\n    result, best.f ← Rbfs(problem, best, Min(f_limit, alternative))\n    if result != failure then return result\n\n\nIllustration\n\nStages in an RBFS search for the shortest route to Bucharest. The f-limit value for each recursive call is shown on top of each current node, and every node is labeled with its f-cost.\n\n\n\nThe path via Rimnicu Vilcea is followed until the current best leaf (Pitesti) has a value that is worse than the best alternative path (Fagaras).\n\n\n\nThe recursion unwinds and the best leaf value of the forgotten subtree (417) is backed up to Rimnicu Vilcea; then Fagaras is expanded, revealing a best leaf value of 450.\n\n\n\nThe recursion unwinds and the best leaf value of the forgotten subtree (450) is backed up to Fagaras; then Rimnicu Vilcea is expanded. This time, because the best alternative path (through Timisoara) costs at least 447, the expansion continues to Bucharest.\n\n\n\n\nProperties\n\nOptimality:\n\nLike A*, optimal if h(n) is admissible.\n\nTime complexity:\n\nDifficult to characterize.\nDepends on accuracy of h(n) and how often the best path changes.\nCan end up “switching” back and forth.\n\nSpace complexity:\n\nLinear space: O(bd).\nOther extreme to A* - uses too little memory even if more memory were available.\n\n\n\n\nSimplified Memory-bound A* – SMA*\n\nSimilar to A*, but deletes the worst node (largest f-value) when memory is full.\nSMA* expands the (newest) best leaf and deletes the (oldest) worst leaf.\nSMA* also backs up the value of the forgotten node to its parent.\n\nIf there is a tie (equal f-values), delete the oldest nodes first.\n\nSimplified-MA* finds the optimal reachable solution given the memory constraint.\n\nThe depth of the shallowest goal node is less than the memory size (expressed in nodes).\n\nTime can still be exponential.\n\n\n\nAlgorithm\nfunction SMA*(problem) returns solution or failure\n\n    node ← Make-Node(problem.Initial-State)\n    node.f ← node.g + node.h\n    frontier ← priority queue ordered by lowest f\n\n    Insert(node, frontier)\n\n    loop\n        if frontier is empty then return failure\n\n        node ← Pop(frontier)\n\n        if Goal-Test(node.State) then return Solution(node)\n\n        if node is not fully expanded then\n            s ← Next-Successor(node)\n            s.f ← max(s.g + s.h, node.f)\n            Insert(s, frontier)\n        else\n            node.f ← min(child.f for each child of node)\n            if node is not root then\n                Insert(node.parent, frontier)\n\n        if memory is full then\n            worst ← Shallowest-Highest-f leaf in frontier\n            Remove(worst, frontier)\n            parent ← worst.parent\n            Remove worst from parent.children\n            parent.f ← min(child.f for each child)\n            Insert(parent, frontier)\n\n\nIllustration\n\nFind the lowest-cost goal node with enough memory:\n\nMax Nodes = 3.\nA: start node and D, F, I, J: goal nodes.\nEach edge/node is labelled with its step cost/heuristic value.\n\n\nConsider only current f-cost:\n\n\n\n\nLearning to search better\n\nCould an agent learn how to search better? Yes.\nMetalevel state space: Each state captures the internal (computational) state of a program that is searching in an object-level state space.\nA metalevel learning algorithm learns from these experiences to avoid exploring unpromising subtrees.\nFor example, the map of Romania problem:\n\nThe internal state of the A* algorithm is the current search tree.\nEach action in the metalevel state space is a computation step that alters the internal state; for example, each computation step in A* expands a leaf node and adds its successors to the tree.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Informed Search</span>"
    ]
  },
  {
    "objectID": "ai-05-informed-revised.html#heuristic-functions",
    "href": "ai-05-informed-revised.html#heuristic-functions",
    "title": "5  Informed Search",
    "section": "5.5 Heuristic functions",
    "text": "5.5 Heuristic functions\n\nThe effect of heuristic accuracy on performance\n\nEffective branching factor b^{*} characterizes the quality of a heuristic.\nIf the total number of nodes generated by A* for a particular problem is N and the solution depth is d, then b^{*} is the branching factor that a uniform tree of depth d would have to have in order to contain N+1 nodes:  N+1=1+\\left(b^{*}\\right)+\\left(b^{*}\\right)^{2}+...+\\left(b^{*}\\right)^{d} \nb^{*} varies across problem instances, but is fairly constant for sufficiently hard problems.\nA well-designed heuristic would have a value of b^{*} close to 1 \\Rightarrow fairly large problems solved at reasonable cost.\n\n\nComparison of the search costs and effective branching factors for the IDS and A* algorithms with h_{1}, h_{2}. Data are averaged over 100 instances of the 8-puzzle for each of various solution lengths d.\n\n\n\n\n\n\n\n\n\n\n\nd\nIDS\nA*(h_{1})\nA*(h_{2})\nIDS\nA*(h_{1})\nA*(h_{2})\n\n\n\n\n2\n10\n6\n6\n2.45\n1.79\n1.79\n\n\n4\n112\n13\n12\n2.87\n1.48\n1.45\n\n\n6\n680\n20\n18\n2.73\n1.34\n1.30\n\n\n8\n6384\n39\n25\n2.80\n1.33\n1.24\n\n\n10\n47127\n93\n39\n2.79\n1.38\n1.22\n\n\n12\n3644035\n227\n73\n2.78\n1.42\n1.24\n\n\n14\n–\n539\n113\n–\n1.44\n1.23\n\n\n16\n–\n1301\n211\n–\n1.45\n1.25\n\n\n18\n–\n3056\n363\n–\n1.46\n1.26\n\n\n20\n–\n7276\n676\n–\n1.47\n1.27\n\n\n22\n–\n18094\n1219\n–\n1.48\n1.28\n\n\n24\n–\n39135\n1641\n–\n1.48\n1.26\n\n\n\n\n\nDominance and Composite\n\nIf h_{2}(n)\\geq h_{1}(n) for all n (both admissible), then h_{2} dominates h_{1} and is better for search.\n\nA* using h_{2} will never expand more nodes than A* using h_{1}.\n\nHence, it is generally better to use a heuristic function with higher values, provided it is consistent and that the computation time for the heuristic is not too long.\nSuppose a collection of admissible heuristics {h_{1},...,h_{m}} is available for a problem and none of them dominates any of the others.\nThe composite heuristic function is defined as h(n)=\\max\\{h_{1}(n),...,h_{m}(n)\\}. It is consistent and dominates all component heuristics.\n\n\n\nRelaxed problems\n\nDefinition\nA problem with fewer restrictions on the actions is called a relaxed problem.\n\n\nProve that the angle in a semicircle is a right angle.\n\n\n\n\nGenerating admissible heuristics from relaxed problems\n\nThe state-space graph of the relaxed problem is a supergraph of the original state space because the removal of restrictions creates added edges in the graph.\nOriginal problem state-space:\n\nRelaxed problem state-space:\n\nHence, the cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem.\nFurthermore, because the derived heuristic is an exact cost for the relaxed problem, it must obey the triangle inequality and is therefore consistent.\n\n\nOriginal problem of 8 puzzle\nA tile can move from square A to square B if A is horizontally or vertically adjacent to B and B is blank.\n\nWe can generate three relaxed problems by removing one or both of the conditions:\n\nRelaxed problems\n\nA tile can move from square A to square B if A is adjacent to B \\to h_{2}.\nA tile can move from square A to square B if B is blank \\to exercise.\nA tile can move from square A to square B \\to h_{1}.\n\n\n\n\nGenerating admissible heuristics from subproblems\n\nAdmissible heuristics can also be derived from the solution cost of a subproblem of a given problem.\n\nThe cost of the optimal solution of this subproblem \\leq the cost of the original problem (lower bound).\nIt can be more accurate than Manhattan distance in some cases.\n\nA subproblem of the 8-puzzle instance. The task is to get tiles 1, 2, 3, and 4 into their correct positions, without worrying about what happens to the other tiles.\n\n\n\n\nGoal fixed?\n\nAssume that the goal state G is fixed:\n\nUsing to find the shortest path from G to the other states:\n\nThe final g values can be considered as optimal h^{\\ast} values:\n\n\n\n\nPattern databases\n\nPattern databases (PDB) store the exact solution costs for every possible subproblem instance.\n\nE.g., every possible configuration of the four tiles and the blank.\n\nThe database itself is constructed by searching back from the goal and recording the cost of each new pattern encountered; the expense of this search is amortized over many subsequent problem instances.\nThe complete heuristic is constructed using the patterns in the databases.\n\n\n\nComputing the Heuristic\n\n\n31 moves needed to solve red tiles.\n22 moves need to solve blue tiles.\nOverall heuristic is maximum of 31=\\max(31,22) moves.\n\n\n\nDisjoint pattern databases\n\nLimitation of traditional PDB: Taking the max \\to diminishing returns on additional PDBs.\nDisjoint pattern databases: Count only moves of the pattern tiles, ignoring non-pattern moves.\nIf no tile belongs to more than one pattern, add their heuristic values.\n\n\n\nComputing the Heuristic\n\n\n20 moves needed to solve red tiles.\n25 moves needed to solve blue tiles.\nOverall heuristic is sum, or 45=20+25 moves.\n\n\n\nPerformance\n\n15 Puzzle\n\n1000 \\times speedup vs. Manhattan distance.\nIDA* with the two DBs (the 7-tile database contains 58 million entries and the 8-tile database contains 519 million entries) solves 15-puzzles optimally in 30 milliseconds.\n\n24 Puzzle\n\n12 million \\times speedup vs. Manhattan distance.\nIDA* can solve random instances in 2 days.\nRequires 4 DBs (each DB has 128 million entries).\nWithout PDBs: 65,000 years.\n\n\n\n\n\nLearning heuristics from experience\n\nExperience means solving a lot of instances of a problem.\n\nE.g., solving lots of 8-puzzles.\n\nEach optimal solution to a problem instance provides examples from which h(n) can be learned.\nLearning algorithms:\n\nNeural nets\nDecision trees\nInductive learning",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Informed Search</span>"
    ]
  },
  {
    "objectID": "ai-05-informed-revised.html#references",
    "href": "ai-05-informed-revised.html#references",
    "title": "5  Informed Search",
    "section": "5.6 References",
    "text": "5.6 References",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Informed Search</span>"
    ]
  },
  {
    "objectID": "ai-06-local-search.html",
    "href": "ai-06-local-search.html",
    "title": "6  Local Search",
    "section": "",
    "text": "Optimization problems",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Local Search</span>"
    ]
  },
  {
    "objectID": "ai-06-local-search.html#hill-climbing-search",
    "href": "ai-06-local-search.html#hill-climbing-search",
    "title": "6  Local Search",
    "section": "6.1 Hill-climbing Search",
    "text": "6.1 Hill-climbing Search\n\nHill-climbing Search\n\nThe hill-climbing search algorithm (steepest-ascent version) is simply a loop that continually moves in the direction of increasing value (uphill). It terminates when it reaches a “peak” where no neighbor has a higher value.\nThe algorithm does not maintain a search tree, so the data structure for the current node need only record the state and the value of the objective function.\nHill climbing does not look ahead beyond the immediate neighbors of the current state\n\n\n\nAlgorithm\nfunction Hill-Climbing(problem) returns a state that is a local maximum\n\n  current ← Make-Node(problem.Initial-State)\n\n  loop do\n    neighbor ← a highest-valued successor of current\n    if neighbor.Value ≤ current.Value then return current.State\n    current ← neighbor\n\nAt each step the current node is replaced by the best neighbor; in this version, that means the neighbor with the highest Value, but if a heuristic cost estimate h is used, we would find the neighbor with the lowest h.\nHill-climbing algorithms typically choose randomly among the set of best successors if there is more than one.\n\n\n\nIllustration\n\nFind a state with the highest value.\n\n\n\n\nExample\n\nFind a state with the highest value in a grid problem.\n\n\n\n\nKnapsack Problem\n\n\nConfigurations: Any combination of objects inside the knapsack\nInitial configuration: Empty knapsack\nActions: Put objects in and take objects from the knapsack\nBest configuration: A configuration with \\max\\sum value_{i}\n\n\n\n8-queens Problem\n\nComplete-state formulation\n\nAll 8 queens on the board, one per column\n\nSuccessor function\n\nMove a single queen to another square in the same column 8\\times7=56 successors\n\nHeuristic cost function h(n)\n\nThe number of pairs of queens that are attacking each other\nGlobal minimum has h(n)=0\n\n(a) An 8-queens state with heuristic cost estimate h=17, showing the value of h for each possible successor obtained by moving a queen within its column. The best moves are marked. (b) A local minimum in the 8-queens state space; the state has h=1 but every successor has a higher cost. It takes just 5 steps from state (a) to state (b)\n\n \n\n\nProblems\n\nHill climbing often gets stuck for the following reasons:\n\nLocal maxima: a local maximum is a peak that is higher than each of its neighboring states but lower than the global maximum\nRidges: ridges result in a sequence of local maxima that is very difficult for greedy algorithms to navigate.\nPlateaux: a plateau is a flat area of the state-space landscape.\n\nIn each case, the algorithm reaches a point at which no progress is being made. Starting from a randomly generated 8-queens state, hill climbing gets stuck 86% of the time, solving only 14% of problem instances.\n\nIt takes 4 steps on average for each successful instance and 3 for each failure\n\n\n\n\nPossible solution\n\nIf no uphill (downhill) moves, allow sideways moves in hope that algorithm can escape local maximum\nA limit on the possible number of sideways moves required to avoid infinite loops\n8-queens problem\n\nAllow sideways moves up to 100 \\to percentage of problem instances solved raises from 14 to 94%\nIt takes 21 steps on average for each successful instance and 64 for each failure\n\n\n\n\nVariants of hill climbing\n\nStochastic hill climbing\n\nChoose at random from among the uphill moves with a probability of selection varied with the moves’ steepness\nUsually converge more slowly than steepest ascent, but find better solutions in some cases\n\nFirst-choice hill climbing\n\nGenerate successors randomly until better than the current state\nGood strategy when a state has many successors (e.g., thousands)\n\nRandom-restart hill climbing: “If at first you don’t succeed, try, try again.”\n\nA series of hill-climbing searches from randomly generated initial states until a goal is found.\nIf each hill-climbing search has a probability p of success, then the expected number of restart 1/p\n\nRandom-walk hill climbing: At each step do one of the two\n\nGreedy: With probability p move to the neighbor with largest value\nRandom: With probability 1-p move to a random neighbor",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Local Search</span>"
    ]
  },
  {
    "objectID": "ai-06-local-search.html#simulated-annealing",
    "href": "ai-06-local-search.html#simulated-annealing",
    "title": "6  Local Search",
    "section": "6.2 Simulated Annealing",
    "text": "6.2 Simulated Annealing\n\nSimulated Annealing\n\nSimulated Annealing = physics inspired twist on random walk\nA metal alloy or dissolution is heated at high temperatures and progressively cooled in a controlled way\nIf the cooling process is adequate the minimal state of energy of the system is achieved (global minimum)\n\n\n\n\nAlgorithm\nfunction Simulated-Annealing(problem, schedule) returns a solution state\n\ninputs: problem, a problem\n        schedule, a mapping from time to \"temperature\"\n\n  current ← Make-Node(problem.Initial-State)\n  for t=1 to infinity do\n    T ← schedule(t)\n    if T = 0 then return current\n    next ← a randomly selected successor of current\n    DeltaE ← next.Value - current.Value\n    if DeltaE &gt; 0 then current ← next\n    else current ← next only with probability exp(DeltaE / T)\n\n\nPhysical Interpretation of Simulated Annealing\n\nA Physical Analogy:\n\nimagine letting a ball roll downhill on the function surface \\tothis is like hill-climbing (for minimization)\nnow imagine shaking the surface, while the ball rolls, gradually reducing the amount of shaking \\to this is like simulated annealing\n\nAnnealing = physical process of cooling a liquid or metal until particles achieve a certain frozen crystal state\n\nfree variables are like particles\nseek “low energy” (high quality) configuration\nslowly reducing temp. T with particles moving around randomly\n\n\n\n\nTSP Problem\n\nGiven a list of cities and the distances\nWhat is the shortest possible route \\{(x_{1},y_{1}),...,(x_{n},y_{n})\\} that visits each city exactly once and returns to the origin city?\n\n\n\nIntial a list of cities\nAn energy function (sum of the distance among the cities, following the order in the list)\n f=\\sum_{i=1}^{n}\\sqrt{(x_{i}-x_{i+1})^{2}+(y_{i}-y_{i+1})^{2}} \nAction: swap two cities",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Local Search</span>"
    ]
  },
  {
    "objectID": "ai-06-local-search.html#local-beam-search",
    "href": "ai-06-local-search.html#local-beam-search",
    "title": "6  Local Search",
    "section": "6.3 Local Beam Search",
    "text": "6.3 Local Beam Search\n\nLocal Beam Search\nKeep track of k states rather than just one (extreme reaction to memory problems)\n\nBegin with k randomly generated states\nAt each step, all successors of all k tates are generated\nIf any of successors is goal \\to finished\nElse select k best from successors and repeat\n\nNot the same as k random-start searches run in parallel!\nIn its simplest form, local beam search can suffer from a lack of diversity among the k states\n\nThey can quickly become concentrated in a small region of the state space \\to an expensive version of hill climbing\n\nStochastic beam search\n\nChoose k successors at random, with the probability of choosing a given successor being an increasing function of its value",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Local Search</span>"
    ]
  },
  {
    "objectID": "ai-06-local-search.html#genetic-algorithms",
    "href": "ai-06-local-search.html#genetic-algorithms",
    "title": "6  Local Search",
    "section": "6.4 Genetic Algorithms",
    "text": "6.4 Genetic Algorithms\n\nGenetic algorithms\n\nA genetic algorithm (or GA) is a variant of stochastic beam search in which successor states are generated by combining two parent states rather than by modifying a single state.\nThe analogy to natural selection is the same as in stochastic beam search, except that now we are dealing with sexual rather than asexual reproduction.\n\nThe GA begins with a set of k randomly generated states, called the population.\nEach state is rated by the objective function, called the fitness function.\n\nHigher values for better state\n\nProduce the next generation by “simulated evolution”\n\nRandom selection\nCrossover\nRandom mutation\n\n\n\n\nAlgorithm\nfunction Genetic-Algorithm(population, Fitness-Fn) returns an individual\n\ninputs: population, a set of individuals\n        Fitness-Fn, a function that measures the fitness of an individual\n\n  repeat\n    new_population ← empty set\n    for i = 1 to SIZE(population) do\n      x ← Random-Selection(population, Fitness-Fn)\n      y ← Random-Selection(population, Fitness-Fn)\n      child ← Reproduce(x, y)\n      if (small random probability) then child ← Mutate(child)\n      add child to new_population\n    population ← new_population\n  until some individual is fit enough, or enough time has elapsed\n  return the best individual in population, according to Fitness-Fn\n\nfunction Reproduce(x, y) returns an individual\n\ninputs: x, y, parent individuals\n\n  n ← Length(x);\n  c ← random number from 1 to n\n  return Append(SubString(x, 1, c), SubString(y, c+1, n))\n\n\nIllustration\n\nRepresentation of Individuals\n\nEach state, or individual, is represented as a string over a finite alphabet – most commonly, a string of 0s and 1s.\nAlternatively, the state could be represented as 8 digits, each in the range from 1 to 8.\n\nThe genetic algorithm, illustrated for digit strings representing 8-queens states. The initial population in (a) is ranked by the fitness function in (b), resulting in pairs for mating in (c). They produce offspring in (d), which are subject to mutation in (e).\n\n\n\nThe 8-queens states corresponding to the first two parents in Figure 3(c) and the first offspring in Figure 3(d). The shaded columns are lost in the crossover step and the unshaded columns are retained.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Local Search</span>"
    ]
  },
  {
    "objectID": "ai-06-local-search.html#local-search-in-continuous-spaces",
    "href": "ai-06-local-search.html#local-search-in-continuous-spaces",
    "title": "6  Local Search",
    "section": "6.5 Local Search in Continuous Spaces",
    "text": "6.5 Local Search in Continuous Spaces\n\nOptimization of Continuous Functions\n\nDiscrete environments: Use hill-climbing\n\nContinuous environments: Use gradient descent\n\n\n\n\nExample\n\nSuppose we want to site three airports in Romania:\n\n6-D state space defined by (x_{1},y_{2}), (x_{2},y_{2}), (x_{3},y_{3})\nObjective function f(\\boldsymbol{x})=f(x_{1},y_{2},x_{2},y_{2},x_{3},y_{3}) = sum of squared distances from each city to nearest airport\n\n\n\n\n\nGradient descent\n\nDiscretization methods turn continuous space into discrete space\n\nE.g., empirical gradient considers \\pm\\delta change in each coordinate\n\nGradient descent methods using \\nabla f=\\left(\\frac{\\partial f}{\\partial x_{1}},\\frac{\\partial f}{\\partial y_{1}},\\frac{\\partial f}{\\partial x_{2}},\\frac{\\partial f}{\\partial y_{2}},\\frac{\\partial f}{\\partial x_{3}},\\frac{\\partial f}{\\partial y_{3}}\\right)\nInit \\boldsymbol{x}\\gets\\boldsymbol{x}_{0}\nTo reduce f, update \\boldsymbol{x} \\boldsymbol{x}\\leftarrow\\boldsymbol{x}-\\eta\\nabla f where \\eta is learning rate (0&lt;\\eta&lt;1)\nNewton–Raphson methods update \\boldsymbol{x} \\boldsymbol{x}\\leftarrow\\boldsymbol{x}-\\eta H^{-1}\\nabla f where H is a Hesian matrix\nH=\\left(\\begin{array}{cccc}\n\\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{1}} & \\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{2}} & \\cdots & \\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{n}}\\\\\n\\frac{\\partial^{2}f}{\\partial x_{2}\\partial x_{1}} & \\frac{\\partial^{2}f}{\\partial x_{2}\\partial x_{2}} & \\cdots & \\frac{\\partial^{2}f}{\\partial x_{2}\\partial x_{n}}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial^{2}f}{\\partial x_{n}\\partial x_{1}} & \\frac{\\partial^{2}f}{\\partial x_{n}\\partial x_{2}} & \\cdots & \\frac{\\partial^{2}f}{\\partial x_{n}\\partial x_{n}}\n\\end{array}\\right)\nx_{i} approach to the optimal point",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Local Search</span>"
    ]
  },
  {
    "objectID": "ai-06-local-search.html#searching-with-nondeterministic-actions",
    "href": "ai-06-local-search.html#searching-with-nondeterministic-actions",
    "title": "6  Local Search",
    "section": "6.6 Searching With Nondeterministic Actions",
    "text": "6.6 Searching With Nondeterministic Actions\n\nThe erratic vacuum world\nIn the erratic vacuum world, the Suck action works as follows:\n\nWhen applied to a dirty square the action cleans the square and sometimes cleans up dirt in an adjacent square, too.\nWhen applied to a clean square the action sometimes deposits dirt on the carpet.\nThe eight possible states of the vacuum world; states 7 and 8 are goal states.\n\n\n\n\nAND–OR search trees\n\nIn this problem, a Results function that returns a set of possible outcome states\nThe solution to the problem is not a sequence but a contingency plan (also known as a strategy) that specifies what to do depending on what percepts are received\nThe first two levels of the search tree for the erratic vacuum world. State nodes are OR nodes where some action must be chosen. At the AND nodes, shown as circles, every outcome must be handled, as indicated by the arc linking the outgoing branches. The solution found is shown in bold lines.\n\n\n\n\nAlgorithm\nfunction And-Or-Graph-Search(problem) returns a conditional plan, or failure\n  Or-Search(problem.Initial-State, problem, empty set)\n\nfunction Or-Search(state, problem, path) returns a conditional plan, or failure\n  if problem.Goal-Test(state) then return the empty plan\n  if state is on path then return failure\n  for each action in problem.Actions(state) do\n    plan ← And-Search(Results(state, action), problem, [state | path])\n    if plan ≠ failure then return [action | plan]\n  return failure\n\nfunction And-Search(states, problem, path) returns a conditional plan, or failure\n  for each s_i in states do\n    plan_i ← Or-Search(s_i, problem, path)\n    if plan_i = failure then return failure\n  return [if s_1 then plan_1\n          else if s_2 then plan_2\n          else if ...\n          else if s_{n-1} then plan_{n-1}\n          else plan_n]",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Local Search</span>"
    ]
  },
  {
    "objectID": "ai-06-local-search.html#references",
    "href": "ai-06-local-search.html#references",
    "title": "6  Local Search",
    "section": "6.7 References",
    "text": "6.7 References",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Local Search</span>"
    ]
  },
  {
    "objectID": "ai-07-csp.html",
    "href": "ai-07-csp.html",
    "title": "7  Constraint Satisfaction Problems",
    "section": "",
    "text": "7.1 Defining Constraint Satisfaction Problems",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Constraint Satisfaction Problems</span>"
    ]
  },
  {
    "objectID": "ai-07-csp.html#defining-constraint-satisfaction-problems",
    "href": "ai-07-csp.html#defining-constraint-satisfaction-problems",
    "title": "7  Constraint Satisfaction Problems",
    "section": "",
    "text": "Constraint Satisfaction Problems\n\nIn standard search problem, state is a “black box”\nA constraint satisfaction problem (CSP) use a factored representation for each state.\n\nState = a set of variables and each of which has a value\nSolution = each variable has a value that satisfies all constraints on that variable\n\nTake advantage of the structure of states\nGeneral-purpose rather than problem-specific heuristics\n\nIdentify combinations of variable-value that violate the constraints \\to eliminate large portions of the search space all at once\nSolutions to complex problems\n\n\n\nA constraint satisfaction problem consists of three components, \\mathcal{X}, \\mathcal{D}, and \\mathcal{C}:\n\n\\mathcal{X} is a set of variables, \\{ X_{1},\\ldots,X_{n} \\}\n\\mathcal{D} is a set of domains, \\{ D_{1},\\ldots,D_{n} \\}\n\nEach domain D_{i} consists of a set of allowable values, \\{ v_{1},\\ldots,v_{k}\\} for variable X_{i}\n\n\\mathcal{C} is a set of constraints that specify allowable combinations of values.\nEach constraint C_{j} consists of a pair \\langle scope,rel \\rangle, where scope is a tuple of variables that participate in the constraint and rel is a relation that defines the values that those variables can take on\n\n\n\n\nState Space And Solution\n\nEach state in a CSP is defined by an assignment of values to some or all of the variables,  { X_{i}=v_{i},X_{j}=v_{j},\\ldots} \nAn assignment that does not violate any constraints is called a consistent or legal assignment.\nA complete assignment is one in which every variable is assigned\nA partial assignment is one that assigns values to only some of the variables.\nA solution to a CSP is a consistent, complete assignment.\n\n\n\nExample problem: Map coloring\n\nThe principal states and territories of Australia. Coloring this map can be viewed as a constraint satisfaction problem (CSP). The goal is to assign colors to each region so that no neighboring regions have the same color.\n\nThe map-coloring problem represented as a constraint graph.\nConstraint graph: nodes are variables, arcs show constraints\nCSP algorithms use the graph structure to speed up search\n\nVariables: \\{ WA,NT,Q,NSW,V,SA,T \\}\nDomains: D_{i}=\\{red,green,blue\\}\nConstraints: adjacent regions must have different colors\n\\begin{align*}\n\\mathcal{C} & =\\{SA\\neq WA,SA\\neq NT,SA\\neq Q,SA\\neq NSW,SA\\neq V,\\\\\n& WA\\neq NT,NT\\neq Q,Q\\neq NSW,NSW\\neq V\\}\n\\end{align*}\nThere are many possible solutions to this problem, e.g.,\n\\begin{align*}\n& \\{WA=red,NT=green,Q=red,NSW=green,\\\\\n& V=red,SA=blue,T=green\\}\n\\end{align*}\n\n\n\n\nExample problem: Job-shop scheduling\n\nWe consider a small part of the car assembly, consisting of 15 tasks:\n\ninstall axles (front and back)\naffix all four wheels (right and left, front and back)\ntighten nuts for each wheel\naffix hubcaps\ninspect the final assembly.\n\nSome tasks must occur before another while many other tasks can go on at once.\n\nE.g., a wheel must be installed before the hubcap is put on\n\nA task takes a certain amount of time to complete.\n\nWe can represent the tasks with 15 variables:\n\nVariables:\n\\begin{align*}\n\\mathcal{X}= & \\{Axle_{F},Axle_{B},\\\\\n& Wheel_{RF},Wheel_{LF},Wheel_{RB},Wheel_{LB},\\\\\n& Nuts_{RF},Nuts_{LF},Nuts_{RB},Nuts_{LB},\\\\\n& Cap_{RF},Cap_{LF},Cap_{RB},Cap_{LB},\\\\\n& Inspect\\}\n\\end{align*}\nDomains: D_{i}: the value of each variable is the time that the task starts\nConstraints: Assume task T_{1} and T_{2} take duration d_{1} and d_{2} to complete\n\nPrecedence constraints: task T_{1} must occur before task T_{2};  T_{1}+d_{1}\\leq T_{2} \nDisjunctive constraint: task T_{1} and task T_{2} must not overlap in time  T_{1}+d_{1}\\leq T_{2}\\text{ or }T_{2}+d_{2}\\leq T_{1} \n\nThe axles have to be in place before the wheels are put on, and it takes 10 minutes to install an axle\n\n\\begin{array}{ll}\nAxle_{F}+10\\leq Wheel_{RF} & Axle_{F}+10\\leq Wheel_{LF}\\\\\nAxle_{B}+10\\leq Wheel_{RB} & Axle_{B}+10\\leq Wheel_{LB}\n\\end{array}\n\nFor each wheel, we must affix the wheel (which takes 1 minute), then tighten the nuts (2 minutes), and finally attach the hubcap (1 minute)\n \\cdots \nSuppose we have four workers to install wheels, but they have to share one tool that helps put the axle in place\n \\cdots \nThe inspection comes last and takes 3 minutes\n \\forall X\\neq Inspect,X+d_{X}\\leq Inspect \nFinally, suppose there is a requirement to get the whole assembly done in 30 minutes \\to limit the domain of all variables to\n D_{j}={ 1,2,3,\\ldots,27} \n\n\n\nExample problem: 4-Queens\n\n\nVariables: \\{Q_{1},Q_{2},Q_{3},Q_{4}\\}\nDomains: D_{i}={1,2,3,4}\nConstraints:\n\nQ_{i}\\neq Q_{j} (cannot be in same row)\n|Q_{i}-Q_{j}|\\neq|i-j| (or same diagonal)\n\n\n\n\nExample problem: Cryptarithmetic\n\n\nVariables: \\{F,T,U,W,R,O,C_{1},C_{2},C_{3}\\}\nDomains: D_{i}=\\{0,1,2,3,4,5,6,7,8,9\\}\nConstraints:\n\nAlldiff(F,T,U,W,R,O)\nT\\neq0,F\\neq0\nC_{3}=F,…\n\n\n\n\nWhy formulate a problem as a CSP\n\nProvide natural representation for a wide variety of problems\nMany problems intractable in regular state-space search can be solved quickly with CSP formulation.\n\nFor example, once we have chosen \\{SA=blue\\} in the Australia problem.\n\nSearch: 3^{5}=243 assignments\nCSP: 2^{5}=32 assignments\nBetter insights to the problem and its solution\n\n\n\nVarieties of CSPs\n\nDiscrete variables and finite domains\n\nn variables with size d \\to O(n^{d}) complete assignments\nE.g., Boolean CSPs, including Boolean satisfiability (NP-complete)\n\nDiscrete variables and infinite domains\n\nE.g., job scheduling, variables are start/end times for each job\n\nContinuous variables\n\nE.g., start/end times for Hubble Telescope observations\n\n\n\n\nVarieties of constraints\n\nUnary constraints involve a single variable, e.g., SA\\neq green\nBinary constraints involve pairs of variables, e.g., SA\\neq WA\nHigher-order constraints involve 3 or more variables, e.g., cryptarithmetic column constraints\nPreferences (soft constraints), e.g., red is better than green often representable by a cost for each variable assignment \\rightarrow constrained optimization problems\n\n\n\nReal-world CSPs\n\nAssignment problems; e.g., who teaches what class?\nTimetabling problems; e.g., which class is offered when and where?\nHardware configuration\nSpreadsheets\nTransportation scheduling\nFactory scheduling\nFloorplanning",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Constraint Satisfaction Problems</span>"
    ]
  },
  {
    "objectID": "ai-07-csp.html#backtracking-search-for-csps",
    "href": "ai-07-csp.html#backtracking-search-for-csps",
    "title": "7  Constraint Satisfaction Problems",
    "section": "7.2 Backtracking Search for CSPs",
    "text": "7.2 Backtracking Search for CSPs\n\nCSP as Standard Search Problem\nLet’s start with the straightforward, dumb approach, then fix it\n\nStates are defined by the values assigned so far\nInitial state: the empty assignment, \\emptyset\nSuccessor function: assign a value to an unassigned variable that does not conflict with current assignment \\implies fail if no legal assignments (not fixable!)\nGoal test: the current assignment is complete 1. This is the same for all CSPs! 2. Every solution appears at depth n with n variables \\implies use depth-first search 3. Path is irrelevant, so can also use complete-state formulation 4. Branching factor b=(n-\\ell)d at depth \\ell, hence n!d^{n} leaves!\n\n\n\nBacktracking search\n\nVariable assignments are commutative, i.e., [WA=red then NT=green] same as [NT=green then WA=red]\nOnly need to consider assignments to a single variable at each node \\implies b=d and there are d^{n} leaves\nDepth-first search for CSPs with single-variable assignments is called backtracking search\nBacktracking search is the basic uninformed algorithm for CSPs\nCan solve n-queens for n\\approx25\n\n\n\nAlgorithm\nfunction Backtracking-Search(csp) returns a solution, or failure\n\n  return Backtrack(⌀, csp)\n\nfunction Backtrack(assignment, csp) returns a solution, or failure\n\n  if assignment is complete then return assignment\n  var ← Select-Unassigned-Variable(csp)\n  for each value in Order-Domain-Values(var, assignment, csp) do\n    if value is consistent with assignment then\n      add {var=value} to assignment\n      inferences ← Inference(csp, var, value)\n      if inferences ≠ failure then\n        add inferences to assignment\n        result ← Backtrack(assignment, csp)\n        if result ≠ failure then\n          return result\n      remove {var=value} and inferences from assignment\n  return failure\n\n\nIllustration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImproving backtracking efficiency\nGeneral-purpose methods can give huge gains in speed:\n\nWhich variable should be assigned next?\nIn what order should its values be tried?\nCan we detect inevitable failure early?\nCan we take advantage of problem structure?\n\n\n\nMinimum remaining values\nThe backtracking algorithm contains the line var \\gets Select-Unassigned-Variable(csp)\nWe need a strategy for Select-Unassigned-Variable to choose the next unassigned variable in \\{X_{1},X_{2},\\ldots\\}\n\nMinimum remaining values (MRV): choose the variable with the fewest legal values\nMRV usually performs better than a random/static ordering, sometimes by a factor of 1,000 or more\n\n\n\n\nDegree heuristic\n\nTie-breaker among MRV variables\nDegree heuristic: choose the variable with the most constraints on remaining variables\n\n\n\n\nLeast constraining value\nThe backtracking algorithm contains the line value in Order-Domain-Values(var, assignment, csp)\n\nGiven a variable, choose the least constraining value: the one that rules out the fewest values in the remaining variables\n\nCombining these heuristics makes 1000 queens feasible\n\n\n\nInference: Forward checking\n\nIdea: Keep track of remaining legal values for unassigned variables\n\\to Terminate search when any variable has no legal values\ninferences \\gets Inference(csp, var, value)\nNote: Forward checking propagates information from assigned to unassigned variables, but doesn’t provide early detection for all failures!\n\n\n\nInference: Forward checking",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Constraint Satisfaction Problems</span>"
    ]
  },
  {
    "objectID": "ai-07-csp.html#constraint-propagation-inference-in-csps",
    "href": "ai-07-csp.html#constraint-propagation-inference-in-csps",
    "title": "7  Constraint Satisfaction Problems",
    "section": "7.3 Constraint Propagation: Inference in CSPs",
    "text": "7.3 Constraint Propagation: Inference in CSPs\n\nConstraint propagation\n\nReduce the number of legal values for a variable by using constraints \\to legal values for another variable also reduced\nIntertwined with search, or done as a preprocessing step\n\nSometimes the preprocessing can solve the whole problem!\n\nEnforcing local consistency in each part of a graph causes inconsistent values to be eliminated throughout the graph\n\n\n\nNode consistency\n\nA single variable is node-consistent if all the values in the variable’s domain satisfy the variable’s unary constraints\n\nSouth Australians dislike green, $SA={ red,green,blue} SA={ red,blue} $\n\nEliminate all the unary constraints in a CSP by running node consistency\n\n\n\nArc consistency\n\nA variable in a CSP is arc-consistent if every value in its domain satisfies the variable’s binary constraints. More formally, X\\rightarrow Y is consistent iff for every value x of X there is some allowed y\nRun as a preprocessor before the search starts or after each assignment, must be run repeatedly until no inconsistency remains.\nTrade-off\n\nRequires some overhead to do, but generally more effective than direct search\nEliminate large (inconsistent) parts of the state-space more effectively than search\n\nNeed a systematic method for arc-checking\n\nIf Y loses a value, neighbors of Y need to be rechecked \\to incoming arcs can become inconsistent again while outgoing arcs stay still\n\n\n\n\nAlgorithm\nfunction AC-3(csp) returns false/true\n\ninputs: csp, a binary CSP with components (X, D, C)\nvariables: queue, a queue of arcs, initially all the arcs in csp\n\n  while queue ≠ ⌀ do\n    (X_i, X_j) ← Remove-First(queue)\n    if Revise(csp, X_i, X_j) then\n      if size of D_i = 0 then return false\n      for each X_k in X_i.Neighbors - {X_j} do\n        add (X_k, X_i) to queue\n  return true\n\nfunction Revise(csp, X_i, X_j) returns true iff we revise the domain of X_i\n\n  revised ← false\n  for each x in D_i do\n    if no value y in D_j allows (x,y) to satisfy \n       the constraint between X_i and X_j then\n      delete x from D_i\n      revised ← true\n  return revised\n\n\nIllustration\n\n \n \n\n\nComparison of Methods\nWhich constraints are tested for each method?",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Constraint Satisfaction Problems</span>"
    ]
  },
  {
    "objectID": "ai-07-csp.html#local-search-for-csps",
    "href": "ai-07-csp.html#local-search-for-csps",
    "title": "7  Constraint Satisfaction Problems",
    "section": "7.4 Local Search for CSPs",
    "text": "7.4 Local Search for CSPs\n\nLocal search for CSPs\n\nComplete-state formulation\n\nThe initial state assigns a value to every variable \\to violation\nThe search changes the value of one variable at a time \\to eliminate the violated constraints\n\nMin-conflicts heuristic: the minimum number of conflicts with other variables\nMin-conflicts is surprisingly effective for many CSPs.\n\nMillion-queens problem can be solved \\sim 50 steps\nHubble Space Telescope: the time taken to schedule a week of observations down from 3 weeks to \\sim 10 minutes\n\n\n\n\nAlgorithm\nfunction Min-Conflicts(csp, max_steps) returns a solution of failure\n\ninputs: csp, a constraint satisfaction problem\n        max_steps, the number of steps allowed before giving up\n\n  current ← an initial complete assignment for csp\n  for i = 1 to max_steps do\n    if current is a solution for csp then return current\n    var ← a randomly chosen conflicted variable from csp.Variables\n    value ← the value v for var that minimizes Conflicts(var, v, current, csp)\n    set {var=value} in current\n  return failure\n\n\nPerformance of min-conflicts\n\nGiven random initial state, can solve n-queens in almost constant time for arbitrary n with high probability (e.g., n=10,000,000)\nThe same appears to be true for any randomly-generated CSP except in a narrow range of the ratio R=\\frac{\\text{number of constraints}}{\\text{number of variables}}\n\n\n\n\nIllustration: 8-queens\n\nA two-step solution using min-conflicts for an 8-queens problem. At each stage, a queen is chosen for reassignment in its column. The number of conflicts (in this case, the number of attacking queens) is shown in each square. The algorithm moves the queen to the min-conflicts square, breaking ties randomly.\n\n\n\n\nImproving min-conflicts\n\nThe landscape of a CSP under the min-conflicts heuristic usually has a series of plateaux.\n\nMillions of variable assignments that are only one conflict away from a solution\n\nPlateau search: allow sideways moves to another state with the same score\nTabu search: keep a small list of recently visited states and forbid the algorithm to return to those states\nSimulated annealing can also be used\n\n\n\nConstraint weighting\n\nConcentrate the search on the important constraints\nEach constraint is given a numeric weight, W_{i}, initially all 1.\nAt each step, chooses a variable/value pair to change that has the lowest total weight of all violated constraints.\nIncrease the weight of each constraint that is violated by the current assignment.\n\n\n\nLocal search in online setting\n\nScheduling problems: online setting\n\nA week’s airline schedule may involve thousands of flights and tens of thousands of personnel assignments\nThe bad weather at one airport can render the schedule infeasible.\n\nThe schedule should be repaired with a minimum number of changes.\n\nDone easily with a local search starting from the current schedule\nA backtracking search with the new set of constraints usually requires much more time and might find a solution with many changes from the current schedule",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Constraint Satisfaction Problems</span>"
    ]
  },
  {
    "objectID": "ai-07-csp.html#the-structure-of-problems",
    "href": "ai-07-csp.html#the-structure-of-problems",
    "title": "7  Constraint Satisfaction Problems",
    "section": "7.5 The Structure of Problems",
    "text": "7.5 The Structure of Problems\n\nIndependent subproblems\n\nA problem CSP is decomposed into independent subproblems CSP_{i}\n\nIndependent subproblems are identifiable as connected components of constraint graph\n\nTasmania and mainland are independent subproblems\n\nIf assignmen S_{i} is a solution of CSP_{i} then\nS=\\bigcup_{i=1}^{n} S_{i}\\text{ is a solution of }CSP\nSuppose a problem of n variables can be broken into independent subproblems of only c variables\n\nWorst-case solution cost is O(\\frac{n}{c}d^{c})\nOriginal CSP has worst-case solution O(d^{n})\n\n\n\n\nTree-structured CSPs\n\nA CSP is a tree-structured CSP if the constraint graph has no loops\n\n\nTheorem\nA tree-structured CSP can be solved in O(n\\times d^{2}) time, compare to general CSPs, where worst-case time is O(d^{n})\n\n\nUsing Topologicalsort to create an ordering of the variables such that each variable appears after its parent in the tree\n(a) The constraint graph of a tree-structured CSP. (b) A linear ordering of the variables consistent with the tree with A as the root. This is known as a topological sort of the variables\n\n\n\n\nAlgorithm\nfunction Tree-Csp-Solver(csp) returns a solution, or failure\n\ninputs: csp, a CSP with components X, D, C\n\n  n ← |X|\n  assignment ← ⌀\n  root ← any variable in X\n  X ← Topologicalsort(X, root)\n  for j = n down to 2 do\n    Make-Arc-Consistent(Parent(X_j), X_j)\n    if it cannot be made consistent then return failure\n  for i = 1 to n do\n    assignment[X_i] ← any consistent value from D_i\n    if there is no consistent value then return failure\n  return assignment\n\n\nNearly tree-structured CSPs\n\nCycle cutset: a set of variables such that the remaining constraint graph is a tree\nThe removal of SA makes the constraint graph to be a tree\n\n\n\n\nCutset Conditioning\n\n\nIf the cycle cutset has size c, then the total run time is O(d^{c}(n-c)d^{2})\n\n\n\nTree Decomposition\n\nIdea: create a tree-structured graph of mega-variables\n\nEach mega-variable encodes part of the original CSP\nSubproblems overlap to ensure consistent solutions\n\nA tree decomposition must satisfy the following three requirements:\n\nEvery variable in the original problem appears in at least one of the subproblems.\nIf two variables are connected by a constraint in the original problem, they must appear together (along with the constraint) in at least one of the subproblems.\nIf a variable appears in two subproblems in the tree, it must appear in every subproblem along the path connecting those subproblems.\n\nA tree decomposition of the constraint graph\n\n\n\n\nThe structure of values\n\nConsider the map-coloring problem with n colors.\nFor every consistent solution, there is actually a set of n! solutions formed by permuting the color names.\n\nE.g., WA, NT, and SA must all have different colors, but there are 3! ways to assign the three colors to these three regions.\n\nSymmetry-breaking constraint: Impose an arbitrary ordering constraint that requires the values to be in alphabetical order.\n\nE.g., NT &lt; SA &lt; WA \\toonly one solution possible {NT = blue, SA = green, WA = red}",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Constraint Satisfaction Problems</span>"
    ]
  },
  {
    "objectID": "ai-07-csp.html#references",
    "href": "ai-07-csp.html#references",
    "title": "7  Constraint Satisfaction Problems",
    "section": "7.6 References",
    "text": "7.6 References",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Constraint Satisfaction Problems</span>"
    ]
  },
  {
    "objectID": "ai-08-adversarial-search.html",
    "href": "ai-08-adversarial-search.html",
    "title": "8  Adversarial Search",
    "section": "",
    "text": "Adversarial Search",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adversarial Search</span>"
    ]
  },
  {
    "objectID": "ai-08-adversarial-search.html#state-of-the-art-game-programs",
    "href": "ai-08-adversarial-search.html#state-of-the-art-game-programs",
    "title": "8  Adversarial Search",
    "section": "8.1 State-Of-The-Art Game Programs",
    "text": "8.1 State-Of-The-Art Game Programs\n\nCheckers\n\nComplexity\n\n\\approx10^{18}nodes, which may require 100k years with 106 positions/sec\n\nChinook (1989-2007)\n\nThe first computer program to win the world champion title in a competition against humans.\n1990: won 2 games in competition with world champion Tinsley (final score: 2 - 4, 33 draws)\n1994: 6 draws\n\nChinook’s search\n\nRan on regular PCs, play perfectly by using alpha-beta search combining with a database of 39 trillion endgame positions\n\n\n\n\nChess\n\nComplexity\n\nb\\approx35, d\\approx100, 10^{154} nodes\nCompletely impractical to search this\n\nDeep Blue (May 11, 1997)\n\nKasparov lost a 6-game match against IBM’s Deep Blue (1 win Kasp – 2 wins DB) and 3 ties.\n\nIn the future, focus will be to allow computers to LEARN to play chess rather than being TOLD how it should play\n\n\n\nGo\n\nComplexity\n\nBoard of 19\\times19, b\\approx361, average depth 200\n10^{174} possible board configuration.\nControl of territory is unpredictable until the endgame\n\nAlphaGo (2016) by Google\n\nBeat 9-dan professional Lee Sedol (4 - 1)\nMachine learning + Monte Carlo search guided by a “value network” and a “policy network” (implemented using deep neural network technology)\nLearn from human + Learn by itself (self-play games)",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adversarial Search</span>"
    ]
  },
  {
    "objectID": "ai-08-adversarial-search.html#optimal-decisions-in-games",
    "href": "ai-08-adversarial-search.html#optimal-decisions-in-games",
    "title": "8  Adversarial Search",
    "section": "8.2 Optimal Decisions in Games",
    "text": "8.2 Optimal Decisions in Games\n\nOptimal decisions in games\n\nNormal search problem\n\nOptimal solution is a sequence of action leading to a goal state.\n\nGames\n\nA search path that guarantee win for a player\nThe optimal strategy can be determined from the minimax value of each node\n\n\n\n\\text{Minimax}(s)=\\left\\{ \\begin{array}{ll}\n\\text{Utility}(s) & \\text{if}\\;\\text{Terminal-Test}(s)\\\\\n\\max_{a\\in\\text{Actions}(s)}\\text{Minimax}(\\text{Result}(s,a)) & \\text{if}\\;\\text{Player}(s)=MAX\\\\\n\\min_{a\\in\\text{Actions}(s)}\\text{Minimax}(\\text{Result}(s,a)) & \\text{if}\\;\\text{Player}(s)=MIN\n\\end{array}\\right.\n\n\nA two-player game tree. The \\Delta nodes are MAX nodes, in which it is MAX’s turn to move, and the \\nabla nodes are MIN nodes. The terminal nodes show the utility values for MAX; the other nodes are labeled with their minimax values.\n\n\nThe minimax algorithm\n\nCompute the minimax decision from the current state\nUse a simple recursive computation of the minimax values of each successor state\n\nThe recursion proceeds all the way down to the leaves of the tree, and then the minimax values are backed up through the tree as the recursion unwinds.\n\n\nfunction Minimax-Decision(state) returns an action\n    bestAction ← null\n    bestValue ← −∞\n\n    for each a in Actions(state) do\n        value ← Min-Value(Result(state, a))\n        if value &gt; bestValue then\n            bestValue ← value\n            bestAction ← a\n\n    return bestAction\n\nfunction Max-Value(state) returns a utility value\n    if Terminal-Test(state) then\n        return Utility(state)\n\n    v ← −∞\n    for each a in Actions(state) do\n        v ← Max(v, Min-Value(Result(state, a)))\n\n    return v\n\nfunction Min-Value(state) returns a utility value\n    if Terminal-Test(state) then\n        return Utility(state)\n\n    v ← +∞\n    for each a in Actions(state) do\n        v ← Min(v, Max-Value(Result(state, a)))\n\n    return v\n\n\nProperties\nA complete depth-first exploration of the game tree\n\nCompleteness\n\nYes (if tree is finite)\n\nOptimality\n\nYes (against an optimal opponent)\n\nTime complexity: O(b^{m})\nSpace complexity: O(bm) (depth-first exploration)\n\nFor chess, b\\approx35, m\\approx100 for ``reasonable’’ games \\Rightarrow exact solution completely infeasible\nBut do we need to explore every path?\n\n\nExercise\n\nCalculate the utility values for the remaining nodes of the game tree\n\n\n\n\nOptimal decisions in multiplayer games\n\nA single value is replaced with a vector of values\n\nthe Utility function return a vector of utilities\n\nFor terminal states, this vector gives the utility of the state from each player’s viewpoint.\n\n\nThe first three plies of a game tree with three players (A, B, C). Each node is labeled with values from the viewpoint of each player. The best move is marked at the root.\n\nChallenges in multiplayer games\n\nMultiplayer games usually involve alliances, which are made and broken as the game proceeds.\nIf the game is not zero-sum, then collaboration can also occur with just two players.",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adversarial Search</span>"
    ]
  },
  {
    "objectID": "ai-08-adversarial-search.html#alphabeta-pruning",
    "href": "ai-08-adversarial-search.html#alphabeta-pruning",
    "title": "8  Adversarial Search",
    "section": "8.3 Alpha–Beta Pruning",
    "text": "8.3 Alpha–Beta Pruning\n\nProblem with minimax search\n\nThe number of game states is exponential in the tree’s depth\n\nDo not examine every node\n\nAlpha-beta pruning: Prune away branches that cannot possibly influence the final decision\nBounded lookahead\n\nLimit depth for each search\nThis is what chess players do: look ahead for a few moves and see what looks best\n\n\n\n\nTrick to compute\n\n\\begin{align*}\n\\text{Minimax}(root) & =\\max(\\min(3,12,8),\\min(2,x,y),\\min(14,5,2))\\\\\n& =\\max(3,\\min(2,x,y),2)\\\\\n& =\\max(3,z,2)\\hspace*{1em}\\text{where}\\:z=\\min(2,x,y)\\leq2\\\\\n& =3\n\\end{align*}\n\n\nAlpha-beta pruning\n\nThe general case for alpha-beta pruning. If m is better than n for Player, we will never get to n in play.\nMinimax search is depth-first, so at any one time we just have to consider the nodes along a single path in the tree. Alpha-beta pruning has the two parameters that describe bounds on the backed-up values that appear anywhere along the path\n\n\\alpha = the value of the best (i.e., highest-value) choice we have found so far at any choice point along the path for MAX.\n\\beta = the value of the best (i.e., lowest-value) choice we have found so far at any choice point along the path for MIN.\n\n\n\nThe alpha-beta search algorithm\nfunction Alpha-Beta-Search(state) returns an action\n    α ← −∞\n    β ← +∞\n    bestAction ← null\n    bestValue ← −∞\n\n    for each a in Actions(state) do\n        value ← Min-Value(Result(state, a), α, β)\n        if value &gt; bestValue then\n            bestValue ← value\n            bestAction ← a\n        α ← Max(α, bestValue)\n\n    return bestAction\n\nfunction Max-Value(state, α, β) returns a utility value\n    if Terminal-Test(state) then\n        return Utility(state)\n\n    v ← −∞\n    for each a in Actions(state) do\n        v ← Max(v, Min-Value(Result(state, a), α, β))\n        if v ≥ β then\n            return v\n        α ← Max(α, v)\n\n    return v\n\nfunction Min-Value(state, α, β) returns a utility value\n    if Terminal-Test(state) then\n        return Utility(state)\n\n    v ← +∞\n    for each a in Actions(state) do\n        v ← Min(v, Max-Value(Result(state, a), α, β))\n        if v ≤ α then\n            return v\n        β ← Min(β, v)\n\n    return v\n\n\nProperties\n\nPruning does not affect final result\n\nPruning can reduce the tree size while its worst case is as good as the minimax algorithm\n\nGood move ordering improves effectiveness of pruning\n\nWith perfect ordering’’, time complexity O(b^{m/2}) \\Rightarrow doubles solvable depth\nIn chess, Deep Blue achieved depth reduction from 38 to 6\n\nKiller move heuristic\n\nFirst, IDS search with 1 ply deep and record the best path. Then search 1 ply deeper with the recorded path to inform move ordering.\n\nTransposition table avoids re-evaluation a state",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adversarial Search</span>"
    ]
  },
  {
    "objectID": "ai-08-adversarial-search.html#imperfect-real-time-decisions",
    "href": "ai-08-adversarial-search.html#imperfect-real-time-decisions",
    "title": "8  Adversarial Search",
    "section": "8.4 Imperfect Real-time Decisions",
    "text": "8.4 Imperfect Real-time Decisions\n\nEvaluation functions\n\nBoth minimax and alpha-beta pruning search all the way to terminal states\n\nThis depth is usually not practical because moves must be made in a reasonable amount of time (~ minutes).\n\nCut off the search earlier with some depth limit\nUse an evaluation function Eval\n\nAn estimation for the desirability of position (win, lose, tie?)\n\n\n\n\\text{H-Minimax}(s,d)=\\left\\{ \\begin{array}{ll}\n\\text{Eval}(s) & \\text{if}\\;\\text{Cutoff-Test}(s,d)\\\\\n\\max_{a\\in\\text{Actions}(s)}\\text{H-Minimax}(\\text{Result}(s,a),d+1) & \\text{if}\\;\\text{Player}(s)=MAX\\\\\n\\min_{a\\in\\text{Actions}(s)}\\text{H-Minimax}(\\text{Result}(s,a),d+1) & \\text{if}\\;\\text{Player}(s)=MIN\n\\end{array}\\right.\n\n\nThe evaluation function should order the terminal states in the same way as the true utility function does\n\nStates that are wins must evaluate better than draws, which in turn must be better than losses.\n\nThe computation must not take too long!\nFor nonterminal states, the evaluation function should be strongly correlated with the actual chances of winning\nFor chess, typically linear weighted sum of features \n\\text{Eval}(s)=w_{1}f_{1}(s)+w_{2}f_{2}(s)+\\ldots+w_{n}f_{n}(s)=\\sum_{i=1}^{n}w_{i}f_{i}(s)\n where each w_{i} is a weight and each f_{i} is a feature of each kind of piece (move number or* number of remaining pieces*)\nImplicit strong assumption: the contribution of each feature is independent of the values of the other features.\nCurrent programs for chess and other games also use nonlinear combinations of features\n\n\nTwo chess positions that differ only in the position of the rook at lower right. In (a), Black has an advantage of a knight and two pawns, which should be enough to win the game. In (b), White will capture the queen, giving it an advantage that should be strong enough to win.\n\nThe horizon effect. With Black to move, the black bishop is surely doomed. But Black can forestall that event by checking the white king with its pawns, forcing the king to capture the pawns. This pushes the inevitable loss of the bishop over the horizon, and thus the pawn sacrifices are seen by the search algorithm as good moves rather than bad ones.\n\n\nCutting off search\nCuttingoff-Search is almost identical to Alpha-Beta-Search except\n\nTerminal-Test is replace by Cutoff-Test\nUtility is replaced by Eval\n\nif Cutoff-Test (state, depth) then return Eval(state)\n\nThe depth is incremented on each recursive call\nCutoff-Test(state, depth) returns true for all depth greater than some fixed depth d\n\n\n\nHow to improve?\n\nUsing a more sophisticated cutoff test\n\nQuiescent positions are those unlikely to exhibit wild swings in value in the near future.\nQuiescence search: Expand nonquiescent positions until quiescent positions are reached.\n\nBeam search\n\nForward pruning, consider only a “beam” of the n best moves only\nMost humans consider only a few moves from each position\nPROBCUT, or probabilistic cut, algorithm (Buro, 1995)\n\nSearch vs. lookup\n\nUse table lookup rather than search for the opening and ending",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adversarial Search</span>"
    ]
  },
  {
    "objectID": "ai-08-adversarial-search.html#stochastic-games",
    "href": "ai-08-adversarial-search.html#stochastic-games",
    "title": "8  Adversarial Search",
    "section": "8.5 Stochastic Games",
    "text": "8.5 Stochastic Games\n\nStochastic behaviors\n\nUncertain outcomes controlled by chance, not an adversary!\nWhy wouldn’t we know what the result of an action will be?\n\nExplicit randomness: rolling dice\nUnpredictable opponents: the ghosts respond randomly\nActions can fail: when moving a robot, wheels might slip\n\n\n\n\nExpectimax Search Trees\n\nA game tree must include chance nodes in addition to MAX and MIN nodes\n\n\nChance nodes introduced by fair coin-flipping\n\n\nExpectimax search\n\nValues should now reflect average-case (expectimax) outcomes, not worst-case (minimax) outcomes\nExpectimax search:\n\nMAX/MIN nodes as in minimax search\nCHANCE nodes: we compute the expected value, which is the sum of the value over all outcomes, weighted by the probability of each chance action\n\nThe underlying uncertain-result problems can be formulated as Markov Decision Processes\n\n\n\\text{Expectiminimax}(s)=\\left\\{ \\begin{array}{ll}\n\\text{Utility}(s) & \\text{if}\\;\\text{Terminal-Test}(s)\\\\\n\\max_{a\\in\\text{Actions}(s)}\\text{Expectiminimax}(\\text{Result}(s,a)) & \\text{if}\\;\\text{Player}(s)=MAX\\\\\n\\min_{a\\in\\text{Actions}(s)}\\text{Expectiminimax}(\\text{Result}(s,a)) & \\text{if}\\;\\text{Player}(s)=MIN\\\\\n\\sum_{r}P(r)\\text{Expectiminimax}(\\text{Result}(s,r)) & \\text{if}\\;\\text{Player}(s)=CHANCE\n\\end{array}\\right.\n\n\n\nExpectimax Pruning?\n\nNot easy\n\nexact: need bounds on possible values\napproximate: sample high-probability branches\n\n\n\n\n\nDepth-Limited Expectimax\n\n\n\nEvaluation functions for games of chance\nMinimax:\n\nExact values don’t matter\nBehaviour is preserved under any monotonic transformation of \n\n\nExpectimax:\n\nExact values DO matter\nBehaviour is preserved only by positive linear transformation of",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adversarial Search</span>"
    ]
  },
  {
    "objectID": "ai-08-adversarial-search.html#references",
    "href": "ai-08-adversarial-search.html#references",
    "title": "8  Adversarial Search",
    "section": "8.6 References",
    "text": "8.6 References",
    "crumbs": [
      "Solving problems by searching",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adversarial Search</span>"
    ]
  },
  {
    "objectID": "ai-09-logic.html",
    "href": "ai-09-logic.html",
    "title": "9  Logical Agents",
    "section": "",
    "text": "9.1 Knowledge-based Agents",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logical Agents</span>"
    ]
  },
  {
    "objectID": "ai-09-logic.html#knowledge-based-agents",
    "href": "ai-09-logic.html#knowledge-based-agents",
    "title": "9  Logical Agents",
    "section": "",
    "text": "Problem-solving agents\n\nThe problem-solving agents know things, but only in a very limited, inflexible sense.\n\nE.g., the 8-puzzle agent cannot deduce that with odd parity cannot be reached from states with even parity\n\nCSP enables some parts of the agent to work domain-independently\n\nRepresent states as assignments of values to variables\nAllow for more efficient algorithms\n\n\n\n\nMotivation\n\nNeed to:\n\nDigest heterogenous information\nReason deeply with that information\n\n\n\nLanguage\n\nNatural languages (informal):\n\nEnglish: “Two divides even numbers”.\nVietnamese: “Hai là số chẵn”.\n\nProgramming languages (formal):\n\nPython:\ndef even(x): \n    return x % 2 == 0\nC++:\nbool even(int x) { \n    return x % 2 == 0; \n}\n\nLogical languages (formal):\n\nFirst-order-logic:\n\\forall x\\: \\text{Even}(x)\\to\\text{Divides}(x,2)\n\n\n\n\nTwo goals of a logic language\n\nRepresent knowledge about the world\n\nReason with that knowledge\n\n\n\n\nStrength and Problem\n\nStrength: provides expressiveness in a compact way\nProblem 1: deterministic, didn’t handle uncertainty (probability addresses this)\nProblem 2: rule-based, didn’t allow fine tuning from data (machine learning addresses this)\n\n\n\nKnowledge-based agents\n\nSupported by logic\nKnowledge-based agents can combine and recombine information to suit myriad purposes.\n\nAccept new tasks in the form of explicitly described goals\nAchieve competence by learning new knowledge of the environment\nAdapt to changes by updating the relevant knowledge\n\nKnowledge base (KB): A set of sentences or facts in a formal language\n\nEach sentence represents some assertion about the world.\nAxiom = the sentence that is not derived from other sentences\n\nInference: Using inference engine to derive (infer) new sentences from old ones\n\nAdd new sentences to the knowledge base and query what is known\n\n\n\n\n\nA generic knowledge-based agent\n\nGiven a percept, the agent adds the percept to its knowledge base, asks the knowledge base for the best action, and tells the knowledge base that it has in fact taken that action.\n\nfunction kb-agent(percept) returns an action\n\npersistent: KB, a knowledge base\n            t, a counter, initially 0, indicating time\n\n    Tell(KB, Make-Percept-Sentence(percept, t))\n    action ← Ask(KB, Make-Action-Query(t))\n    Tell(KB, Make-Action-Sentence(action, t))\n    t ← t + 1\n    return action\n\n\nBuilding an Agent\n\nProcedural approach\n\nEncode desired behaviors directly as program code.\n\nDeclarative approach to building an agent\n\nTell it what it needs to know, then it can Ask itself what to do – answers should follow from the KB\n\nCombined approach \\to Partially autonomous\nLearning approach \\to Fully autonomous\n\nProvide a knowledge-based agent with mechanisms that allow it to learn for itself",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logical Agents</span>"
    ]
  },
  {
    "objectID": "ai-09-logic.html#the-wumpus-world",
    "href": "ai-09-logic.html#the-wumpus-world",
    "title": "9  Logical Agents",
    "section": "9.2 The Wumpus World",
    "text": "9.2 The Wumpus World\n\nWumpus World PEAS description\nThe wumpus world is a cave consisting of rooms connected by passageways\n\nPerformance measure\n\n+1000 for climbing out of the gold\n-1000 for falling into a pit or being eaten by the wumpus\n-1 each action taken\n-10 for using the arrow\nThe game ends when agent dies or climbs out of the cave\n\n\nEnvironment\n\nA 4\\times4 grid of rooms\nAgent starts in the square [1,1], facing to the right\nThe locations of gold and wumpus are random\nEach square can be a pit, with probability 0.2\n\n\nActuators: The agent can\n\nForward\nLeft turn by 90°\nRight turn by 90°\nShooting kills wumpus if you are facing it (the agent has only one arrow)\nGrabbing picks up gold if in same square\nReleasing drops the gold in same square\n\n\nSensors: The agent has five sensors\n\nIn the square containing the wumpus and in the directly (not diagonally) adjacent squares, the agent will perceive a Stench.\nIn the squares directly adjacent to a pit, the agent will perceive a Breeze.\nIn the square where the gold is, the agent will perceive a Glitter.\nWhen an agent walks into a wall, it will perceive a Bump.\nWhen the wumpus is killed, it emits a woeful Scream that can be perceived anywhere in the cave [Stench, Breeze, None, None, None]\n\n\n\n\n\nCharacterize the Wumpus World\n\nFully Observable\n\nNo – only local perception\n\nDeterministic\n\nYes – outcomes exactly specified\n\nEpisodic\n\nNo – sequential at the level of actions\n\nStatic\n\nYes – Wumpus and Pits do not move\n\nDiscrete\n\nYes\n\nSingle-agent\n\nYes – Wumpus is essentially a natural feature\n\n\n\n\nExploring a wumpus world\n\nThe first step taken by the agent in the wumpus world. (a) The initial situation, after percept [None, None, None, None, None]. (b) After one move, with percept [None, Breeze, None, None, None].\n\nTwo later stages in the progress of the agent. (a) After the third move, with percept [Stench, None, None, None, None]. (b) After the fifth move, with percept [Stench, Breeze, Glitter, None, None].\n\nBreeze in (1,2) and (2,1) \\implies no safe actions, assuming pits uniformly distributed, (2,2) has pit with probability 0.86 vs. 0.31\n\nSmell in (1,1) \\impliescannot move; can use a strategy of coercion:\n\nshoot straight ahead\nwumpus was there \\implies dead \\implies safe\nwumpus wasn’t there \\implies safe",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logical Agents</span>"
    ]
  },
  {
    "objectID": "ai-09-logic.html#logic",
    "href": "ai-09-logic.html#logic",
    "title": "9  Logical Agents",
    "section": "9.3 Logic",
    "text": "9.3 Logic\n\nLogic\n\nDefinition\nLogics are formal languages for representing information such that conclusions can be drawn\n\nSyntax defines the sentences (statements, formulas) in the language\nSemantics define the “meaning” of sentences; i.e., define truth of a sentence in a world\n\n\nThe language of arithmetic\n\nx+2\\geq y is a sentence\nx^{2}+y&gt; is not a sentence\nx+2\\geq y is true in a world where x=7,y=1\nx+2\\geq y is false in a world where x=0,y=6\nx+2\\geq y is true iff the number x+2 is no less than the number y\n\n\n\nModels\n\nDefinition\nModels are formally structured worlds with respect to which truth can be evaluated. A model m in propositional logic is an assignment of truth values to propositional symbols\n\n\n3 propositional symbols: A, B, C\n2^{3}=8 possible models m_{i}:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nA\nB\nC\n\nModel\nA\nB\nC\n\n\n\n\nm_{1}\ntrue\ntrue\ntrue\n\nm_{5}\nfalse\ntrue\ntrue\n\n\nm_{2}\ntrue\ntrue\nfalse\n\nm_{6}\nfalse\ntrue\nfalse\n\n\nm_{3}\ntrue\nfalse\ntrue\n\nm_{7}\nfalse\nfalse\ntrue\n\n\nm_{4}\ntrue\nfalse\nfalse\n\nm_{8}\nfalse\nfalse\nfalse\n\n\n\n\n\nInterpretation function/semantic\n\nDefinition\nLet \\alpha be a sentence and m be a model. An interpretation function \\mathcal{I}(\\alpha,m) returns:\n\ntrue (1) say that m satisfies \\alpha or sometimes m is a model of \\alpha\nfalse (0) say that m does not satisfies \\alpha\n\nGiven a sentence \\alpha, \\mathcal{M}(\\alpha) is the set of all models of \\alpha\n\n\n\nEntailment\n\nDefinition\nLet \\alpha,\\beta be sentences, \\alpha entails \\beta\n\\begin{equation}\n\\alpha\\models\\beta\n\\end{equation}\niff in every model where \\alpha is true, \\beta is also true or\n\\begin{equation}\n\\mathcal{M}(\\alpha)\\subseteq\\mathcal{M}(\\beta)\n\\end{equation}\n\n\n\n\nContradiction\n\nDefinition\nLet \\alpha,\\beta be sentences, \\alpha contradicts \\beta iff \\mathcal{M}(\\alpha)\\cap\\mathcal{M}(\\beta)=\\emptyset.\n\n\n\n\nContradiction vs. entailment\n\nTheorem\nLet \\alpha,\\beta be sentences, \\alpha contradicts \\beta iff \\alpha entails \\lnot\\beta.\n\n\n\n\nContingency\n\nDefinition\nLet \\alpha,\\beta be sentences, \\beta is contingent on \\alpha iff\n\\begin{equation}\n\\emptyset\\neq\\mathcal{M}(\\alpha)\\cap\\mathcal{M}(\\beta)\\neq\\mathcal{M}(\\alpha)\n\\end{equation}\n\n\n\n\nTell operation\n\nTell: \\alpha=\\text{\"It is raining\"}. Tell[KB, Rain]\nPossible responses:\n\nAlready knew that: entailment (KB\\models\\alpha)\nDon’t believe that: contradiction (KB\\models\\lnot\\alpha)\nLearned something new (update KB): contingent KB\\gets KB,\\alpha\n\n\n\nAsk operation\n\nAsk: \\alpha=\\text{“Is it raining?”}. Ask[KB, Rain]\nPossible responses:\n\nYes: entailment (KB\\models\\alpha)\nNo: contradiction (KB\\models\\lnot\\alpha)\nI don’t know: contingent\n\n\n\nWumpus models\n\nSituation after the agent detecting nothing in [1,1], moving right and feel breeze in [2,1]\nConsider possible models? (assuming only pits)\n\n3 Boolean choices \\implies 8 possible models\n\n \n\n\nKnowledge base\n\nThe agent building knowledge base KB from wumpus-world rules + observations\n\n\n\n\nEntailment\n\n\\alpha_{1} = “[1,2] is safe”, KB\\models\\alpha_{1}, proved by model checking\n\n\n\n\nContingency\n\n\\alpha_{2} = “[2,2] is safe”, KB\\not\\models\\alpha_{2}",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logical Agents</span>"
    ]
  },
  {
    "objectID": "ai-09-logic.html#propositional-logic",
    "href": "ai-09-logic.html#propositional-logic",
    "title": "9  Logical Agents",
    "section": "9.4 Propositional Logic",
    "text": "9.4 Propositional Logic\n\nSyntax\nPropositional logic (a formal language) is the simplest logic – illustrates basic ideas.\nThe syntax of propositional logic defines\n\nConstants: True, False\nSymbols: stand for propositions A, B, B_{1,1}, P_{2,1}\nLogical connectives (operator)\n\n\n\n\nconnectives\nmeaning\nexample\n\n\n\n\n\\lnot\nnegation (NOT)\n\\lnot S\n\n\n\\land\nconjunction (AND)\nS_{1}\\land S_{2}\n\n\n\\lor\ndisjunction (OR)\nS_{1}\\lor S_{2}\n\n\n\\implies\nimplication\nS_{1}\\implies S_{2}\n\n\n\\iff\nequivalence, biconditional\nS_{1}\\iff S_{2}\n\n\n\n\nA BNF (Backus–Naur Form) grammar of sentences in propositional logic, along with operator precedences, from highest to lowest.\n\n\n\\begin{array}{rcl}\nSentence & \\to & AtomicSentence\\mid ComplexSentence\\\\\nAtomicSentence & \\to & True\\mid False\\mid P\\mid Q\\mid R\\mid\\dots\\\\\nComplexSentence & \\to & (Sentence)\\mid[Sentence]\\\\\n& | & \\lnot Sentence\\\\\n& | & Sentence\\land Sentence\\\\\n& | & Sentence\\lor Sentence\\\\\n& | & Sentence\\implies Sentence\\\\\n& | & Sentence\\iff Sentence\\\\\n\\text{Operator Precedence} & : & \\lnot,\\land,\\lor,\\implies,\\iff\n\\end{array}\n\n\n\nSemantic\n\nThe semantics defines the rules for determining the truth of a sentence with respect to a particular model m\n\nEach model m specifies true/false for each proposition symbol\nArbitrary sentence can be evaluateed by recursive process PL-True and truth tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\\lnot P\nP\\land Q\nP\\lor Q\nP\\implies Q\nP\\iff Q\n\n\n\n\nfalse\nfalse\ntrue\nfalse\nfalse\ntrue\ntrue\n\n\nfalse\ntrue\ntrue\nfalse\ntrue\ntrue\nfalse\n\n\ntrue\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\n\n\ntrue\ntrue\nfalse\ntrue\ntrue\ntrue\ntrue\n\n\n\nfunction PL-True?(alpha, model) returns true/false\n\n    if alpha is a symbol then return Lookup(alpha, model)\n    if Op(alpha) = '¬' then return Not(PL-True?(Arg1(alpha), model))\n    if Op(alpha) = '∧' then return And(PL-True?(Arg1(alpha), model),\n                                     PL-True?(Arg2(alpha), model))\n    if Op(alpha) = '∨' then return Or(PL-True?(Arg1(alpha), model),\n                                  PL-True?(Arg2(alpha), model))\n    if Op(alpha) = '⇒' then return ...\n    if Op(alpha) = '⇔' then return ...\n\n\nEntailment\n\nProblem\nGiven a set of sentences KB and \\alpha. Prove that  KB\\models\\alpha \n\nMethod 1: model-checking\n\nTime complexity: O(2^{n}) (if KB and \\alpha contain n symbols \\to there are 2^{n} models)\nSpace complexity: O(n) (depth-first)\nOK for propositional logic; not easy for first-order logic\n\nMethod 2: theorem-proving\n\nSearch for a sequence of proof steps (applications of inference rules)\n\n\n\nModel checking\nfunction TT-Entails?(KB, alpha) returns true/false\n\ninputs: KB, the knowledge base, a sentence in propositional logic\n        alpha, the query, a sentence in propositional logic\n\n    symbols ← a list of the propositional symbols in KB and alpha\n    return TT-Check-All(KB, alpha, symbols, ⌀)\n\nfunction TT-Check-All(KB, alpha, symbols, model) returns true/false\n\n    if Empty?(symbols) then\n        if PL-True?(KB, model) then return PL-True?(alpha, model)\n        else return true\n    else\n        P ← First(symbols)\n        rest ← Rest(symbols)\n        return (TT-Check-All(KB, alpha, rest, model U {P=true})\n                and\n                TT-Check-All(KB, alpha, rest, model U {P=false}))\n\n\nValidity\n\nDefinition\nA sentence is valid if it is true in all models. Valid sentences are also known as tautologies\n\n\nTheorem (Deduction theorem)\nFor any sentences \\alpha and \\beta, \\alpha\\models\\beta if and only if the sentence (\\alpha\\implies\\beta) is valid.\n\n\n\nSatisfiability\n\nDefinition\nA sentence is satisfiable if it is true in, or satisfied by, some model\n\n\nThe SAT problem\nThe problem of determining the satisfiability of sentences in propositional logic was the first problem proved to be NP-complete\n\n\n\nValidity, satisfiability and entailment\n\nGiven two sentences \\alpha,\\beta\n\n\\alpha is valid iff \\lnot\\alpha is unsatisfiable\n\\alpha is satisfiable iff \\lnot\\alpha is not valid\n\\alpha\\models\\beta iff the sentence (\\alpha\\land\\lnot\\beta) is unsatisfiable (refutation or contradiction)\n\n\n\n\nA simple knowledge base in Wumpus world\nSymbols for each [x,y] location:\n\nP_{x,y} is true if there is a pit in [x,y].\nW_{x,y} is true if there is a wumpus in [x,y], dead or alive.\nB_{x,y} is true if the agent perceives a breeze in [x,y].\nS_{x,y} is true if the agent perceives a stench in [x,y].\n\nSentences in Wumpus world\n\n\\begin{array}{rcl}\ns_{1} & : & \\lnot P_{1,1}\\\\\ns_{2} & : & B_{1,1}\\iff(P_{1,2}\\lor P_{2,1})\\\\\ns_{3} & : & B_{2,1}\\iff(P_{1,1}\\lor P_{2,2}\\lor P_{3,1})\\\\\ns_{4} & : & \\lnot B_{1,1}\\\\\ns_{5} & : & B_{2,1}\n\\end{array}\n\n\n\n\nInference in Wumpus world\n\nA truth table constructed for the knowledge base given in the text. KB is true if s_{1} through s_{5} are true, which occurs in just 3 of the 128 rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB_{1,1}\nB_{2,1}\nP_{1,1}\nP_{1,2}\nP_{2,1}\nP_{2,2}\nP_{3,1}\ns_{1}\ns_{2}\ns_{3}\ns_{4}\ns_{5}\nKB\n\n\n\n\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\nfalse\nfalse\n\n\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\nfalse\ntrue\nfalse\nfalse\n\n\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\n\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\nfalse\ntrue\ntrue\nfalse\n\n\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\n\n\nfalse\ntrue\nfalse\nfalse\nfalse\ntrue\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\n\n\nfalse\ntrue\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\n\n\nfalse\ntrue\nfalse\nfalse\ntrue\nfalse\nfalse\ntrue\nfalse\nfalse\ntrue\ntrue\nfalse\n\n\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\\vdots\n\n\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\nfalse\ntrue\ntrue\nfalse\ntrue\nfalse\n\n\n\n\nThe agent makes some conclusion\n\nKB\\models\\lnot P_{1,2} means there is no pit in [1,2]\nKB\\not\\models\\lnot P_{2,2} means there might (or might not) be a pit in [2,2]",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logical Agents</span>"
    ]
  },
  {
    "objectID": "ai-09-logic.html#propositional-inference",
    "href": "ai-09-logic.html#propositional-inference",
    "title": "9  Logical Agents",
    "section": "9.5 Propositional Inference",
    "text": "9.5 Propositional Inference\n\nInference framework\n\nDefinition\nIf \\ell_{1},\\cdots,\\ell_{k},m are sentences, then the following is an inference rule: \\begin{equation}\n\\frac{\\ell_{1},\\cdots,\\ell_{k}}{m}\\quad\\frac{(\\text{premises})}{(\\text{conclusion})}\n\\end{equation}\n\n\nNote: Rules operate directly on syntax, not on semantics.\n\n\n\nForward inference\n\nInput: set of inference rules Rs.\nRepeat until no changes to KB:\n\nChoose set of formulas \\ell_{1},\\cdots,\\ell_{k}\\in KB.\nIf matching rule \\frac{\\ell_{1},\\cdots,\\ell_{k}}{m} exists then add m to KB.\n\n\n\nDefinition\nKB derives/proves a sentence \\alpha, denoted by \\begin{equation}\nKB\\vdash_{Rs}\\alpha\n\\end{equation} iff \\alpha eventually gets added to KB.\n\n\n\nSoundness\n\nDefinition\nA set of inference rules Rs is sound if whenever KB\\vdash_{Rs}\\alpha, it is also true that KB\\models\\alpha or \\begin{equation}\n\\{\\alpha\\mid KB\\vdash_{Rs}\\alpha\\}\\subseteq\\{\\alpha\\mid KB\\models\\alpha\\}\n\\end{equation}\n\n\n\nCompleteness\n\nDefinition\nA set of inference rules Rs is complete if whenever KB\\models\\alpha, it is also true that KB\\vdash_{Rs}\\alpha \\begin{equation}\n\\{\\alpha\\mid KB\\models\\alpha\\}\\subseteq\\{\\alpha\\mid KB\\vdash_{Rs}\\alpha\\}\n\\end{equation}\n\n\n\nWorld and representation\n\nif KB is true in the real world, then any sentence \\alpha derived from KB by a sound inference procedure is also true in the real world\nSentences are physical configurations of the agent, and reasoning is a process of constructing new physical configurations from old ones. Logical reasoning should ensure that the new configurations represent aspects of the world that actually follow from the aspects that the old configurations represent.\n\n\n\n\nLogical equivalence\n\nDefinition\nTwo sentences \\alpha and \\beta are logically equivalent if they are true in the same set of models. We denotes as \\alpha\\equiv\\beta\n\n\n\\begin{array}{rcl}\n(\\alpha\\land\\beta) & \\equiv & (\\beta\\land\\alpha)\\text{ commutativity of }\\land\\\\\n(\\alpha\\lor\\beta) & \\equiv & (\\beta\\lor\\alpha)\\text{ commutativity of }\\lor\\\\\n((\\alpha\\land\\beta)\\land\\gamma) & \\equiv & (\\alpha\\land(\\beta\\land\\gamma))\\text{ associativity of }\\land\\\\\n((\\alpha\\lor\\beta)\\lor\\gamma) & \\equiv & (\\alpha\\lor(\\beta\\lor\\gamma))\\text{ associativity of }\\lor\\\\\n\\lnot(\\lnot\\alpha) & \\equiv & \\alpha\\text{ double-negation elimination}\\\\\n(\\alpha\\implies\\beta) & \\equiv & (\\lnot\\beta\\implies\\lnot\\alpha)\\text{ contraposition}\\\\\n(\\alpha\\implies\\beta) & \\equiv & (\\lnot\\alpha\\lor\\beta)\\text{ implication elimination}\\\\\n(\\alpha\\iff\\beta) & \\equiv & ((\\alpha\\implies\\beta)\\land(\\beta\\implies\\alpha))\\text{ biconditional elimination}\\\\\n\\lnot(\\alpha\\land\\beta) & \\equiv & (\\lnot\\alpha\\lor\\lnot\\beta)\\text{ De Morgan}\\\\\n\\lnot(\\alpha\\lor\\beta) & \\equiv & (\\lnot\\alpha\\land\\lnot\\beta)\\text{ De Morgan}\\\\\n(\\alpha\\land(\\beta\\lor\\gamma)) & \\equiv & ((\\alpha\\land\\beta)\\lor(\\alpha\\land\\gamma))\\text{ distributivity of }\\land\\text{ over }\\lor\\\\\n(\\alpha\\lor(\\beta\\land\\gamma)) & \\equiv & ((\\alpha\\lor\\beta)\\land(\\alpha\\lor\\gamma))\\text{ distributivity of }\\lor\\text{ over }\\land\n\\end{array}\n\n\n\nInference rule approach\n\nTheorem proving: Apply rules of inference directly to the sentences in KB to construct a proof of the desired sentence without consulting models.\n\nMore efficient than model checking when the number of models is large but the length of the proof is short\nLegitimate (sound) generation of new sentences from old\n\nProof = a sequence of inference rule applications to the desired goal\n\nCan use inference rules as operators in a standard search algorithm. Typically require translation of sentences into a normal form\n\nNote: Logical systems is monotonicity, which says that the set of entailed sentences can only increase as information is added to the knowledge base. For any sentences \\alpha and \\beta, if KB\\models\\alpha then KB\\land\\beta\\models\\alpha\n\n\n\nImportant inference rules\n\n\n\n\n\n\n\nModus ponens\n\\dfrac{\\alpha\\implies\\beta,\\hspace*{1em}\\alpha}{\\beta}\n\n\n\n\nModus tollens\n\\dfrac{\\alpha\\implies\\beta,\\hspace*{1em}\\lnot\\beta}{\\lnot\\alpha}\n\n\nAnd-introduction\n\\dfrac{\\alpha,\\hspace*{1em}\\beta}{\\alpha\\land\\beta}\n\n\nAnd-elimination\n\\dfrac{\\alpha\\land\\beta}{\\alpha}\n\n\n\n\n\nExample 1\nProblem Given KB=\\{P\\land Q,P\\implies R,Q\\land R\\implies S\\}, prove that KB\\models S\nSolution\n\n\n\n#\nSentence\nExplanation\n\n\n\n\ns_{1}\nP\\land Q\nfrom KB\n\n\ns_{2}\nP\\implies R\nfrom KB\n\n\ns_{3}\nQ\\land R\\implies S\nfrom KB\n\n\ns_{4}\nP\n(1) and-elimination\n\n\ns_{5}\nR\n(4,2) modus ponens\n\n\ns_{6}\nQ\n(1) and-elimination\n\n\ns_{7}\nQ\\land R\n(5,6) and-introduction\n\n\ns_{8}\nS\n(3,7) modus ponens\n\n\n\n\n\nExample 2\nProblem In Wumpus wolrd, given KB=\\{s_{1},s_{2},s_{3},s_{4},s_{5}\\}, prove that KB\\models\\lnot P_{1,2}\nSolution\n\n\n\n\n\n\n\n\n#\nSentence\nExplanation\n\n\n\n\ns_{1}\n\\lnot P_{1,1}\nfrom KB\n\n\ns_{2}\nB_{1,1}\\iff(P_{1,2}\\lor P_{2,1})\nfrom KB\n\n\ns_{3}\nB_{2,1}\\iff(P_{1,1}\\lor P_{2,2}\\lor P_{3,1})\nfrom KB\n\n\ns_{4}\n\\lnot B_{1,1}\nfrom KB\n\n\ns_{5}\nB_{2,1}\nfrom KB\n\n\ns_{6}\n(B_{1,1}\\implies(P_{1,2}\\lor P_{2,1}))\\land((P_{1,2}\\lor P_{2,1})\\implies B_{1,1})\nBi-conditional elimination to s_{2}\n\n\ns_{7}\n(P_{1,2}\\lor P_{2,1})\\implies B_{1,1}\nAnd-elimination to s_{6}\n\n\ns_{8}\n\\lnot B_{1,1}\\implies\\lnot(P_{1,2}\\lor P_{2,1})\nContrapositives to s_{7}\n\n\ns_{9}\n\\lnot(P_{1,2}\\lor P_{2,1})\nModus ponens to s_{4},s_{8}\n\n\ns_{10}\n\\lnot P_{1,2}\\land\\lnot P_{2,1}\nDe Morgan’s rule to s_{9}\n\n\ns_{11}\n\\lnot P_{1,2}\nAnd-elimination to s_{10}\n\n\n\n\n\nProving by search\nAny search algorithms can be applied to find a sequence of steps that constitutes a proof:\n\nINITIAL STATE: the initial knowledge base KB.\nACTIONS: the set of actions consists of all the inference rules applied to all the sentences that match the top half of the inference rule.\nRESULT: the result of an action is to add the sentence in the bottom half of the inference rule.\nGOAL: the goal is a state that contains the sentence we are trying to prove.\n\n\n\nConjunctive Normal Form\n\nDefinition\nConjunctive Normal Form (CNF—universal)\n\n\\begin{array}{cc}\n\\text{conjunction}\\text{ of } & \\underbrace{\\text{disjunctions of }literals}\\\\\n& clauses\n\\end{array}\n\nA BNF (Backus–Naur Form) grammar for conjunctive normal form\n\n\\begin{array}{rcl}\nCNFSentence & \\to & Clause_{1}\\land...\\land Clause_{n}\\\\\nClause & \\to & Literal_{1}\\lor...\\lor Literal_{m}\\\\\nLiteral & \\to & Symbol\\mid\\lnot Symbol\\\\\nSymbol & \\to & P\\mid Q\\mid R...\n\\end{array}\n\n\n\n\nConversion to CNF\nGiven a sentence B_{1,1}\\iff(P_{1,2}\\lor P_{2,1})\n\nEliminate \\iff, replacing \\alpha\\iff\\beta with (\\alpha\\implies\\beta)\\land(\\beta\\implies\\alpha).  (B_{1,1}\\implies(P_{1,2}\\lor P_{2,1}))\\land((P_{1,2}\\lor P_{2,1})\\implies B_{1,1}) \nEliminate \\implies, replacing \\alpha\\implies\\beta with \\lnot\\alpha\\lor\\beta.  (\\lnot B_{1,1}\\lor P_{1,2}\\lor P_{2,1})\\land(\\lnot(P_{1,2}\\lor P_{2,1})\\lor B_{1,1}) \nMove \\lnot inwards using de Morgan’s rules and double-negation:  (\\lnot B_{1,1}\\lor P_{1,2}\\lor P_{2,1})\\land((\\lnot P_{1,2}\\land\\lnot P_{2,1})\\lor B_{1,1}) \nApply distributivity law (\\lor over \\land) and flatten:  (\\lnot B_{1,1}\\lor P_{1,2}\\lor P_{2,1})\\land(\\lnot P_{1,2}\\lor B_{1,1})\\land(\\lnot P_{2,1}\\lor B_{1,1}) \n\n\n\nResolution inference rule\nResolution inference rule (for CNF):\n\\begin{equation}\n\\frac{\\ell_{1}\\lor\\cdots\\lor{\\color{red}\\ell_{i}}\\lor\\cdots\\lor\\ell_{k},\\qquad m_{1}\\lor\\cdots\\lor{\\color{green}m_{j}}\\lor\\cdots\\lor m_{n}}{\\ell_{1}\\lor\\cdots\\lor\\ell_{i-1}\\lor\\ell_{i+1}\\lor\\cdots\\lor\\ell_{k}\\lor m_{1}\\lor\\cdots\\lor m_{j-1}\\lor m_{j+1}\\lor\\cdots\\lor m_{n}}\n\\end{equation}\nwhere \\ell_{i} and m_{j} are complementary literals\n\nTheorem\nResolution inference rule is sound and complete for CNF KB\n\n\n\nThe resolution algorithm\n\nProof by contradiction: To show that KB\\models\\alpha, prove that KB\\land\\lnot\\alpha is unsatisfiable\n\nfunction PL-Resolution(KB, alpha) returns true/false\n\ninputs: KB, the knowledge base, a sentence in propositional logic\n        alpha, the query, a sentence in propositional logic\n\n    clauses ← the set of clauses in the CNF representation of KB & !alpha\n    new ← ⌀\n    loop do\n        for each pair of clauses C_i, C_j in clauses do\n            resolvents ← PL-Resolve(C_i, C_j)\n            if resolvents contains the empty clause then return true\n            new ← new U resolvents\n        if new is subset of clauses then return false\n        clauses ← clauses U new\n\n\nExample 3\n\nKB=\\left\\{ (B\\_{1,1}\\iff(P\\_{1,2}\\lor P\\_{2,1}))\\land\\lnot B\\_{1,1}\\right\\} and \\alpha=\\lnot P_{1,2}\nPartial application of PL-Resolution to a simple inference in the wumpus world. \\lnot P_{1,2} is shown to follow from the first four clauses in the top row\n\nNote: many resolution steps are pointless.\n\n\n\nHorn Form\n\nIn many practical situations, the full power of resolution is not needed. Some real-world knowledge bases satisfy certain restrictions (Horn form) on the form of sentences\n\n\nDefinition\nHorn Form (restricted)\nconjunction of Horn clauses\nA BNF (Backus–Naur Form) grammar for Horn form \n\\begin{array}{rcl}\nHornClauseForm & \\to & DefiniteClauseForm\\mid Symbol\\\\\nDefiniteClauseForm & \\to & (Symbol_{1}\\land...\\land Symbol_{n})\\implies Symbol\\\\\nSymbol & \\to & P\\mid Q\\mid R...\n\\end{array}\n\n\n\n\nModus ponens inference rule\n\nModus ponens inference rule (for Horn Form)\n\\begin{equation}\n\\frac{\\alpha_{1},\\ldots,\\alpha_{n},\\alpha_{1}\\land\\cdots\\land\\alpha_{n}\\implies\\beta}{\\beta}\n\\end{equation}\nCan be used with forward chaining or backward chaining.\nThese algorithms are very natural and run in linear time\n\n\nTheorem\nModus ponens inference rule is sound and complete for Horn KB\n\n\n\nForward chaining (FC)\n\nIdea: fire any rule whose premises are satisfied in the KB, add its conclusion to the KB, until query is found\nExample: Given the following KB, prove that KB\\models Q\nP\\implies Q\nL\\land M\\implies P\nB\\land L\\implies M\nA\\land P\\implies L\nA\\land B\\implies L\nA\nB\nThe corresponding AND–OR graph.\n\n\n\n\nThe forward-chaining algorithm\nfunction PL-FC-Entails?(KB, q) returns true/false\n\ninputs: KB, the knowledge base, a set of propositional definite clauses\n        q, the query, a proposition symbol\n\n    count ← a table, where count[c] is the number of symbols in c's premise\n    inferred ← a table, where inferred[s] is initially false for all symbols\n    agenda ← a queue of symbols, initially symbols known to be true in KB\n\n    while agenda ≠ ⌀ do\n        p ← Pop(agenda)\n        if p = q then return true\n        if inferred[p] = false then\n            inferred[p] ← true\n        for each clause c in KB where p is in c.Premise do\n            decrement count[c]\n            if count[c] = 0 then add c.Conclusion to agenda\n    return false\n\n\nProof of completeness\nFC derives every atomic sentence that is entailed by KB\n\nFC reaches a fixed point where no new atomic sentences are derived\nConsider the final state as a model m, assigning true/false to symbols\nEvery clause in the original KB is true in m\nProof: Suppose a clause a_{1}\\land\\ldots\\land a_{k}\\Rightarrow b is false in m\n\nThen a_{1}\\land\\ldots\\land a_{k} is true in m and b is false in m\nTherefore the algorithm has not reached a fixed point!\n\nHence m is a model of KB\nIf KB\\models q, q is true in every model of KB, including m\n\nGeneral idea: construct any model of KB by sound inference, check \\alpha\n\n\nForward chaining example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackward chaining (BC)\nIdea: work backwards from the query q:\n\nTo prove q by BC,\ncheck if q is known already, or\nprove by BC all premises of some rule concluding q\nAvoid loops: check if new subgoal is already on the goal stack\nAvoid repeated work: check if new subgoal\n\nhas already been proved true, or\nhas already failed\n\n\n\n\nBackward chaining example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForward vs. backward chaining\n\nFC is data-driven, cf. automatic, unconscious processing,\n\ne.g., object recognition, routine decisions\nMay do lots of work that is irrelevant to the goal\n\nBC is goal-driven, appropriate for problem-solving,\n\ne.g., Where are my keys? How do I get into a PhD program?\nComplexity of BC can be much less than linear in size of KB",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logical Agents</span>"
    ]
  },
  {
    "objectID": "ai-09-logic.html#propositional-model-checking",
    "href": "ai-09-logic.html#propositional-model-checking",
    "title": "9  Logical Agents",
    "section": "9.6 Propositional Model Checking",
    "text": "9.6 Propositional Model Checking\n\nEfficient propositional inference\n\nProblem The SAT problem is checking satisfiability of sentence \\alpha\n\nTwo families of efficient algorithms for general propositional inference based on model checking\n\nComplete backtracking search algorithms\n\nDPLL algorithm (proposed by Davis, Putnam, Logemann and Loveland)\n\nIncomplete local search algorithms (hill-climbing)\n\nWalkSAT algorithm\n\n\n\nApplication of SAT: testing entailment, \\alpha\\models\\beta, can be done by testing unsatisfiability of \\alpha\\land\\lnot\\beta.\n\n\n\nDPLL\n\nThe DPLL algorithm\n\nDetermine whether an input propositional logic sentence (in CNF) is satisfiable\nA recursive, depth-first enumeration of possible models.\nImprovements over truth table enumeration\n\nEarly termination\nPure symbol heuristic\nUnit clause heuristic\n\nEarly termination\n\nA clause is true if any literal is true.\nA sentence is false if any clause is false.\nAvoid examination of entire subtrees in the search space\nE.g., (B\\lor C)\\land(B\\lor D) is true if B is true, regardless C and D\n\nPure symbol heuristic\n\nPure symbol: always appears with the same sign in all clauses.\nMake a pure symbol literal true \\to can never make a clause false\nFor example, given a sentence (A\\lor\\lnot B),(\\lnot B\\lor\\lnot C),(A\\lor C) \\to the symbols A and B are pure, C is impure.\n\nUnit clause heuristic\n\nUnit clause: only one literal in the clause \\to the only literal in a unit clause must be true \\to cause “cascade” of forced assignments (unit propagation)\nFor example, given a sentence B,\\lnot B\\lor\\lnot C, if the model contains B=true then C=false\n\n\n\n\nAlgorithm\nfunction DPLL-Satisfiable?(s) returns true/false\n\ninputs: s, a sentence in propositional logic.\n\n    clauses ← the set of clauses in the CNF representation of s\n    symbols ← a list of the proposition symbols in s\n    return DPLL(clauses, symbols, {})\n\nfunction DPLL(clauses, symbols, model) returns true/false\n\n    if every clause in clauses is true in model then return true\n    if some clause in clauses is false in model then return false\n    P, value ← Find-Pure-Symbol(symbols, clauses, model)\n    if P ≠ ⌀ then\n        return DPLL(clauses, symbols-P, model U {P=value})\n    P, value ← Find-Unit-Clause(clauses, model)\n    if P ≠ ⌀ then\n        return DPLL(clauses, symbols-P, model U {P=value})\n    P ← First(symbols)\n    rest ← Rest(symbols)\n    return DPLL(clauses, rest, model U {P=true}) \n        or DPLL(clauses, rest, model U {P=false})\n\n\nSuccess of DPLL\n\n1962 – DPLL invented\n1992 – 300 propositions\n1997 – 600 propositions (satz)\n2002 (zChaff) 1,000,000 propositions – encodings of hardware verification problems\n\n\n\n\nWalkSAT\n\nThe WalkSAT algorithm\n\nEvaluation function: The min-conflict heuristic of minimizing the number of unsatisfied clauses\nBalance between greediness and randomness\nWhen the algorithm returns a model\n\nThe input sentence is indeed satisfiable\n\nWhen it returns failure\n\nThe sentence is unsatisfiable OR we need to give it more time\n\nWalkSAT cannot always detect unsatisfiability\nIt is most useful when a solution is expected to exist. For example,\n\nAn agent cannot reliably use WalkSAT to prove that a square is safe in the Wumpus world.\nInstead, it can say, “I thought about it for an hour and couldn’t come up with a possible world in which the square isn’t safe.”\n\n\n\n\nAlgorithm\nfunction WalkSAT(clauses, p, max_flips) returns a satisfying model or failure\n\ninputs: clauses, a set of clauses in propositional logic\n        p, the probability of choosing to do a random walk move,\n        typically around 0.5\n        max_flips, number of flips allowed before giving up\n\n    model ← a random assignment of true/false to the symbols in clauses\n    for i = 1 to max_flips do\n        if model satisfies clauses then return model\n        clause ← a randomly selected clause from clauses that is false in model\n        with probability p\n            flip the value in model of a randomly selected symbol from clause\n        else\n            flip whichever symbol in clause maximizes the number of satisfied clauses\n    return failure",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logical Agents</span>"
    ]
  },
  {
    "objectID": "ai-09-logic.html#propositional-logic-based-agent",
    "href": "ai-09-logic.html#propositional-logic-based-agent",
    "title": "9  Logical Agents",
    "section": "9.7 Propositional Logic Based Agent",
    "text": "9.7 Propositional Logic Based Agent\n\nPropositional Logic Based Agent\n\nAgent has to act given only local perception\nAgent is installed with two kinds of knowledge base\n\n“Hardcode” knowledge base\nIF glitter THEN grab gold\nIF wumpus or pit around THEN avoid it\n“Softcode” knowledge base KB \n\\begin{array}{l}\n\\lnot P_{1,1},\\lnot W_{1,1}\\\\\nB_{x,y}\\iff(P_{x,y+1}\\lor P_{x,y-1}\\lor P_{x+1,y}\\lor P_{x-1,y})\\\\\nS_{x,y}\\iff(W_{x,y+1}\\lor W_{x,y-1}\\lor W_{x+1,y}\\lor W_{x-1,y})\\\\\nW_{1,1}\\lor W_{1,2}\\lor...\\lor W_{4,3}\\lor W_{4,4}\\\\\n\\lnot W_{1,1}\\lor\\lnot W_{1,2},...\n\\end{array}\n\n\n\n\nFor a 4\\times4 wumpus world, the KB begin with a total of 155 sentences containing 64 distinct symbols\n\n\nAlgorithm\nfunction PL-Wumpus-Agent(percept) returns an action\n\ninputs: percept, a list, [stench, breeze, glitter]\nstatic: KB, a knowledge base\n        x, y, , the agent’s position (initially 1,1)\n        orientation, orientation (initially right)\n        visited, an array indicating which squares have been visited,\n                 initially false\n        action, the agent’s most recent action, initially null\n        plan, an action sequence, initially empty\n\n    update x, y, orientation, visited based on action\n    if stench then Tell(KB, S_{x,y}) else Tell(KB, !S_{x,y})\n    if breeze then Tell(KB, B_{x,y}) else Tell(KB, !B_{x,y})\n    if glitter then action ← grab\n    else if plan ≠ ⌀ then action ← Pop(plan)\n    else if for some fringe square [i,j], Ask(KB, (!P_{i,j} & !W_{i,j})) is true or\n            for some fringe square [i,j], Ask(KB, (P_{i,j} | W_{i,j})) is false then\n        plan ← A*-Graph-Search(Route-Problem([x,y], orientation, [i,j], visited))\n        action ← Pop(plan)\n    else action ← a randomly chosen move\n    return action",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logical Agents</span>"
    ]
  },
  {
    "objectID": "ai-09-logic.html#references",
    "href": "ai-09-logic.html#references",
    "title": "9  Logical Agents",
    "section": "9.8 References",
    "text": "9.8 References",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Logical Agents</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html",
    "href": "ai-10-fl.html",
    "title": "10  First-order Logic",
    "section": "",
    "text": "10.1 Representation Revisited",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html#representation-revisited",
    "href": "ai-10-fl.html#representation-revisited",
    "title": "10  First-order Logic",
    "section": "",
    "text": "Programming languages\n\nProgramming language is a kind of formal languages.\n\n\nPrograms represent computational processes while their data structures represent facts.\n\nE.g., the Wumpus world can be represented by a 4\\times4 array, “World[2,2] \\gets Pit” states that “There is a pit in square [2,2].”\n\nLack of general mechanisms to derive facts from other facts\n\nUpdate to a data structure is done by a domain-specific procedure.\n\nLack of expressiveness to handle partial information\n\nE.g., to say “There is a pit in [2,2] or [3,1]”, a program stores a single value for each variable and allows the value to be “unknown”, while the propositional logic sentence, P_{2,2}\\lor P_{1,1}, is more intuitive.\n\n\n\n\nPropositional logic\n\nPropositional logic is a declarative language.\n\nSemantics is based on the truth relation between sentences and possible worlds.\n\nPropositional logic allows partial/disjunctive/negated information\n\nUnlike most data structures and databases\n\nPropositional logic is compositional, which is desirable in representation languages\n\nThe meaning of a sentence is a function of the meaning of its parts; e.g., the meanings of S_{1,4}\\land S_{1,2} relates the meanings of S_{1,4} and S_{1,2}.\n\nMeaning in propositional logic is context-independent\n\nUnlike natural language, where meaning depends on context\n\nPropositional logic has very limited expressive power\n\nE.g., cannot say “pits cause breezes in adjacent squares”\n\n\n\n\nFirst-order logic\nWhereas propositional logic assumes world contains facts, first-order logic (like natural language) assumes the world contains\n\nObjects: are referred by nouns and noun phrases\n\nE.g., people, houses, numbers, theories, Ronald McDonald, colors, baseball games, wars, centuries \\ldots\n\nRelations: can be unary relations (properties) or n-ary relations, representing by verbs and verb phrases\n\nProperites: red, round, bogus, prime, multistoried, etc.\nn-ary relations: brother of, bigger than, inside, part of, has color, occurred after, owns, comes between, etc.\n\nPredicates: are relations that return true/false\nFunctions: are relations that return object\n\n\n\nLogics in general\n\n\n\n\n\n\n\n\nLanguage\nOntological Commitment (What exists in the world)\nEpistemological Commitment (What an agent believes about facts)\n\n\n\n\nPropositional logic\nfacts\ntrue/false/unknown\n\n\nFirst-order logic\nfacts, objects, relations\ntrue/false/unknown\n\n\nTemporal logic\nfacts, objects, relations, time\ntrue/false/unknown\n\n\nProbability logic\nfacts\ndegree of belief \\in[0,1]\n\n\nFuzzy logic\nfacts + degree of truth \\in[0,1]\nknown interval value",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html#syntax-and-semantics",
    "href": "ai-10-fl.html#syntax-and-semantics",
    "title": "10  First-order Logic",
    "section": "10.2 Syntax and Semantics",
    "text": "10.2 Syntax and Semantics\n\nBNF Grammar\n\n\\begin{array}{rcl}\nSentence & \\to & AtomicSentence\\mid ComplexSentence\\\\\nAtomicSentence & \\to & Predicate\\mid Predicate(Term,...)\\mid Term_{1}=Term_{2}\\\\\nComplexSentence & \\to & (Sentence)\\mid[Sentence]\\\\\n& | & \\lnot Sentence\\\\\n& | & Sentence\\land Sentence\\\\\n& | & Sentence\\lor Sentence\\\\\n& | & Sentence\\implies Sentence\\\\\n& | & Sentence\\iff Sentence\\\\\n& | & Quantifier\\:Variable,...Sentence\\\\\n\\\\Term & \\to & Function(Term,...)\\\\\n& | & Constant\\\\\n& | & Variable\\\\\n\\\\Quantifier & \\to & \\forall\\mid\\exists\\\\\nConstant & \\to & A\\mid X_{1}\\mid John\\mid...\\\\\n{\\color{red}Variable} & \\to & {\\color{red}a}\\mid{\\color{red}x}\\mid{\\color{red}s}\\mid...\\\\\n{\\color{green}Predicate} & \\to & {\\color{green}True}\\mid{\\color{green}False}\\mid{\\color{green}After}\\mid{\\color{green}Loves}\\mid{\\color{green}Raining}\\mid...\\\\\n{\\color{green}Function} & \\to & {\\color{green}mother}\\mid{\\color{green}leftleg}\\mid...\\\\\n\\\\\\text{Operator Precedence} & : & \\lnot,=,\\land,\\lor,\\implies,\\iff\n\\end{array}\n\n\n\nTerms\n\nTerm is a logical expression that refers to an object\n\nConstants\nFunctions\nVariables\n\n\n\nGround term is a term without variables\n\n\n\nGraph representation of a model\n\nA model can be represented as a directed graph.\nThe following graph contains five objects, two binary relations, three unary relations (indicated by labels on the objects), and one unary function, left-leg.\n\n\n\n\nModels in First-order logic\n\nA model m in first-order logic maps:\n\nConstant symbols to objects m(R)=o_{1} and m(J)=o_{2}\nPredicate/function symbols to tuples of objects\n\n\\begin{align*}\n  m({\\color{green}Person}) & =\\{o_{1},o_{2}\\} \\\\\n  m({\\color{green}King}) & =\\{o_{2}\\} \\\\\n  m({\\color{green}Crown}) & =\\{o_{5}\\} \\\\\n  m({\\color{green}Brother}) & =\\{(o_{1},o_{2}),(o_{2},o_{1})\\} \\\\\n  m({\\color{green}onhead}) & =\\{(o_{5},o_{2})\\} \\\\  \n  m({\\color{green}leftleg}) & =\\{(o_{1},o_{3}),(o_{2},o_{4})\\} \\\\\n  \\end{align*}\n\n\nSimilar to propositional logic, entailment, validity, and so on are defined in terms of all possible models.\nThe number of possible models is unbounded \\to checking entailment by the enumeration is infeasible.\n137,506,194,466 models with six or fewer objects.\n\n\n\n\nA restriction on models\n“Richard and John are people.”\n\n{\\color{green}Person}(Richard)\\land{\\color{green}Person}(John)\n\n\n\nUnique names assumption: Each object has at most one constant symbol. This rules out m_{2}.\nDomain closure: Each object has at least one constant symbol. This rules out m_{3}.\n\nPoint: contstant symbol \\longleftrightarrow object\n\n\nQuantifiers\n\nUniversal quantification\n\\forall\\left\\langle {\\color{red}variables}\\right\\rangle \\;\\left\\langle sentence\\right\\rangle\n\\forall{\\color{red}x}\\:P({\\color{red}x}) is true in a model m iff P({\\color{red}x}) is true with {\\color{red}x} being each possible object in the model\n\n\nThink conjunction: \\forall{\\color{red}x}\\:P({\\color{red}x}) is like P(A)\\land P(B)\\land\\dots\n\n\nExistential quantification\n\\exists\\left\\langle {\\color{red}variables}\\right\\rangle \\;\\left\\langle sentence\\right\\rangle\n\\exists{\\color{red}x}\\:P({\\color{red}x}) is true in a model m iff P({\\color{red}x}) is true with {\\color{red}x} being some possible object in the model\n\n\nThink disjunction: \\exists{\\color{red}x}\\:P({\\color{red}x}) is like P(A)\\lor P(B)\\lor\\dots\n\n\n\nNatural language quantifiers\n\n“Everyone at Berkeley is smart”\n\n\\forall{\\color{red}x}\\;\\left({\\color{green}At}({\\color{red}x},Berkeley)\\implies{\\color{green}Smart}({\\color{red}x})\\right)\n\nequivalent to the conjunction of instantiations\n\n\\begin{array}{cl}\n& ({\\color{green}At}(King\\,John,Berkeley)\\implies{\\color{green}Smart}(King\\,John))\\\\\n\\land & ({\\color{green}At}(Richard,Berkeley)\\implies{\\color{green}Smart}(Richard))\\\\\n\\land & ({\\color{green}At}(Berkeley,Berkeley)\\implies{\\color{green}Smart}(Berkeley))\\\\\n\\land & \\ldots\n\\end{array}\n\n“Someone at Stanford is smart”\n\n\\exists{\\color{red}x}\\;\\left({\\color{green}At}({\\color{red}x},Stanford)\\land{\\color{green}Smart}({\\color{red}x})\\right)\n\nequivalent to the disjunction of instantiations\n\n\\begin{array}{cl}\n& ({\\color{green}At}(King\\,John,Berkeley)\\land{\\color{green}Smart}(King\\,John))\\\\\n\\lor & ({\\color{green}At}(Richard,Berkeley)\\land{\\color{green}Smart}(Richard))\\\\\n\\lor & ({\\color{green}At}(Berkeley,Berkeley)\\land{\\color{green}Smart}(Berkeley))\\\\\n\\lor & \\ldots\n\\end{array}\n\n\n\n\nA common mistake to avoid\n\nThe main connective with \\forall is \\implies; mistake: using \\land as the main connective with \\forall\n\\forall{\\color{red}x}\\;\\left({\\color{green}At}({\\color{red}x},Berkeley)\\land{\\color{green}Smart}({\\color{red}x})\\right) means “Everyone is at Berkeley and everyone is smart” (too strong implication)\nThe main connective with \\exists is \\land; mistake: using \\implies as the main connective with \\exists\n\\exists{\\color{red}x}\\;\\left({\\color{green}At}({\\color{red}x},Stanford)\\implies{\\color{green}Smart}({\\color{red}x})\\right) means “It is true even with anyone who is not at Stanford” (too weak implication)\n\n\n\nNested quantifiers\nMultiple quantifiers enable more complex sentences. The order of quantification is therefore very important.\n\nSimplest cases: Quantifiers are of the same type\n\\forall{\\color{red}x}\\forall{\\color{red}y}\\:\\left({\\color{green}Brother}({\\color{red}x},{\\color{red}y})\\implies{\\color{green}Sibling}({\\color{red}x},{\\color{red}y})\\right)\n\\forall{\\color{red}x}\\forall{\\color{red}y}\\:\\left({\\color{green}Sibling}({\\color{red}x},{\\color{red}y})\\iff{\\color{green}Sibling}({\\color{red}y},{\\color{red}x})\\right)\nMixtures\n\\forall{\\color{red}x}\\exists{\\color{red}y}\\:{\\color{green}Loves}({\\color{red}x},{\\color{red}y}) \\to “Everybody loves somebody”\n\\exists{\\color{red}x}\\forall{\\color{red}y}\\:{\\color{green}Loves}({\\color{red}x},{\\color{red}y}) \\to “There is someone loved by everyone”\n\nConfusion: can arise when two quantifiers are used with the same variable name\n\n\\forall{\\color{red}x}\\:({\\color{green}Crown}({\\color{red}x})\\lor(\\exists{\\color{red}x}\\:{\\color{green}Brother}(Richard,{\\color{red}x})))\n\nRule:\n\nThe variable belongs to the innermost quantifier that mentions it or\nUse different variable names with nested quantifier\n\n\n\\forall{\\color{red}x}\\:({\\color{green}Crown}({\\color{red}x})\\lor(\\exists{\\color{red}z}\\:{\\color{green}Brother}(Richard,{\\color{red}z})))\n\n\n\nProperties of quantifiers\n\nNested quantifiers\n\n\n\\begin{array}{lcr}\n  \\forall{\\color{red}x}\\forall{\\color{red}y}\\:P({\\color{red}x},{\\color{red}y}) & \\equiv & \\forall{\\color{red}y}\\forall{\\color{red}x}\\:P({\\color{red}x},{\\color{red}y})\\\\\n  \\exists{\\color{red}x}\\exists{\\color{red}y}\\:P({\\color{red}x},{\\color{red}y}) & \\equiv & \\exists{\\color{red}y}\\exists{\\color{red}x}\\:P({\\color{red}x},{\\color{red}y})\\\\\n  \\exists{\\color{red}x}\\forall{\\color{red}y}\\:P({\\color{red}x},{\\color{red}y}) & \\not\\equiv & \\forall{\\color{red}y}\\exists{\\color{red}x}\\:P({\\color{red}x},{\\color{red}y})\n\\end{array}\n\n\nDe Morgan’s rules\n\n\n\\begin{array}{rcl}\n  \\forall{\\color{red}x}\\:\\lnot P({\\color{red}x}) & \\equiv & \\lnot\\exists{\\color{red}x}\\:P({\\color{red}x})\\\\\n  \\lnot\\forall{\\color{red}x}\\:P({\\color{red}x}) & \\equiv & \\exists{\\color{red}x}\\:\\lnot P({\\color{red}x})\\\\\n  \\forall{\\color{red}x}\\:P({\\color{red}x}) & \\equiv & \\lnot\\exists{\\color{red}x}\\:\\lnot P({\\color{red}x})\\\\\n  \\lnot\\exists{\\color{red}x}\\:\\lnot P({\\color{red}x}) & \\equiv & \\exists{\\color{red}x}\\:P({\\color{red}x})\n\\end{array}\n\n\n\nEquality\n\n\nterm_{1}=term_{2} is true under a given interpretation if and only if term_{1} and term_{2} refer to the same object\n\\lnot(term_{1}=term_{2}) means term_{1} and term_{2} not refer to the same object (sometimes write as term_{1}\\neq term_{2})\n\n\n\n{\\color{green}father}(John)=Henry means that {\\color{green}father}(John) and Henry refer to the same object\n\n\n\nFun with sentences\n\nBrothers are siblings \n\\forall{\\color{red}x},{\\color{red}y}\\;\\left({\\color{green}Brother}({\\color{red}x},{\\color{red}y})\\implies{\\color{green}Sibling}({\\color{red}x},{\\color{red}y})\\right)\n\n“Sibling” is symmetric \n\\forall{\\color{red}x},{\\color{red}y}\\;\\left({\\color{green}Sibling}({\\color{red}x},{\\color{red}y})\\iff{\\color{green}Sibling}({\\color{red}y},{\\color{red}x})\\right)\n\nOne’s mother is one’s female parent \n\\forall{\\color{red}x},{\\color{red}y}\\;\\left({\\color{green}Mother}({\\color{red}x},{\\color{red}y})\\iff({\\color{green}Female}({\\color{red}x})\\land{\\color{green}Parent}({\\color{red}x},{\\color{red}y}))\\right)\n\nA first cousin is a child of a parent’s sibling \n\\forall{\\color{red}x},{\\color{red}y}\\;\\left({\\color{green}FirstCousin}({\\color{red}x},{\\color{red}y})\\iff\\exists{\\color{red}p},{\\color{red}ps}\\;\\left({\\color{green}Parent}({\\color{red}p},{\\color{red}x})\\land{\\color{green}Sibling}({\\color{red}ps},{\\color{red}p})\\land{\\color{green}Parent}({\\color{red}ps},{\\color{red}y})\\right)\\right)",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html#applications",
    "href": "ai-10-fl.html#applications",
    "title": "10  First-order Logic",
    "section": "10.3 Applications",
    "text": "10.3 Applications\n\nUsing First-Order Logic\nFirst-order knowledge base KB has // interface\n\nSentences (assertions) are added to a knowledge base KB using \\text{Tell}\n\n\\text{Tell}(KB,{\\color{green}King}(John))\n\\text{Tell}(KB,{\\color{green}Person}(Richard))\n\\text{Tell}(KB,\\forall{\\color{red}x}\\:({\\color{green}King}({\\color{red}x})\\implies{\\color{green}Person}({\\color{red}x})))\n\nWe can ask questions (queries or goals) of the knowledge base KB using \\text{Ask}\n\n\\text{Ask}(KB,{\\color{green}Person}(John)) \\to return true\n\\text{Ask}(KB,\\exists{\\color{red}x}\\:{\\color{green}Person}({\\color{red}x})) \\to return true\n\nIf we want to know what value of {\\color{red}x} makes the sentence true using \\text{AskVars}\n\n\\text{AskVars}(KB,{\\color{green}Person}({\\color{red}x})) \\to return a substitution list \\left\\{{\\color{red}x}/John\\right\\} and \\left\\{{\\color{red}x}/Richard\\right\\}\n\nThe assertions can be considered as the axioms\nLogical sentences which are entailed by the axioms are called theorems\nThe theorems do not increase the set of conclusions that follow from the knowledge base KB\n\nFrom a practical point of view, theorems are essential to reduce the computational cost of deriving new sentences\n\n\nThe Kinship Domain\n\nUnary predicates\n\nMale and Female\n\nBinary predicates represent kinship relations\n\nParenthood, brotherhood, marriage, etc.\nParent, Sibling, Brother , Sister, Child, Daughter, Son, Spouse, Wife, Husband, Grandparent , Grandchild , Cousin, Aunt, and Uncle.\n\nFunctions\n\nMother and Father, each person has exactly one of each of these.\n\n\n\n\n\nThe Little Kinship Domain\nThe possible axioms for Kinship domain\n\nOne’s mother is one’s female parent \n\\forall m,c\\,({\\color{green}mother}(c)=m\\iff{\\color{green}Female}(m)\\land{\\color{green}Parent}(m,c)).\n\nOne’s husband is one’s male spouse \n\\forall w,h\\,({\\color{green}Husband}(h,w)\\iff{\\color{green}Male}(h)\\land{\\color{green}Spouse}(h,w)).\n\nMale and female are disjoint categories \n\\forall x\\,({\\color{green}Male}(x)\\iff\\lnot{\\color{green}Female}(x)).\n\nParent and child are inverse relations \n\\forall p,c\\,({\\color{green}Parent}(p,c)\\iff{\\color{green}Child}(c,p)).\n\nA grandparent is a parent of one’s parent \n\\forall g,c\\,({\\color{green}GrandParent}(g,c)\\iff\\exists p\\;{\\color{green}Parent}(g,p)\\land{\\color{green}Parent}(p,c)).\n\nA sibling is another child of one’s parents \n\\forall x,y\\,({\\color{green}Sibling}(x,y)\\iff x\\neq y\\land\\exists p\\;{\\color{green}Parent}(p,x)\\land{\\color{green}Parent}(p,y)).\n\n\nUsing axioms to entail theorems\n\n\\text{axioms of kinship}\\models\\forall x\\forall y\\:({\\color{green}Sibling}(x,y)\\iff{\\color{green}Sibling}(y,x))\n\n\n\nNatural number theory\n\nTo present the theory of natural numbers, we need\n\na predicate {\\color{green}NatNum} that will be true of natural numbers\none constant symbol, 0\none function symbol, {\\color{green}s} (successor)\none addition function, \\mathbin{\\color{green}+}\n\nThe Peano axioms define natural numbers and addition. Natural numbers are defined recursively\n\n{\\color{green}NatNum}(0)\n\\forall n\\,({\\color{green}NatNum}(n)\\implies{\\color{green}NatNum}(S(n)))\n\\forall n\\,(0\\neq{\\color{green}s}(n))\n\\forall m,n\\,(m\\neq n\\implies{\\color{green}s}(m)\\neq{\\color{green}s}(n))\n\\forall m\\,(0\\neq{\\color{green}NatNum}(m)\\implies\\mathbin{\\color{green}+}(0,m)=m)\n\\forall m,n\\,({\\color{green}NatNum}(m)\\land{\\color{green}NatNum}(n)\\implies\\mathbin{\\color{green}+}({\\color{green}s}(m),n)={\\color{green}s}(\\mathbin{\\color{green}+}(m,n)))\n\n\n\n\nSet theory\n\nThe domain of sets is also fundamental to mathematics as well as to commonsense reasoning\nWe need\n\nThe empty set is a constant written as \\emptyset\nThe unary predicate, {\\color{green}Set}, which is true of sets.\nThe infix binary predicate x\\in s (x is a member of set s)\nThe infix binary predicate s_{1}\\subseteq s_{2} (set s_{1} is a subset of set s_{2})\nThe infix binary function s_{1}\\cap s_{2} (the intersection of two sets)\nThe infix binary function s_{1}\\cup s_{2} (the union of two sets)\nThe binary function $ $ (the set resulting from adjoining element x to set s)\n\n\nOne possible set of axioms is as follows\n\nThe only sets are the empty set and those made by adjoining something to a set \n   \\forall s\\,\\left({\\color{green}Set}(s)\\iff\\left(s=\\emptyset\\right)\\lor\\left(\\exists x,s_{2}\\,{\\color{green}Set}(s_{2})\\land s=\\left\\{ x\\mid s_{2}\\right\\} \\right)\\right)\n   \nThe empty set has no elements adjoined into it. In other words, there is no way to decompose \\emptyset into a smaller set and an element \n   \\lnot\\exists x,s\\;\\left\\{ x\\mid s\\right\\} =\\emptyset.\n   \nAdjoining an element already in the set has no effect \n   \\forall x,s\\;x\\in s\\iff s=\\left\\{ x\\mid s\\right\\} .\n   \nThe only members of a set are the elements that were adjoined into it. We express this recursively, saying that x is a member of s if and only if s is equal to some set s_{2} adjoined with some element y, where either y is the same as x or x is a member of s_{2} \n   \\forall x,s\\;\\left(x\\in s\\iff\\exists y,s_{2}\\left(s=\\left\\{ y\\mid s_{2}\\right\\} \\land\\left(x=y\\lor x\\in s_{2}\\right)\\right)\\right).\n   \nA set s_{1} is a subset of another set s_{2} if and only if all of the first set’s members are members of the second set \n   \\forall s_{1},s_{2}\\,\\left(s_{1}\\subseteq s_{2}\\iff(\\forall x\\;x\\in s_{1}\\Rightarrow x\\in s_{2})\\right).\n   \nTwo sets s_{1} and s_{2} are equal if and only if each is a subset of the other \n   \\forall s_{1},s_{2}\\,\\left(s_{1}=s_{2}\\iff(s_{1}\\subseteq s_{2}\\land s_{2}\\subseteq s_{1})\\right).\n   \nAn object x is in the intersection of two sets s_{1} and s_{2} if and only if it is a member of both sets \n   \\forall x,s_{1},s_{2}\\,\\left(x\\in(s_{1}\\cap s_{2})\\iff(x\\in s_{1}\\land x\\in s_{2})\\right).\n   \nAn object x is in the union of two sets s_{1} and s_{2} if and only if it is a member of either set \n   \\forall x,s_{1},s_{2}\\,\\left(x\\in(s_{1}\\cup s_{2})\\iff(x\\in s_{1}\\lor x\\in s_{2})\\right).\n   \n\n\n\nKnowledge base for the wumpus world\n\nThe corresponding first-order sentence stored in the knowledge base must include both the percept and the time t at which it occurred\nThe actions in the wumpus world are also represented by logical terms\n\n\nAgent\n\nPerception:\n\nPercept([s,b,g,m,c],t),Stench(t),Breeze(t),Glitter(t)\n\\text{Tell}(KB,\\forall t,s,g,m,c \\;Percept\\left([s,Breeze,g,m,c],t\\right)\\implies Breeze(t))\n\\text{Tell}(KB,\\forall t,s,b,m,c \\;Percept\\left([s,b,Glitter,m,c],t\\right)\\implies Glitter(t))\n\\text{Tell}(KB,Percept([Stench,Breeze,Glitter,None,None],5))\n\nAction:\n\nTurnRight,TurnLeft,Forward,Shoot,Grab,Climb,BestAction\nFor simple “reflex” behavior \\text{Tell}(KB,\\forall t\\:(Glitter(t)\\implies BestAction(Grab,t)))\nTo determine which is best, the agent program executes the query \\text{AskVars}(KB,\\exists a\\:BestAction(a,t))\n\n\nEnvironment\n\\begin{array}{l}\n    \\text{Tell}(KB,\\forall x,y,a,b\\:(Adjacent([x,y],[a,b])\\iff \\\\\n    (x=a\\land(y=b-1\\lor y=b+1))\\lor(y=b\\land(x=a-1\\lor x=a+1))))\n\\end{array}\n\\text{Tell}(KB,\\forall x,s_{1},s_{2},t\\:(At(x,s_{1},t)\\land At(x,s_{2},t)\\implies s_{1}=s_2))\n\\text{Tell}(KB,\\forall s,t\\:(At(Agent,s,t)\\land Breeze(t)\\implies Breezy(s)))\n\\text{Tell}(KB,\\forall s\\:(Breezy(s)\\iff\\exists r\\:Adjacent(r,s)\\land Pit(r)))",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html#simple-inference",
    "href": "ai-10-fl.html#simple-inference",
    "title": "10  First-order Logic",
    "section": "10.4 Simple Inference",
    "text": "10.4 Simple Inference\n\nA brief history of reasoning\n\n\n\n450B.C.\nStoics\n\n\n322B.C.\nAristotle\n\n\n1965\nRobinson\n\n\n\n\n\nSubstitution\n\nA substitution \\theta is a mapping from variables to terms. \\text{Subst}[\\theta,\\alpha] returns the result of performing substitution \\theta on \\alpha. Note that: \\text{Subst}[\\theta,\\alpha] also writtened as \\alpha\\theta.\n\n\n\\text{Subst}(\\ \\{{\\color{red}x}/alice\\},{\\color{green}P}({\\color{red}x}))={\\color{green}P}(alice)\n\\text{Subst}(\\ \\{{\\color{red}x}/alice,{\\color{red}y}/{\\color{red}z}\\},{\\color{green}P}({\\color{red}x})\\land{\\color{green}Q}({\\color{red}x},{\\color{red}y}))={\\color{green}P}(alice)\\land{\\color{green}Q}(alice,{\\color{red}z})\n\n\n\nSkolem normal form\n\nA sentence of first-order logic is in Skolem normal form if it is written as a string of quantifiers and variables (with only universal first-order quantifiers) followed by a quantifier-free part.\n\n\nEvery first-order sentence may be converted into Skolem normal form while not changing its satisfiability.\n\n\n\nUniversal instantiation (UI)\n\nEvery instantiation of a universally quantified sentence is entailed by it \n\\frac{\\forall{\\color{red}x}\\\\ \\alpha}{\\text{Subst}(\\ \\{{\\color{red}x}/g\\},\\alpha)}\n for any variable {\\color{red}x} and ground term g (a term without variables)\n\n\\begin{align*}\n& \\forall{\\color{red}x}\\:({\\color{green}King}({\\color{red}x})\\land{\\color{green}Greedy}({\\color{red}x})\\implies{\\color{green}Evil}({\\color{red}x}))\\models\\\\\n& King\\text{-}John\\land Greedy\\text{-}John\\implies Evil\\text{-}John\\\\\n& King\\text{-}Richard\\land Greedy\\text{-}Richard\\implies Evil\\text{-}Richard\\\\\n& King\\text{-}Father\\text{-}John\\land Greedy\\text{-}Father\\text{-}John\\implies Evil\\text{-}Father\\text{-}John\n\\end{align*}\n\n\nExistential instantiation (EI)\n\nFor any sentence \\alpha, variable {\\color{red}x}, and constant symbol k (skolem constant) that does not appear elsewhere in the knowledge base \n\\frac{\\exists{\\color{red}x}\\\\ \\alpha}{\\text{Subst}(\\ \\{{\\color{red}x}/k\\},\\alpha)}\n\n\n\n\\exists{\\color{red}x}\\:({\\color{green}Crown}({\\color{red}x})\\land{\\color{green}OnHead}({\\color{red}x},John))\\models{\\color{green}Crown}(C_{1})\\land{\\color{green}OnHead}(C_{1},John)\n provided C_{1} is a new constant symbol\n\nThe logic equivalence \n\\forall{\\color{red}x}\\,\\exists{\\color{red}y}\\,{\\color{green}R}({\\color{red}x},{\\color{red}y})\\Longleftrightarrow\\exists{\\color{red}y}\\,\\forall{\\color{red}x}\\,{\\color{green}R}({\\color{red}x},{\\color{green}f}({\\color{red}x}))\n where {\\color{green}f}({\\color{red}x}) is a function that maps {\\color{red}x} to {\\color{red}y}.\n\n\nFor any sentence \\alpha, variable {\\color{red}x},{\\color{red}y}, and and function {\\color{green}f} (skolem function) \n\\frac{\\forall{\\color{red}x}\\,\\exists{\\color{red}y}\\,\\alpha}{\\text{Subst}(\\ \\{{\\color{red}y}/{\\color{green}f}({\\color{red}x})\\},\\alpha)}\n\n\n\n\nUI vs. EI\n\nUI can be applied several times to add new sentences; the new KB is logically equivalent to the old\nEI can be applied once to replace the existential sentence; the new KB is not equivalent to the old, but it can be shown to be inferentially equivalent (the new KB is satisfiable iff the old KB was satisfiable)\n\n\n\nPropositionalization\n\nKnowledge base KB in first-order logic\n\n\\begin{array}{l}\n{\\color{green}King}(John)\\\\\n{\\color{green}Greedy}(John)\\\\\n{\\color{green}Brother}(Richard,John).\\forall{\\color{red}x}\\:({\\color{green}King}({\\color{red}x})\\land{\\color{green}Greedy}({\\color{red}x})\\implies{\\color{green}Evil}({\\color{red}x}))\n\\end{array}\n\n\n\n\\downarrow\\;\\text{(Instantiating)}\n\n\nKnowledge base in propositional logic\n\n\\begin{array}{l}\nKing\\text{-}John\\\\\nGreedy\\text{-}John\\\\\nBrother\\text{-}Richard\\text{-}John\\\\\nKing\\text{-}John\\land Greedy\\text{-}John\\implies Evil\\text{-}John\\\\\nKing\\text{-}Richard\\land Greedy\\text{-}Richard\\implies Evil\\text{-}Richard\n\\end{array}\n\nClaim: A ground sentence is entailed by new KB iff entailed by original KB\nClaim: Every FOL KB can be propositionalized so as to preserve entailment\nIdea: Propositionalize KB and query, apply resolution, return result\nProblem: with function symbols, there are infinitely many ground terms,\n\nE.g., {\\color{green}father}({\\color{green}father}({\\color{green}father}(John)))\n\n\n\nTheorem (Herbrand, 1930)\nIf a sentence \\alpha is entailed by an FOL KB, it is entailed by a finite subset of the propositionalization of KB\n\n\nIdea:\nfor n = 0 to ∞ do\n    create a propositional KB by instantiating with\n    depth-n terms see if α is entailed by this KB\nProblem: works if \\alpha is entailed, loops if \\alpha is not entailed\n\n\nTheorem (Turing, 1936; Church, 1936)\nEntailment in FOL is semidecidable\n\n\nAlgorithms exist that say yes to every entailed sentence, but no algorithm exists that also says no to every non-entailed sentence.\n\n\n\nProblems with propositionalization\n\nPropositionalization seems to generate lots of irrelevant sentences. For example,\n\\begin{align*}\n& {\\color{green}King}(John)\\\\\n& {\\color{green}Brother}(Richard,John)\\\\\n& \\forall{\\color{red}x}\\:{\\color{green}Greedy}({\\color{red}x})\\\\\n& \\forall{\\color{red}x}\\:({\\color{green}King}({\\color{red}x})\\land{\\color{green}Greedy}({\\color{red}x})\\implies{\\color{green}Evil}({\\color{red}x}))\n\\end{align*}\nit seems obvious that Evil\\text{-}John, but propositionalization produces lots of facts such as Greedy\\text{-}Richard that are irrelevant\nWith p k-ary predicates and n constants, there are p\\cdot n^{k} instantiations\nWith function symbols, it gets nuch much worse!",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html#unification",
    "href": "ai-10-fl.html#unification",
    "title": "10  First-order Logic",
    "section": "10.5 Unification",
    "text": "10.5 Unification\n\nUnification\n\nUnification is a process to find substitutions \\theta that make different logical expressions p and q look identical. \\text{Unify}(p,q)=\\theta where \\text{Subst}(\\theta,p)=\\text{Subst}(\\theta,q)\n\n\n\n\n\n\n\n\n\np\nq\n\\theta\n\n\n\n\n{\\color{green}Knows}(John,{\\color{red}x})\n{\\color{green}Knows}(John,Jane)\n\\{{\\color{red}x}/Jane\\}\n\n\n{\\color{green}Knows}(John,{\\color{red}x})\n{\\color{green}Knows}({\\color{red}y},Mary)\n\\{{\\color{red}x}/Mary,{\\color{red}y}/John\\}\n\n\n{\\color{green}Knows}(John,{\\color{red}x})\n{\\color{green}Knows}({\\color{red}y},{\\color{green}mother}({\\color{red}y}))\n\\{{\\color{red}y}/John,{\\color{red}x}/{\\color{green}mother}(John)\\}\n\n\n{\\color{green}Knows}(John,{\\color{red}x})\n{\\color{green}Knows}({\\color{red}x},Mary)\nfail\n\n\n\n\n\nMost General Unifier (MGU)\n\nConsider the unification \\text{Unify}({\\color{green}Knows}(John,{\\color{red}x}),{\\color{green}Knows}({\\color{red}y},{\\color{red}z})), the results could be\n\n\\theta_{1}=\\{{\\color{red}y}/John,{\\color{red}x}/z\\}\n\\theta_{2}=\\{{\\color{red}y}/John,{\\color{red}x}/John,{\\color{red}z}/John\\} The first unifier \\theta_{1} is more general than the second \\theta_{2}\n\nThere is a single Most General Unifier (MGU) that is unique up to renaming of variables \n\\theta_{MGU}=\\{{\\color{red}y}/John,{\\color{red}x}/{\\color{red}z}\\}\n\n\n\n\nThe unification algorithm\nfunction Unify(s1, s2, θ) returns a substitution to make s1 and s2 identical\n\ninputs: s1, a variable, constant, list, or compound\n        s2, a variable, constant, list, or compound\n        θ, the substitution built up so far (optional, defaults to empty)\n\n    if θ = failure then \n        return failure\n    else if s1 = s2 then \n        return θ\n    else if Variable?(s1) \n        then return Unify-Var(s1, s2, θ)\n    else if Variable?(s2) \n        then return Unify-Var(s2, s1, θ)\n    else if Compound?(s1) and Compound?(s2) \n        then return Unify(s1.Args, s2.Args, Unify(s1.Op, s2.Op, θ))\n    else if List?(s1) and List?(s2) then \n        return Unify(s1.Rest, s2.rest, Unify(s1.First, s2.First, θ))\n    else return failure\n\nfunction Unify-Var(var, s, θ) returns a substitution\n\n    if {var/val} in θ then return Unify(val, s, θ)\n    else if {s/val} in θ then return Unify(var, val, θ)\n    else if Occur-Check?(var, s) then return failure\n    else return add {var/s} to θ\n\n\nOccur Check\n\nGiven \\text{Unify-Var}({\\color{red}var}, s) return failure if where {\\color{red}var} occurs in s and s is not a variable\nFor example, \\text{Unify-Var}({\\color{red}x}, {\\color{green}father}({\\color{red}x})) cannot be unified.\n\n\n\nQuiz: Unification\n\n\n\n\n\n\n\n\np\nq\n\\theta\n\n\n\n\n{\\color{green}P}({\\color{green}f}(A),{\\color{green}g}({\\color{red}x}))\n{\\color{green}P}({\\color{red}y},{\\color{red}y})\n\n\n\n{\\color{green}P}(A,{\\color{red}x},{\\color{green}h}({\\color{green}g}({\\color{red}z})))\n{\\color{green}P}({\\color{red}z},{\\color{green}h}({\\color{red}y}),{\\color{green}h}({\\color{red}y}))\n\n\n\n{\\color{green}P}({\\color{red}x},{\\color{green}f}({\\color{red}x}),{\\color{red}z})\n{\\color{green}P}({\\color{green}g}({\\color{red}y}),{\\color{green}f}({\\color{green}g}(B)),{\\color{red}y})\n\n\n\n{\\color{green}P}({\\color{red}x},{\\color{green}f}({\\color{red}x}))\n{\\color{green}P}({\\color{green}f}({\\color{red}y}),{\\color{red}y})\n\n\n\n{\\color{green}P}({\\color{red}x},{\\color{green}f}({\\color{red}z}))\n{\\color{green}P}({\\color{green}f}({\\color{red}y}),{\\color{red}y})",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html#forward-chaining",
    "href": "ai-10-fl.html#forward-chaining",
    "title": "10  First-order Logic",
    "section": "10.6 Forward Chaining",
    "text": "10.6 Forward Chaining\n\nFirst-order definite clauses\n\nA definite clause is a disjunctions of literals of which exactly one is positive. It is\n\nan atomic or\nan implication whose antecedent is a conjunctions of positive literals and consequent is a positive literal\n\nA first-order literal can include variables, which are assumed to be universally quantified\n\n\n\\begin{array}{l}\n{\\color{green}King}({\\color{red}x})\\land{\\color{green}Greedy}({\\color{red}x})\\Rightarrow{\\color{green}Evil}({\\color{red}x})\\\\\n{\\color{green}King}(John)\\\\\n{\\color{green}Greedy}({\\color{red}y})\n\\end{array}\n\n\nDatalog = first-order definite clauses + no functions\n\n\n\nGeneralized Modus Ponens (GMP)\nGeneralized Modus Ponens For atomic sentences p_{i}, p_{i}^{\\prime}, and q, where there is a substitution \\theta such that \\text{Subst}(\\theta,p_{i}^{\\prime})=\\text{Subst}(\\theta,p_{i}), for all i \n\\dfrac{p_{1}^{\\prime},p_{2}^{\\prime},...,p_{n}^{\\prime},\\left(p_{1}\\land p_{2}\\land...\\land p_{n}\\Rightarrow q\\right)}{\\text{Subst}(\\theta,q)}\n\nFor example\n\n\n\n\n\n\n\np_{1}^{\\prime} is {\\color{green}King}(John)\np_{1} is {\\color{green}King}({\\color{red}x})\n\n\np_{2}^{\\prime} is {\\color{green}Greedy}({\\color{red}y})\np_{2} is {\\color{green}Greedy}({\\color{red}x})\n\n\n\nq is {\\color{green}Evil}({\\color{red}x})\n\n\n\\theta is \\left\\{ {\\color{red}x}/John,{\\color{red}y}/John\\right\\}\n\\text{Subst}(\\theta,q) is Evil\\text{-}John\n\n\n\n\n\nSoundness of GMP\n\nLemma\nFor any definite clause p, we have p\\models p\\theta by UI\n\nProof Need to show that p_{1}^{\\prime},\\ldots,p_{n}^{\\prime},(p_{1}\\land\\ldots\\land p_{n}\\Rightarrow q)\\models q\\theta provided that p_{i}^{\\prime}\\theta=p_{i}\\theta for all i\n\n(p_{1}\\land\\ldots\\land p_{n}\\Rightarrow q)\\models(p_{1}\\land\\ldots\\land p_{n}\\Rightarrow q)\\theta=(p_{1}\\theta\\land\\ldots\\land p_{n}\\theta\\Rightarrow q\\theta)\np_{1}^{\\prime},\\ldots,p_{n}^{\\prime}\\models p_{1}^{\\prime}\\land\\ldots\\land p_{n}^{\\prime}\\models p_{1}^{\\prime}\\theta\\land\\ldots\\land p_{n}^{\\prime}\\theta\nFrom 1 and 2, q\\theta follows by ordinary Modus Ponens\n\n\n\nExample 1\n\nProblem\nThe law says that it is a crime for an American to sell weapons to hostile nations. The country Nono, an enemy of America, has some missiles, and all of its missiles were sold to it by Colonel West, who is American.\nProve that Colonel West is a criminal?\n\n\n… it is a crime for an American to sell weapons to hostile nations\n\n{\\color{green}American}({\\color{red}x})\\land{\\color{green}Weapon}({\\color{red}y})\\land{\\color{green}Sells}({\\color{red}x},{\\color{red}y},{\\color{red}z})\\land{\\color{green}Hostile}({\\color{red}z})\\implies{\\color{green}Criminal}({\\color{red}x})\n\nNono … has some missiles\n\n{\\color{green}Owns}(Nono,{\\color{red}x})\\land{\\color{green}Missile}({\\color{red}x})\n\n… all of its missiles were sold to it by Colonel West\n{\\color{green}Missile}({\\color{red}x})\\land{\\color{green}Owns}(Nono,{\\color{red}x})\\implies{\\color{green}Sells}(West,{\\color{red}x},Nono)\nMissiles are weapons\n{\\color{green}Missile}({\\color{red}x})\\implies{\\color{green}Weapon}({\\color{red}x})\nAn enemy of America counts as “hostile”\n{\\color{green}Enemy}({\\color{red}x},America)\\implies{\\color{green}Hostile}({\\color{red}x})\nWest, who is American …\n{\\color{green}American}(West)\nThe country Nono, an enemy of America …\n{\\color{green}Enemy}(Nono,America)\n… and Colonel West is American\n{\\color{green}American}(West)\n\n\n\nForward chaining algorithm\nfunction FOL-FC-Ask(KB, α) returns a substitution or false\n\n    repeat\n        new ← ⌀\n\n        for each rule in KB do\n            (p1 ∧ … ∧ pn ⇒ q) ← Standardize-Variables(rule)\n\n            for each substitution θ such that\n                for all i, Subst(θ, pi) unifies with some fact in KB do\n\n                q′ ← Subst(θ, q)\n\n                if no sentence in KB ∪ new unifies with q′ then\n                    add q′ to new\n\n                    φ ← Unify(q′, α)\n                    if φ ≠ fail then\n                        return φ\n\n        add all sentences in new to KB\n    until new = ⌀\n\n    return false\n\n\nForward chaining proof\n\n\n\n\n\nExample 2\n\nProblem\n\nArt is the father of Bob and Bud.\nBob is the father of Cal and Coe.\nGrandfather is the father of a father.\n\nQuestion Is Art the grandfather of Coe?\n\nConvert English sentences into FOL sentences\n\n\n\n\n\n\n\n\n#\nFOL sentence\nexplain\n\n\n\n\n1\n{\\color{green}F}(Art,Bob)\nKB\n\n\n2\n{\\color{green}F}(Art,Bud)\nKB\n\n\n3\n{\\color{green}F}(Bob,Cal)\nKB\n\n\n4\n{\\color{green}F}(Bob,Coe)\nKB\n\n\n5\n{\\color{green}F}({\\color{red}x},{\\color{red}y})\\land{\\color{green}F}({\\color{red}y},{\\color{red}z})\\implies{\\color{green}G}({\\color{red}x},{\\color{red}z})\nKB\n\n\n6\n{\\color{green}G}(Art,Coe)\n1,4,5 \\left\\{{\\color{red}x}/Art,{\\color{red}y}/Bob,{\\color{red}z}/Coe\\right\\}\n\n\n\n\n\nProperties of forward chaining\n\nSound:\n\nYES, every inference is just an application of GMP\n\nComplete:\n\nYES for definite clause knowledge bases\nIt answers every query whose answers are entailed by any KB of definite clauses\n\nIt terminates for Datalog in poly iterations: at most p\\cdot n^{k} literals\nIt may not terminate in general if \\alpha is not entailed\n\nThis is unavoidable: entailment with definite clauses is semidecidable\n\n\n\n\nEfficiency of forward chaining\n\nSimple observation: no need to match a rule on iteration k if a premise wasn’t added on iteration k-1 \\to match each rule whose premise contains a newly added literal\nMatching itself can be expensive\nDatabase indexing allows O(1) retrieval of known facts E.g., query {\\color{green}Missile}({\\color{red}x}) retrieves {\\color{green}Missile}(M_{1})\nMatching conjunctive premises against known facts is NP-hard\nForward chaining is widely used in deductive databases\n\n\n\nDefinite clauses with function symbols\n\nInference can explode forward and may never terminate.\nConsider the following KB with two predicates and two functions\n\n\n\\begin{array}{l}\n{\\color{green}Even}(2)\\\\\n{\\color{green}Even}({\\color{red}x})\\implies{\\color{green}Even}({\\color{green}plus}({\\color{red}x},2))\\\\\n{\\color{green}Integer}({\\color{red}x})\\implies{\\color{green}Even}({\\color{green}times}({\\color{red}x},2))\\\\\n{\\color{green}Even}({\\color{red}x})\\implies{\\color{green}Integer}({\\color{red}x})\n\\end{array}\n\n\n\n\nQuiz: Forward chaining\n\nGiven a KB containing the following sentence\n{\\color{green}Parent}({\\color{red}x},{\\color{red}y})\\land{\\color{green}Male}({\\color{red}x})\\implies{\\color{green}Father}({\\color{red}x},{\\color{red}y})\n{\\color{green}Father}({\\color{red}x},{\\color{red}y})\\land{\\color{green}Father}({\\color{red}x},{\\color{red}z})\\implies{\\color{green}Sibling}({\\color{red}y},{\\color{red}z})\n{\\color{green}Male}(Tom)\n{\\color{green}Parent}(Tom,John)\n{\\color{green}Parent}(Tom,Fred)\nPerform the forward chaining until a fixed point is reached.",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html#backward-chaining",
    "href": "ai-10-fl.html#backward-chaining",
    "title": "10  First-order Logic",
    "section": "10.7 Backward Chaining",
    "text": "10.7 Backward Chaining\n\nA backward-chaining algorithm\nfunction FOL-BC-Ask(KB, query) returns a generator of substitutions\n    return FOL-BC-Or(KB, query, {})\n\ngenerator FOL-BC-Or(KB, goal, θ) yields a substitution\n    for each rule (lhs ⇒ rhs) in Fetch-Rules-For-Goal(KB, goal) do\n        (lhs, rhs) ← Standardize-Variables((lhs, rhs))\n        θ1 ← Unify(rhs, goal, θ)\n        if θ1 ≠ failure then\n            for each θ2 in FOL-BC-And(KB, lhs, θ1) do\n                yield θ2\n\ngenerator FOL-BC-And(KB, goals, θ) yields a substitution\n    if θ = failure then return\n    else if goals is empty then\n        yield θ\n    else\n        first ← First(goals)\n        rest ← Rest(goals)\n        for each θ1 in FOL-BC-Or(KB, Subst(θ, first), θ) do\n            for each θ2 in FOL-BC-And(KB, Subst(θ1, rest), θ1) do\n                yield θ2\n\n\nBackward chaining example\n\n\n\n\n\n\n\n\n\nProperties of backward chaining\n\nDepth-first recursive proof search\n\nspace is linear in size of proof\n\nIncomplete due to infinite loops\n\nfix by checking current goal against every goal on stack\n\nInefficient due to repeated subgoals (both success and failure)\n\nfix using caching of previous results (extra space!)\n\nWidely used for logic programming",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html#resolution",
    "href": "ai-10-fl.html#resolution",
    "title": "10  First-order Logic",
    "section": "10.8 Resolution",
    "text": "10.8 Resolution\n\nConversion to CNF\nA sentence “Everyone who loves all animals is loved by someone” is represented by\n\n\\forall{\\color{red}x}[[\\forall{\\color{red}y}[{\\color{green}Animal}({\\color{red}y})\\implies{\\color{green}Loves}({\\color{red}x},{\\color{red}y})]]\\implies[\\exists{\\color{red}z}\\:{\\color{green}Loves}({\\color{red}z},{\\color{red}x})]]\n\n\nStandardize variables: each quantifier should use a different one\n\n\\forall{\\color{red}x}[[\\forall{\\color{red}y}[{\\color{green}Animal}({\\color{red}y})\\implies{\\color{green}Loves}({\\color{red}x},{\\color{red}y})]]\\implies[\\exists{\\color{red}z}\\:{\\color{green}Loves}({\\color{red}z},{\\color{red}x})]]\n\nEliminate biconditionals and implications\n\n\\forall{\\color{red}x}[[\\lnot\\forall{\\color{red}y}\\:[\\lnot{\\color{green}Animal}({\\color{red}y})\\lor{\\color{green}Loves}({\\color{red}x},{\\color{red}y})]]\\lor[\\exists{\\color{red}z}\\:{\\color{green}Loves}({\\color{red}z},{\\color{red}x})]]\n\nMove \\lnot inwards:\n\n\\begin{array}{l}\n\\forall{\\color{red}x}[[\\exists{\\color{red}y}\\:\\lnot(\\lnot{\\color{green}Animal}({\\color{red}y})\\lor{\\color{green}Loves}({\\color{red}x},{\\color{red}y}))]\\lor[\\exists z\\:{\\color{green}Loves}({\\color{red}z},{\\color{red}x})]] \\\\\n\\forall{\\color{red}x}[[\\exists{\\color{red}y}\\:(\\lnot\\lnot{\\color{green}Animal}({\\color{red}y})\\land\\lnot{\\color{green}Loves}({\\color{red}x},{\\color{red}y}))]\\lor[\\exists z\\:{\\color{green}Loves}({\\color{red}z},{\\color{red}x})]] \\\\\n\\forall{\\color{red}x}[[\\exists{\\color{red}y}\\:(\\lnot{\\color{green}Animal}({\\color{red}y})\\land\\lnot{\\color{green}Loves}({\\color{red}x},{\\color{red}y}))]\\lor[\\exists z\\:{\\color{green}Loves}({\\color{red}z},{\\color{red}x})]]\n\\end{array}\n\nSkolemize: a more general form of existential instantiation. Each existential variable is replaced by a Skolem function of the enclosing universally quantified variables\n\n\\forall{\\color{red}x}[[{\\color{green}Animal}({\\color{green}F}({\\color{red}x}))\\land\\lnot{\\color{green}Loves}({\\color{red}x},{\\color{green}f}({\\color{red}x}))]\\lor{\\color{green}Loves}({\\color{green}g}({\\color{red}x}),{\\color{red}x})\n\nDrop universal quantifiers\n\n{\\color{green}Animal}({\\color{green}f}({\\color{red}x}))\\land\\lnot{\\color{green}Loves}({\\color{red}x},{\\color{green}f}({\\color{red}x}))]\\lor{\\color{green}Loves}({\\color{green}g}({\\color{red}x}),{\\color{red}x})\n\nDistribute \\land over \\lor\n\n{\\color{green}Animal}({\\color{green}f}({\\color{red}x}))\\lor{\\color{green}Loves}({\\color{green}g}({\\color{red}x}),{\\color{red}x})]\\land[\\lnot{\\color{green}Loves}({\\color{red}x},{\\color{green}f}({\\color{red}x}))\\lor{\\color{green}Loves}({\\color{green}g}({\\color{red}x}),{\\color{red}x})\n\n\n\n\nGeneralized Resolution\n\nFull first-order version \\begin{equation}\n\\frac{\\ell_{1}\\lor\\cdots\\lor{\\color{red}\\ell_{i}}\\lor\\cdots\\lor\\ell_{k},\\qquad m_{1}\\lor\\cdots\\lor{\\color{green}{\\color{orange}m_{j}}}\\lor\\cdots\\lor m_{n}}{(\\ell_{1}\\lor\\cdots\\lor\\ell_{i-1}\\lor\\ell_{i+1}\\lor\\cdots\\lor\\ell_{k}\\lor m_{1}\\lor\\cdots\\lor m_{j-1}\\lor m_{j+1}\\lor\\cdots\\lor m_{n})\\theta}\n\\end{equation} where \\text{Unify}({\\color{red}\\ell_{i}},\\lnot{\\color{green}{\\color{orange}m_{j}}})=\\theta.\n\n\n\\frac{\\begin{array}{l}\n{\\color{green}\\lnot Rich}({\\color{red}x})\\lor{\\color{green}Unhappy}({\\color{red}x}),\\qquad{\\color{green}Rich}(Ken)\\end{array}}{\\begin{array}{l}\n{\\color{green}Unhappy}(Ken)\\end{array}}\n with \\theta=\\{{\\color{red}x}/Ken\\}\n\n\nSolution to Example 1\n\n\n\nSolution to Example 2\n\n\n\n\n\n\n\n\n#\nFOL clause\nexplain\n\n\n\n\n1\n{\\color{green}F}(Art,Bob)\nKB\n\n\n2\n{\\color{green}F}(Art,Bud)\nKB\n\n\n3\n{\\color{green}F}(Bob,Cal)\nKB\n\n\n4\n{\\color{green}F}(Bob,Coe)\nKB\n\n\n5\n\\lnot{\\color{green}F}({\\color{red}x},{\\color{red}y})\\lor\\lnot{\\color{green}F}({\\color{red}y},{\\color{red}z})\\lor{\\color{green}G}({\\color{red}x},{\\color{red}z})\nKB\n\n\n6\n\\lnot{\\color{green}G}(Art,Coe)\n\\lnot\\alpha\n\n\n7\n\\lnot{\\color{green}F}(Art,{\\color{red}y})\\lor\\lnot{\\color{green}F}({\\color{red}y},Code)\n5,6 \\left\\{{\\color{red}x}/Art,{\\color{red}z}/Coe\\right\\}\n\n\n8\n\\lnot{\\color{green}F}(Art,Bob)\n4,7 \\left\\{{\\color{red}y}/Bob\\right\\}\n\n\n9\n\\emptyset\n1,8\n\n\n\n\n\nExample 3\n\nProblem\nEveryone who loves all animals is loved by someone. Anyone who kills an animal is loved by no one. Jack loves all animals. Either Jack or Curiosity killed the cat, who is named Tuna. Did Curiosity kill the cat?\n\n\n\nSolution to Example 3\nExtract simple sentences from the problem\n\nEveryone who loves animals is loved by someone.\nAnyone who kills an animal is loved by no one.\nJack loves all animals.\nEither Jack or Curiosity killed the cat.\nThe cat is named Tuna.\nCats are animals.\nDid Curiosity kill the cat?\n\nConvert to FOL sentences\n\n\\forall{\\color{red}x}[[\\forall{\\color{red}y}{\\color{green}Animal}({\\color{red}y})\\implies{\\color{green}Loves}({\\color{red}x},{\\color{red}y})]\\implies\\exists{\\color{red}z}{\\color{green}Loves}({\\color{red}z},{\\color{red}x})]\n\\forall{\\color{red}x}[[\\exists{\\color{red}y}{\\color{green}Animal}({\\color{red}y})\\land{\\color{green}Kills}({\\color{red}x},{\\color{red}y})]\\implies\\forall{\\color{red}z}\\lnot{\\color{green}Loves}({\\color{red}z},{\\color{red}x})]\n\\forall{\\color{red}x}[{\\color{green}Animal}({\\color{red}x})\\implies{\\color{green}Loves}(Jack,{\\color{red}x})]\n{\\color{green}Kills}(Jack,Tuna)\\lor{\\color{green}Kills}(Curiosity,Tuna)\n{\\color{green}Cat}(Tuna)\n\\forall{\\color{red}x}[{\\color{green}Cat}({\\color{red}x})\\implies{\\color{green}Animal}(x)]\n\\lnot{\\color{green}Kills}(Curiosity,Tuna)\n\nConvert to CNF sentences\n\n{\\color{green}Animal}({\\color{green}F}({\\color{red}x})\\lor{\\color{green}Loves}({\\color{green}G}({\\color{red}x}),{\\color{red}x})\n\\lnot{\\color{green}Loves}({\\color{red}x},{\\color{green}F}({\\color{red}x}))\\lor{\\color{green}Loves}({\\color{green}G}({\\color{red}x}),{\\color{red}x})\n\\lnot{\\color{green}Loves}({\\color{red}x},{\\color{red}y})\\lor\\lnot{\\color{green}Animal}({\\color{red}z})\\lor\\lnot{\\color{green}Kills}({\\color{red}x},{\\color{red}z})\n\\lnot{\\color{green}Animal}({\\color{red}x})\\lor\\lnot{\\color{green}Loves}(Jack,{\\color{red}x})\n{\\color{green}Kills}(Jack,Tuna)\\lor{\\color{green}Kills}(Curiousity,Tuna)\n{\\color{green}Cat}(Tuna)\n\\lnot{\\color{green}Cat}({\\color{red}x})\\lor{\\color{green}Animal}({\\color{red}x})\n\\lnot{\\color{green}Kills}(Curiousity,Tuna)\n\nUsing resolution to prove that Curiosity killed the cat\n\n\nSuppose Curiosity did not kill Tuna. We know that either Jack or Curiosity did; thus Jack must have. Now, Tuna is a cat and cats are animals, so Tuna is an animal. Because anyone who kills an animal is loved by no one, we know that no one loves Jack. On the other hand, Jack loves all animals, so someone loves him; so we have a contradiction. Therefore, Curiosity killed the cat.\n\n\n\nExercise\nGiven a KB of the following sentences\n\nAnyone whom Mary loves is a football star.\nAny student who does not pass does not play.\nJohn is a student.\nAny student who does not study does not pass.\nAnyone who does not play is not a football star.\n\nProve that “If John does not study, Mary does not love John”",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "ai-10-fl.html#references",
    "href": "ai-10-fl.html#references",
    "title": "10  First-order Logic",
    "section": "10.9 References",
    "text": "10.9 References",
    "crumbs": [
      "Knowledge and reasoning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>First-order Logic</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html",
    "href": "lect10-rl.html",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "",
    "text": "Agent and Environment",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#the-reinforcement-learning-problem",
    "href": "lect10-rl.html#the-reinforcement-learning-problem",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.1 The Reinforcement Learning Problem",
    "text": "11.1 The Reinforcement Learning Problem",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#problems",
    "href": "lect10-rl.html#problems",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.2 Problems",
    "text": "11.2 Problems\n\nCart-Pole Problem\n\n\nObjective: Balance a pole on top of a movable cart\nState: angle, angular speed, position, horizontal velocity\nAction: horizontal force applied on the cart\nReward: 1 at each time step if the pole is upright\n\n\n\nRobot Locomotion\n\n\nObjective: Make the robot move forward\nState: Angle, position, velocity of all joints\nAction: Torques applied on joints\nReward: 1 at each time step upright + forward movement\n\n\n\nAtari Games\n\n\nObjective: Complete the game with the highest score\nState: Raw pixel inputs of the game screen\nAction: Game controls e.g. Left, Right, Up, Down\nReward: Score increase/decrease at each time step\n\n\n\nGo\n\n\nObjective: Win the game!\nState: Position of all pieces\nAction: Where to put the next piece down\nReward: On last turn: 1 if you won, 0 if you lost",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#state",
    "href": "lect10-rl.html#state",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.3 State",
    "text": "11.3 State\n\nHistory and State\n\nThe history is the sequence of observations, actions, rewards, i.e. all observable variables up to time t \\begin{equation}\nH_{t}=O_{1},R_{1},A_{1},...,A_{t-1},O_{t},R_{t}\n\\end{equation}\n\n\nWhat happens next depends on the history:\n\nThe agent selects actions\nThe environment selects observations/rewards\n\n\n\nState is the information used to determine what happens next. Formally, state is a function of the history \\begin{equation}\nS_{t}=f(H_{t})\n\\end{equation}\n\n\n\nEnvironment State\n\n\nThe environment state S_{t}^{e} is the environment’s private representation\ni.e. whatever data the environment uses to pick the next observation/reward\nThe environment state is not usually visible to the agent\nEven if S_{t}^{e} is visible, it may contain irrelevant information\n\n\n\nAgent State\n\n\nThe agent state S_{t}^{a} is the agent’s internal representation\ni.e. whatever information the agent uses to pick the next action\ni.e. it is the information used by reinforcement learning algorithms\nIt can be any function of history: \\begin{equation}\nS_{t}^{a}=f(H_{t})\n\\end{equation}\n\n\n\nInformation State\nAn information state (a.k.a. Markov state) contains all useful information from the history.\n\nA state S_{t} is Markov if and only if \\begin{equation}\nP[S_{t+1}\\mid S_{t}]=P[S_{t+1}\\mid S_{1},...,S_{t}]\n\\end{equation}\n\n\n“The future is independent of the past given the present”\n\n\\begin{equation}\nH_{1:t}\\to S_{t}\\to H_{t+1:\\infty}\n\\end{equation}\n\nOnce the state is known, the history may be thrown away; i.e., the state is a sufficient statistic of the future\n\n\n\nMarkov Assumption\n\n\nThe environment state S_{t}^{e} is Markov\nThe history H_{t} is Markov\n\n\n\n\nFully Observable Environments\n\nFull observability: agent directly observes environment state \\begin{equation}\nO_{t}=S_{t}^{a}=S_{t}^{e}\n\\end{equation}\n\n\nFormally, this is a Markov decision process (MDP)\n\n\n\nPartially Observable Environments\n\nPartial observability: agent indirectly observes environment; now agent state is different from environment state. Agent must construct its own state representation S_{t}^{a} , e.g.\n\nComplete history: S_{t}^{a}=H_{t}\nBeliefs of environment state: S_{t}^{a}=(P[S_{t}^{e}=s^{1}],...,P[S_{t}^{e}=s^{n}])\nRecurrent neural network: S_{t}^{a}=\\sigma(S_{t-1}^{a}W_{s}+O_{t}W_{o})\n\n\n\nA robot with camera vision isn’t told its absolute location\nA trading agent only observes current prices\nA poker playing agent only observes public cards\nFormally this is a partially observable Markov decision process (POMDP)",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#inside-an-rl-agent",
    "href": "lect10-rl.html#inside-an-rl-agent",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.4 Inside An RL Agent",
    "text": "11.4 Inside An RL Agent\n\nMajor Components of an RL Agent\nAn RL agent may include one or more of these components:\n\nAgent state\nPolicy: agent’s behaviour function\nValue function: how good is each state and/or action\nModel: agent’s representation of the environment\n\n\n\nPolicy\n\nA policy is the agent’s behaviour. It is a map from state to action\n\n\nDeterministic policy: \\begin{equation}\na=\\pi(s)\n\\end{equation}\nStochastic policy: \\begin{equation}\n\\pi(a\\mid s)=P[A_{t}=a\\mid S_{t}=s]\n\\end{equation}\n\n\n\nValue Function\n\nValue function is a prediction of future reward\n\n\nUsed to evaluate the goodness/badness of states\nAnd therefore to select between actions, e.g.\n\n\\begin{equation}\nv_{\\pi}(s)=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+...\\mid S_{t}=s]\n\\end{equation}\n\n\nModel\n\nA model predicts what the environment will do next\n\n\n\\mathcal{P} predicts the next state\n\n\\begin{equation}\n\\mathcal{P}_{ss'}^{a}=P[S_{t+1}=s'\\mid S_{t}=s,A_{t}=a]\n\\end{equation}\n\n\\mathcal{R} predicts the next (immediate) reward \\begin{equation}\n\\mathcal{R}_{s}^{a}=\\mathbb{E}[R_{t+1}\\mid S_{t}=s,A_{t}=a]\n\\end{equation}\n\n\n\nMaze Example\n\n\nRewards: -1 per time-step\nActions: N, E, S, W\nStates: Agent’s location\n\n\n\nMaze Example: Policy\n\nArrows represent policy \\pi(s) for each state s\n\n\n\n\nMaze Example: Value Function\n\nNumbers represent value v^{\\pi}(s) of each state s\n\n\n\n\nMaze Example: Model\n\nAgent may have an internal model of the environment\nDynamics: how actions change the state\nRewards: how much reward from each state\nThe model may be imperfect\nGrid layout represents transition model P_{ss'}^{a}\nNumbers represent immediate reward R_{s}^{a} from each state s (same for all a)\n\n\n\nCategorizing RL agents\n\n\n\n\n\n\n\n\nValue Based\n\nNo Policy (Implicit)\nValue Function\n\nPolicy Based\n\nPolicy\nNo Value Function\n\nActor Critic\n\nPolicy\nValue Function\n\n\n\nModel Free\n\nPolicy and/or Value Function\nNo Model\n\nModel Based\n\nPolicy and/or Value Function\nModel",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#problems-within-reinforcement-learning",
    "href": "lect10-rl.html#problems-within-reinforcement-learning",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.5 Problems within Reinforcement Learning",
    "text": "11.5 Problems within Reinforcement Learning\n\nLearning and Planning\nTwo fundamental problems in sequential decision making\n\nReinforcement Learning:\n\nThe environment is initially unknown\nThe agent interacts with the environment\nThe agent improves its policy\n\nPlanning:\n\nA model of the environment is known\nThe agent performs computations with its model (without any external interaction)\nThe agent improves its policy\n\n\n\n\nAtari Example: Reinforcement Learning\n\n\nRules of the game are unknown\nLearn directly from interactive game-play\nPick actions on joystick, see pixels and scores\n\n\n\nAtari Example: Planning\n\n\nRules of the game are known\nCan query emulator\n\nperfect model inside agent’s brain\n\nIf I take action a from state s:\n\nwhat would the next state be?\nwhat would the score be?\n\nPlan ahead to find optimal policy\n\ne.g. tree search\n\n\n\n\nExploration and Exploitation\n\nReinforcement learning is like trial-and-error learning\n\nThe agent should discover a good policy\nfrom its experiences of the environment\nwithout losing too much reward along the way\n\nExploration finds more information about the environment\nExploitation exploits known information to maximise reward\nIt is usually important to explore as well as exploit\n\n\n\nExploration and exploitation (cont.)\n\n\n\n\nExploitation\nExploration\n\n\n\n\n\ngo with the best strategy found so far\ntake a new action with unknown consequences\n\n\nPros\nMaximize reward as reflected in the current utility estimates\nGet a more accurate model of the environment\n\n\n\nAvoid bad stuff\nDiscover higher-reward states than the ones found so far\n\n\nCons\nMight also prevent you from discovering the true optimal strategy\nWhen you’re exploring, you’re not maximizing your utility\n\n\n\n\nSomething bad might happen\n\n\n\n\n\nExamples\n\n\n\n\n\nExploitation\nExploration\n\n\n\n\nRestaurant Selection\nGo to your favourite restaurant\nTry a new restaurant\n\n\nOnline Banner Advertisements\nShow the most successful advert\nShow a different advert\n\n\nOil Drilling\nDrill at the best known location\nDrill at a new location\n\n\nGame Playing\nPlay the move you believe is best\nPlay an experimental move\n\n\n\n\n\nPrediction and Control\n\nPrediction: evaluate the future\n\nGiven a policy\n\nControl: optimise the future\n\nFind the best policy",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#agents-learning-task",
    "href": "lect10-rl.html#agents-learning-task",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.6 Agent’s Learning Task",
    "text": "11.6 Agent’s Learning Task\n\nValue Functions\n\nThe state-value function at state s, is the expected cumulative reward from following the policy \\pi from state s \\begin{equation}\nv^{\\pi}(s)=\\mathbb{E}\\left[G_{t}\\mid S_{t}=s\\right]\n\\end{equation}\n\n\nThe action-value Q function at state s and action a, is the expected cumulative reward from taking action a in state s and then following the policy \\pi \\begin{equation}\nq^{\\pi}(s,a)=\\mathbb{E}\\left[G_{t}\\mid S_{t}=s,A_{t}=a\\right]\n\\end{equation}\n\n\n\nOptimal Value Functions\n\n\nThe optimal state-value function v^{*}(s) is the maximum value function over all policies \\begin{equation}\nv^{*}(s)=\\max_{\\pi}v^{\\pi}(s)\n\\end{equation}\nThe optimal action-value function q^{*}(s,a) is the maximum action-value function over all policies \\begin{equation}\nq^{*}(s,a)=\\max_{\\pi}q^{\\pi}(s,a)\n\\end{equation}\n\n\n\n\nEstimating Value Functions\n\nEstimating v^{\\pi} or q^{\\pi} is called policy evaluation or, simply, prediction\nEstimating v^{*} or q^{*} is sometimes called control, because these can be used for policy optimization\n\n\n\nOptimal Policy\n\nA partial ordering over policies \\begin{equation}\n\\pi\\geq\\pi'\\text{ if }v^{\\pi}(s)\\geq v^{\\pi'}(s),\\forall s\n\\end{equation}\n\n\nFor any Markov Decision Process\n\nThere exists an optimal policy \\pi^{\\ast} that is better than or equal to all other policies, \\pi^{\\ast}\\geq\\pi,\\forall\\pi\nAll optimal policies achieve the optimal value function, v^{\\pi^{\\ast}}(s)=v^{\\ast}(s)\nAll optimal policies achieve the optimal action-value function, q^{\\pi^{\\ast}}(s,a)=q^{\\ast}(s,a)\n\n\n\n\nAgent’s Learning Task\n\nAgent’s Goal: Find the optimal policy \\pi^{\\ast} that maximize the expected sum of rewards . \\begin{equation}\n\\pi^{\\ast}=\\arg\\max_{\\pi}\\mathbb{E}\\left[G_{t}\\right]\n\\end{equation}\n\n\n\nOptimal Q-function\n\nq^{\\ast} encodes the optimal policy, an optimal policy can be found by maximising over q^{\\ast}(s,a) \\begin{equation}\n\\pi^{\\ast}(a\\mid s)=\\begin{cases}\n1 & \\text{if }a=\\arg\\max_{a\\in\\mathcal{A}}q^{\\ast}(s,a)\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{equation} \\begin{equation}\n\\pi^{\\ast}(s)=\\arg\\max_{a}q^{\\ast}(s,a)\n\\end{equation}\nIf we know q^{\\ast}(s,a), we immediately have the optimal policy",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#bellman-equation",
    "href": "lect10-rl.html#bellman-equation",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.7 Bellman Equation",
    "text": "11.7 Bellman Equation\n\nBellman equations\n\nTwo Bellman equations for policy \\pi \\begin{align}\nv^{\\pi}(s) & =\\mathbb{E}[R_{t+1}+\\gamma v^{\\pi}(S_{t+1})\\mid S_{t}=s]\\\\\nq^{\\pi}(s,a) & =\\mathbb{E}[R_{t+1}+\\gamma q^{\\pi}(S_{t+1},A_{t+1})\\mid S_{t}=s,A_{t}=a]\\nonumber\n\\end{align}\nTwo Bellman optimality equations\n\n\\begin{align}\nv^{\\ast}(s) & =\\max_{a}\\mathbb{E}[R_{t+1}+\\gamma v^{\\ast}(S_{t+1})\\mid S_{t}=s,A_{t}=a]\\\\\nq^{\\ast}(s,a) & =\\mathbb{E}[R_{t+1}+\\gamma\\max_{a'}q^{\\ast}(S_{t+1},a')\\mid S_{t}=s,A_{t}=a]\\nonumber\n\\end{align}\n\n\nBellman equations (cont.)\n\nThere are equivalences between state and action values \\begin{align}\nv^{\\pi}(s) & =\\sum_{a}\\pi(a\\mid s)q^{\\pi}(s,a)\\\\\nv^{\\ast}(s) & =\\max_{a}q^{\\ast}(s,a)\n\\end{align}\n\n\n\nSolving the Bellman Optimality Equation\n\nBellman Optimality Equation is non-linear and there is no closed form solution (in general)\nMany iterative solution methods\nUsing models/dynamic programming\n\nValue iteration\nPolicy iteration\n\nUsing samples\n\nMonte Carlo\nQ-learning\nSarsa",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#dynamic-programming",
    "href": "lect10-rl.html#dynamic-programming",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.8 Dynamic Programming",
    "text": "11.8 Dynamic Programming\n\nWhat is Dynamic Programming?\n\nDynamic Programming is a method for solving complex problems by breaking them down into subproblems\n\nSolve the subproblems\nCombine solutions to subproblem\n\n\n\nAll such methods consist of two important parts: policy evaluation and policy improvement\n\n\n\nRequirements for Dynamic Programming\nDynamic Programming is a very general solution method for problems which have two properties:\n\nOptimal substructure\n\nPrinciple of optimality applies\nOptimal solution can be decomposed into subproblems\n\nOverlapping subproblems\n\nSubproblems recur many times\nSolutions can be cached and reused",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#q-learning",
    "href": "lect10-rl.html#q-learning",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.9 Q-Learning",
    "text": "11.9 Q-Learning\n\nQ-Learning for Deterministic Worlds\n\nFor each (s,a) initialize table entry Q(s,a)\\leftarrow0\nObserve current state s\nDo forever:\n\nSelect an action a and execute it\nReceive immediate reward r and observe the new state s'\nUpdate the table entry for Q(s,a) as follows \\begin{align}\nQ(s,a) & \\leftarrow r+\\gamma\\max_{a'}Q(s',a')\\\\\ns & \\leftarrow s'\\nonumber\n\\end{align}\n\n\n\n\nQ-Learning Theorem\nNotice if rewards non-negative, then (\\forall s,a,n)\\ 0\\leq Q_{n}(s,a)\\leq Q_{n+1}(s,a)\\leq Q^{\\ast}(s,a)\n\nQ converges to Q^{\\ast}.\n\n\nProof. Proof. Define a full interval to be an interval during which each (s,a) is visited. During each full interval the largest error in Q table is reduced by factor of \\gamma\nLet Q_{n} be table after n updates, and \\Delta_{n} be the maximum error in Q_{n}; that is \\Delta_{n}=\\max_{s,a}|Q_{n}(s,a)-Q^{\\ast}(s,a)|\nFor any table entry Q_{n}(s,a) updated on iteration n+1, the error in the revised estimate Q_{n+1}(s,a) is\n\\begin{align*}\n|Q_{n+1}(s,a)-Q^{\\ast}(s,a)| & = & |(r+\\gamma\\max_{a'}Q_{n}(s',a'))-(r+\\gamma\\max_{a'}Q^{\\ast}(s',a'))|\\\\\n& = & \\gamma|\\max_{a'}Q_{n}(s',a')-\\max_{a'}Q^{\\ast}(s',a')|\\\\\n& \\leq & \\gamma\\max_{a'}|Q_{n}(s',a')-Q^{\\ast}(s',a')|\\\\\n& \\leq & \\gamma\\max_{s'',a'}|Q_{n}(s'',a')-Q^{\\ast}(s'',a')|\\\\\n|Q_{n+1}(s,a)-Q^{\\ast}(s,a)| & \\leq & \\gamma\\Delta_{n}\n\\end{align*} ◻\n\n\n\nQ-Learning for Nondeterministic Worlds\n\nFor each (s,a) initialize table entry Q(s,a)\\leftarrow0\nIterate over t=1,2,\\dotsc\n\\qquadUpdate the table entry for Q(s,a) as follows \\begin{align}\nQ^{new}(s_{t},a_{t})\\leftarrow & Q(s_{t},a_{t})+\\nonumber \\\\\n& \\underset{\\text{learning rate}}{\\underbrace{\\alpha}}\\overset{\\text{temporal difference}}{\\overbrace{[\\underset{\\text{new value}}{\\underbrace{r_{t}+\\gamma\\max_{a}Q(s_{t+1},a)}}-\\underset{\\text{old value}}{\\underbrace{Q(s_{t},a_{t})}}]}}\n\\end{align}\n\n\nQ converges to Q^{\\ast} [Watkins and Dayan, 1992]",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  },
  {
    "objectID": "lect10-rl.html#references",
    "href": "lect10-rl.html#references",
    "title": "11  REINFORCEMENT LEARNING",
    "section": "11.10 References",
    "text": "11.10 References",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>REINFORCEMENT LEARNING</span>"
    ]
  }
]